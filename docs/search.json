[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inspect Element",
    "section": "",
    "text": "Inspect Element is a practitioner‚Äôs guide to hypothesis-driven accountability work.\nThe guide walks through in-depth case studies and hands-on tutorials to help you audit algorithims and investigate opaque systems, systematically.\nYou will learn how to build your own datasets, find hidden APIs, analyze missing data, measure disparate outcomes, bullet-proof findings, and write with precision.\nAs practitioners ourselves, we‚Äôve had to learn on the job, hit dead ends, and seek external expertise across disparate disciplines and industries. Here, we distill these experiences and include tips for veterans and new-comers alike.\nThe disciplines we‚Äôll draw from include: investigative journalism, data science, computational social science, and various branches of computer and information science.\nDon‚Äôt code? No problem: the guide emphasizes underlying principles and uses plain-language (‚Äúpseudocode‚Äù) explainations to accompany any code.\n\nWho wrote this?\nInspect Element is written and edited by investigative data journalist Leon Yin with contributions by TK, [REDACTED], and [REDACTED].\nI‚Äôll frequently reference past investigations I‚Äôve worked on in this guide. You can read those investigations plus new stores (when I make deadlines) at The Markup.\nThis site was generated using the Quarto open-source publishing system."
  },
  {
    "objectID": "posts/2022-07-05-finding-apis.html#how-to-find-apis",
    "href": "posts/2022-07-05-finding-apis.html#how-to-find-apis",
    "title": "2¬† Finding Undocumented APIs",
    "section": "2.1 How to find APIs?",
    "text": "2.1 How to find APIs?\nMost APIs are undocumented, hidden in plain sight.\nYou can use these API‚Äôs to collect datasets for investigations, audits, and tools.\nYou can sniff them out using a web browser‚Äôs developer tools (shortened to dev tools).\nNote that if you‚Äôre in a workshop setting: hitting the example API at the same time will get us all blocked from the website!\n\n2.1.1 1. First open the developer console.\nSee how on Chrome or Firefox here.\nIn this tutorial, we‚Äôll see how Amazon.com autocomplete search suggestions work.\nMy go-to method, (regardless of browser,) is to right-click and ‚ÄúInspect‚Äù an element on the page.\n\nThis will open the dev tools under the ‚ÄúElements‚Äù tab, which is used to explore the source code of a page.\nPage source code is useful because it reveals useful clues that are otherwise unseen by regular users. Often times, clues are in accessibility features known as ARIA elements.\nHowever, this tutorial is not about source code‚Ä¶ it‚Äôs about API requests that populate what we see on the page, and the hidden fields that we don‚Äôt see.\nLet‚Äôs try this!\nWith dev tools open, go to Amazon.com, select the search bar on the website, and start typing a query (such as ‚Äúspicy‚Äù).\n\n\n2.1.2 2. Click the ‚ÄúNetwork‚Äù tab.\nThis section of the dev tools is used to monitor network requests.\nBackground\nEverything on a page is retrieved from some outside source, likely a server. This includes things like images embedded on the page, JavaScript code running in the background, and all the bits of ‚Äúcontent‚Äù that populate the page before us.\nUsing the network tab, we can find out how this information is requested from a server, and intercept the response before it is rendered on the page.\nThese responses are information-rich, and contain fields that don‚Äôt end up in the source code or in the user interface that most people encounter when they visit a site.\nFurther, we can reverse-engineer how this request is made, and use it to collect structured data at scale. This is the power of finding undocumented APIs.\nBack to the console‚Ä¶\nThe network tab can look pretty hectic at first. It has many uses, and a lot of information. We‚Äôll cover some of the basics.\n\n%%HTML\n<video width=70% controls loop>\n  <source src=\"assets/dev-console.mov\" type=\"video/mp4\">\n</video>\n\n\n  \n\n\n\n\n\n2.1.3 3. Filter requests by fetch/XHR\nThis will reveal only API calls made to servers. This includes internal servers that are hosted by the folks who run the website we‚Äôre inspecting, as well as external servers. The latter often includes third-party trackers used in adtech, and verification services to authenticate user behavior.\n\n%%HTML\n<video width=70% controls loop>\n  <source src=\"assets/filter-network.mov\" type=\"video/mp4\">\n</video>\n\n\n  \n\n\n\nYou might see quite a few network requests that were loaded onto the page. Look at ‚ÄúDomain‚Äù and ‚ÄúFile‚Äù to narrow down where requests were sent, and whether the names are telling of the purpose of the request.\nüí°Pro tip: you can ‚ÄúFilter URLs‚Äù using many different properties (see how to do this for Chrome and Firefox).\nIn this example, notice that a request was sent to the ‚ÄúDomain‚Äù completion.amazon.com, using an API endpoint (in the ‚ÄúFile‚Äù column) named suggestions. This is likely the API being called to populate autocompleted search suggestions on the Amazon marketplace.\nWhen clicking the network request, you‚Äôll see ‚ÄúHeaders‚Äù. Those are the HTTP headers that were sent along with the network request. This is not useful for us just yet, instead we want to see what data gets transferred as a result of the API call.\nTo do this, we‚Äôll look at the request‚Äôs ‚ÄúResponse‚Äù attributes.\n\n\n2.1.4 4. Analyze the response\nThis might seem intimidating at first, but let me key you in on some tips. Responses are almost always JSON-formatted. JSON is made up of lists and key-value pairs. This means the information is stored like a dictionary, with words and their corresponding definitions.\nLooking at the JSON response, it looks like Amazon‚Äôs completion.amazon.com/suggestions API returns a list of ‚Äúsuggestions‚Äù. Each item in the list of suggestions has a ‚Äúvalue‚Äù, in the example above that ‚Äúvalue‚Äù is spicy ramen.\nCheck your work: confirm this interpretation is correct by cross-referencing the API response with what a user would see on the website.\n\nGetting these steps down, is your one way ticket to spicy town, and you don‚Äôt need to code at all.\nHowever, some rudimentary coding can help you figure out how to use the API for vast inputs to collect your own dataset.\n\n\n2.1.5 5. Copy as cURL\nIf you find an HTTP request that returns a response with useful information you can start to reverse-engineer it. To do that, we can isolate it by right-clicking the HTTP request and selecting ‚Äúcopy as cURL‚Äù. (cURL stands for client URL, and is a tool used to tranfer data across networks.)\n\n\n\n2.1.6 6. Curl to requests\nWe can use a site like curlconverter.com to convert the cURL we copied into a reusable API call. In this example, we use the default conversion to a Python requests script. You can do the same for any language and framework, but we use this to demonstrate because it‚Äôs exactly what I do in practice.\nHere is what the converted cURL looks like after being converted to a Python request:\n\nimport requests\n\ncookies = {\n    'aws-ubid-main': '836-8365128-6734270',\n    'session-id-time': '2082787201l',\n    'ubid-main': '135-7086948-2591317',\n    'aws-priv': 'eyJ2IjoxLCJldSI6MCwic3QiOjB9',\n    'aws-target-static-id': '1593060129944-225088',\n    'lc-main': 'en_US',\n    'aws-userInfo': '%7B%22arn%22%3A%22arn%3Aaws%3Asts%3A%3A335340120612%3Aassumed-role%2FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%2Fleon.yin%40themarkup.org%22%2C%22alias%22%3A%22themarkup-journalist-sandbox%22%2C%22username%22%3A%22assumed-role%252FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%252Fleon.yin%2540themarkup.org%22%2C%22keybase%22%3A%22Fwu%2FzLIz0yD%2FyfrORX%2BSIpIQPXrzr0kt24uqa8mNs7g%5Cu003d%22%2C%22issuer%22%3A%22https%3A%2F%2Fd-90677ebe1e.awsapps.com%2Fstart%2F%23%2Fsaml%2Fcustom%2F335340120612%2520%2528AWS%2520Journalist%2520Sandbox%2529%2FMTI5NTE5MjY4NzI0X2lucy0wNzcxNmExOGZlNThiYjIyX3AtNDUyYTgyOTRhNTJlYWZjNQ%5Cu003d%5Cu003d%22%2C%22signinType%22%3A%22PUBLIC%22%7D',\n    'x-main': 'Oz3Tb5n2p0ic7OhF3cU5dc9B4ZR2gFjhKEsP4zikHHD3Gk2O7NpSmuShBxLFrhpZ',\n    'at-main': 'Atza|IwEBILB5ARQ_IgTCiBLam_XE2pyT76jXTbAXHOm2AJomLPmDgoJUJIIlUmyFeh_gChLHCycKjNlys-5CqqMabKieAzqSf607ChJsNevw-V06e7VKgcWjvoMaZRWlGiZ-c5wSJ-e4QzIWzAxTS1EI6sRUaRZRv-a0ZpOJQ-sHHB99006ytcrHhubdrXYPJRqEP5Q-_30JtESMpAkASoOs4vETSFp5BDBJfSWWETeotpIVXwA4NoC8E59bZb_5wHTW9cRBSWYGi1XL7CRl2xGbJaO2Gv3unuhGMB1tiq9iwxodSPBBTw',\n    'sess-at-main': '\"PUq9PW1TbO9CTYhGMo7l1Dz+wedh40Ki8Z9rPC+1TSI=\"',\n    'sst-main': 'Sst1|PQHsbeSFCMSY0X0_WgvTo5NUCaZkG2J9RPqWWy0fCpyWopJXgu6_drU_LstOdJB2cDmaVCXwkNpsF5yNPrBDj3Wtx-TC-AaYZn6WUdp8vNRPb6iYqxPAjRDnfK3pCnHqt19I0GoG7Bd1wnOxkAvnH0992IUq14kH6Ojm0J8noVPwMez0lltD-jxBwtDQ_EZYUkZG741RDVEojfziawJY9iKc-cLCnKmhi-ca1PPJnsimPV4lXRtMAGFbf9nMkKq4CbpkaRMdVtlPr20vF9eqg_V_-LY_V7S44WlO-_t_bFBnK8Q',\n    'i18n-prefs': 'USD',\n    'session-token': 'ptze73uznXExrMCSV9AklvNOKa1ND9F0rlQH2ioSM26Vr6hSheH8O4v4P8Lg3zuv7oDM+HZ+8f2TlyoPXUmPShprMXdvEpAQieXUw7+83PZOJvkkg1jwP0NiG0ZqksIYOr3Zuwt3omMcfCKRReWKxl5rGaDEM6AISpwI5aMDDCnA7fWbVO/QQYNxUZMifc599EZ5Fg3uGjCAhBlb6I7UO8ewRbXJ1bo9',\n    'session-id': '139-9925917-2023535',\n    'aws-userInfo-signed': 'eyJ0eXAiOiJKV1MiLCJrZXlSZWdpb24iOiJ1cy1lYXN0LTEiLCJhbGciOiJFUzM4NCIsImtpZCI6ImFhNDFkZjRjLTMxMzgtNGVkOC04YmU5LWYyMzUzYzNkOTEzYiJ9.eyJzdWIiOiJ0aGVtYXJrdXAtam91cm5hbGlzdC1zYW5kYm94Iiwic2lnbmluVHlwZSI6IlBVQkxJQyIsImlzcyI6Imh0dHBzOlwvXC9kLTkwNjc3ZWJlMWUuYXdzYXBwcy5jb21cL3N0YXJ0XC8jXC9zYW1sXC9jdXN0b21cLzMzNTM0MDEyMDYxMiUyMCUyOEFXUyUyMEpvdXJuYWxpc3QlMjBTYW5kYm94JTI5XC9NVEk1TlRFNU1qWTROekkwWDJsdWN5MHdOemN4Tm1FeE9HWmxOVGhpWWpJeVgzQXRORFV5WVRneU9UUmhOVEpsWVdaak5RPT0iLCJrZXliYXNlIjoiRnd1XC96TEl6MHlEXC95ZnJPUlgrU0lwSVFQWHJ6cjBrdDI0dXFhOG1OczdnPSIsImFybiI6ImFybjphd3M6c3RzOjozMzUzNDAxMjA2MTI6YXNzdW1lZC1yb2xlXC9BV1NSZXNlcnZlZFNTT19Qb3dlclVzZXJBY2Nlc3NfNGRjNTIzMjM5YTkwMTlkY1wvbGVvbi55aW5AdGhlbWFya3VwLm9yZyIsInVzZXJuYW1lIjoiYXNzdW1lZC1yb2xlJTJGQVdTUmVzZXJ2ZWRTU09fUG93ZXJVc2VyQWNjZXNzXzRkYzUyMzIzOWE5MDE5ZGMlMkZsZW9uLnlpbiU0MHRoZW1hcmt1cC5vcmcifQ.LWFZOJMDcYdu6od6Nk8TmhAFMGA9O98O4tIOsVlR7w5vAS_JgVixL8j75u6jTgjfWkdddhKqa5kgsXDmGNbjhzLIsD48ch1BUodlzxqeQfn0r8onIwLbUIHEnk6X-AJE',\n    'skin': 'noskin',\n}\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n    # 'Accept-Encoding': 'gzip, deflate, br',\n    'Origin': 'https://www.amazon.com',\n    'Connection': 'keep-alive',\n    'Referer': 'https://www.amazon.com/',\n   \n    'Sec-Fetch-Dest': 'empty',\n    'Sec-Fetch-Mode': 'cors',\n    'Sec-Fetch-Site': 'same-site',\n}\n\nparams = {\n    'limit': '11',\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'page-type': 'Gateway',\n    'alias': 'aps',\n    'site-variant': 'desktop',\n    'version': '3',\n    'event': 'onKeyPress',\n    'wc': '',\n    'lop': 'en_US',\n    'last-prefix': '\\0',\n    'avg-ks-time': '2486',\n    'fb': '1',\n    'session-id': '139-9925917-2023535',\n    'request-id': 'SVMTJXRDBQ9T8M7BRGNJ',\n    'mid': 'ATVPDKIKX0DER',\n    'plain-mid': '1',\n    'client-info': 'amazon-search-ui',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                        params=params, cookies=cookies, headers=headers)\n\nYou can run this Python code, as-is, and it should work.\n\n\n2.1.7 7. Strip it down\nYou might be overwhelmed with the parameters that go into this API request. Like the response output, the inputs are formatted like a JSON, too. Start removing these parameters one-by-one.\nI tend to keep a few essential ones for authentication, and also the parameters you care about changing for your own purposes. Notice that our example query of ‚Äúspicy‚Äù stored in the prefix parameter.\nüí°Pro tip: parameter values can expire, so test the request and each parameter to assure you only keep the shelf-stable parts.\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n}\n\nparams = {\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'alias': 'aps',\n    'plain-mid': '1',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', params=params, headers=headers)\n\n\n\n2.1.8 8. Recycle and reuse\nWith the stripped down request, try to submit a few‚Äî let‚Äôs say 10 or 20, requests with new parameters set by you.\nBelow, I converted the stripped down API call into a Python function that takes any keyword as input.\n\nimport pandas as pd\nimport time\n\ndef search_suggestions(keyword):\n    \"\"\"\n    Get autocompleted search suggestions for a `keyword` search on Amazon.com.\n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'Accept-Language': 'en-US,en;q=0.5',\n    }\n\n    params = {\n        'prefix': keyword,\n        'suggestion-type': [\n            'WIDGET',\n            'KEYWORD',\n        ],\n        'alias': 'aps',\n        'plain-mid': '1',\n    }\n\n    response = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                            params=params, headers=headers)\n    return response.json()\n\nHere we can set new input parameters in keyword, and make the an API call using each keyword.\n\nkeywords = [\n    'a', 'b', 'cookie', 'sock', 'zelda', '12'\n]\n\ndata = []\nfor keyword in keywords:\n    suggestions = search_suggestions(keyword)\n    suggestions['search_word'] = keyword # keep track of the seed keyword\n    time.sleep(1) # best practice to put some time between API calls.\n    data.extend(suggestions['suggestions'])\n\nWe save the API responses in a list called data, and put them into a Pandas DataFrame to analyze.\n\ndf = pd.DataFrame(data)\n\n# show 5 random auto suggestions\ndf.sample(5, random_state=303)\n\n\n\n\n\n  \n    \n      \n      suggType\n      type\n      value\n      refTag\n      candidateSources\n      strategyId\n      prior\n      ghost\n      help\n      queryUnderstandingFeatures\n    \n  \n  \n    \n      4\n      KeywordSuggestion\n      KEYWORD\n      asmanex twisthaler 30 inhaler\n      nb_sb_ss_i_5_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      13\n      KeywordSuggestion\n      KEYWORD\n      bathroom organizer\n      nb_sb_ss_i_4_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      19\n      KeywordSuggestion\n      KEYWORD\n      baby wipes\n      nb_sb_ss_i_10_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      12\n      KeywordSuggestion\n      KEYWORD\n      baby registry search\n      nb_sb_ss_i_3_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      16\n      KeywordSuggestion\n      KEYWORD\n      b013xkha4m b08xzrxczm b07xxphqzk b09rwjblc7\n      nb_sb_ss_i_7_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n  \n\n\n\n\nIf you look at the columns, you might be flooded with more questions: - Some terms may be blackListed, what does that mean and what words, if any, are blackListed = True? - Are some searches paid for, and not organic? - What is ghost?\nThis metadata is only visable from the API, and can lead to new story ideas and directions to pursue.\nUnfortunately, because this API is undocumented, asking these questions and figuring out what everything represents is difficult. Use your curiousity and look at many examples. The feature of the API is being able to make many queries at scale, which should help answer these questions. Reporting this out with sources is also essential in this process."
  },
  {
    "objectID": "posts/2022-07-05-finding-apis.html#do-it-yourself",
    "href": "posts/2022-07-05-finding-apis.html#do-it-yourself",
    "title": "2¬† Finding Undocumented APIs",
    "section": "2.2 Do it Yourself",
    "text": "2.2 Do it Yourself\nUse this space below to find another undocumented API and reuse it for a few other input parameters.\nRevisit the steps we outlined above, and apply them to a new website.\nEven if you aren‚Äôt a coder, try to get steps 1-6 ;)\nIf you are a coder, try some of the advanced usage below.\n\n2.2.1 For advanced usage‚Ä¶\n\nHandle errors for bad requests, rate limiting, and other issues that could arise.\nyou can use session instead of pure requests. This is helpful if cookies and authentication are involved. Read more about that here.\nyou can make a request asyncronous to speed up data collection (without overloading the site‚Äôs servers, of course).\nImplement steps 6-onwards in another programming language."
  },
  {
    "objectID": "posts/2022-07-05-finding-apis.html#homework-assignment",
    "href": "posts/2022-07-05-finding-apis.html#homework-assignment",
    "title": "2¬† Finding Undocumented APIs",
    "section": "2.3 Homework Assignment",
    "text": "2.3 Homework Assignment\nDry-run: Think about the websites you frequent and topics that interest you. Find an API in the wild, isolate it, and analyze some of its results.\nScoping: Determine how the API could be used to produce data to answer a reporting question or hypothesis.\nReporting: Figure out the ‚Äúsample‚Äù: how many inputs will you test and for what purpose. Determine the meaning and signifigance of hidden fields that are returned.\nUltimately undocumented APIs are a tool and data is useless without a purpose. Hopefully this worksheet helps you in your time of need.\nThanks!"
  },
  {
    "objectID": "posts/2022-07-05-finding-apis.html#contact",
    "href": "posts/2022-07-05-finding-apis.html#contact",
    "title": "2¬† Finding Undocumented APIs",
    "section": "2.4 Contact",
    "text": "2.4 Contact\nSuggestions, corrections, follow-ups, or cool work you want to share that uses undocumented APIs?\nemail me at leon@leonyin.org"
  },
  {
    "objectID": "undocumented-apis.html",
    "href": "undocumented-apis.html",
    "title": "Finding Undocumented APIs",
    "section": "",
    "text": "Intro\nWhen I pitched my first story at The Markup, I was (and still am) obsessed with YouTube. Not only is YouTube the largest video-hosting website worldwide, but it also paved the way for the creator economy.\nI wanted to better understand YouTube‚Äôs advertising system, especially how they treated hateful conspiracy theories, which at the time, thrived on the platform.\nTo do this, I got acqainted with the Google Ads portal. Anyone can sign-up, and see all the tools marketers use to reach users across the Google adverse. YouTube has a special section of the ad portal, where marketers can target their ads based on user demographics and the content of videos.\nAlong with Aaron Sankin, we investigated a specific targetting tool that allows ad-buyers to find and select specific videos and channels to place ads on, based on a search term.\nWe found that the racist ‚ÄúWhite Genocide‚Äù conspiracy theory returned no videos, but by removing spaces, we were returned results.\nThese initial tests suggested a keyword block list that hid results for certain keywords. However, we saw that swears and gibberish (‚Äúasdfoiasf‚Äù) also would surface no results. Based on what we saw, it was difficult to discern a clear signal between something that was blocked, and something that may have been too obscure to return any results.\nMy collegue Surya Mattu suggested using my web browser‚Äôs built-in developer tools to watch the network requests while we made searches in the portal. Doing so helped us isolate the API-endpoint being called during this process, and reverse-engineer it to return results for any given keyword. We found structural differences in the data returned from the API depending on Google‚Äôs verdict of the keyword.\nBlocked terms, returned an empty JSON packet of data {}, whereas obscure terms returned a JSON with labels but no results:\n{‚Äúvideos‚Äù: [], ‚Äúchannels‚Äù: []}\nWith this method in tow, we solicited help from civil rights groups and researchers to build keyword lists to test. We found found YouTube‚Äôs uneven enforcement of their advertising guidelines‚Äìblocking social and racial justice terms, while showing advertisers results for well-known hate terms.\nThis was my first time finding and using undocumented APIs, and it is an essential tool for data collection. Here we will introduce you to APIs, explain the difference between ‚Äúundocumented API‚Äùs and those that are documented, and we‚Äôll also share the key strengths that come with this technology through several case studies.\nLastly, we will go through a hands-on excercise finding and using an undocumented API in the wild.\nMore tutorials on the same subject:\nTopical and timeless:"
  },
  {
    "objectID": "undocumented-apis.html#what-is-an-a-p-i",
    "href": "undocumented-apis.html#what-is-an-a-p-i",
    "title": "Finding Undocumented APIs",
    "section": "What is an A-P-I?",
    "text": "What is an A-P-I?\nIf you ever tried to get your driver‚Äôs license at the Department of Motor Vehicles (DMV), needed a travel visa, or reimbursement for a business expense, you have experienced bureaucracy at its finest‚Äì a series of lines, forms, credential-showing, and waiting.\nYou bring the proper paperwork, wait in line, order, pay, and then, hopefully, you get what you came for. That is, only if you did everything exactly right.\nApplication program interfaces, or APIs, are digitzed bureaucracy. You make a request, and then wait in a queue to be served. However, instead of leaving with a driver‚Äôs license or a custom plate, what you‚Äôre waiting for is well-formatted data. As for making mistakes‚Ä¶ well, you‚Äôll get an automated response and zero sympathy.\nA great deal of the actions you might perform on digital applications or websites depend on APIs to authorize and record your requests, put you in a queue, and send display freshly queried information on a screen.\nSome APIs are well-documented.\n\nDocumented APIs\nMany businesses sell their services using APIs.\nThe benefit of documented APIs is self-explanitory, you know what you‚Äôre going to get, and there‚Äôs notes and examples to help other developers use the tool as intended.\nSome documented APIs are also free to use, making them a great tool for teaching and research. Unfortunately, these free APIs can often disappear, or their access severely limited ‚Äì as we‚Äôve seen with Twitter, YouTube, and Facebook.\nOther APIs are undocumented.\n\n\nUndocumented APIs\nThese are the silent heros making sure websites run, often times executing essential functions behind the scenes. Many of these essential functions are so mundane, you probably don‚Äôt even realize that something is happening.\nIf you kill time on social media platforms, you‚Äôll have noticed that the good times keep rolling. That is because ‚Äúinfinate scroll‚Äù is powered by an API that is called upon as you approach the bottom of the page to load more things to eat up your day.\nSimilarly, when you upload an image onto Instagram, an API will recieve that image, run a series of machine-learning models to categorize it, assure it is ‚ÄúOK‚Äù (an oversimplification, I know), and store that image and its metadata into a database.\nLearning how to find and use these hidden APIs opens up endless possibilies for reporting on and researching technology."
  },
  {
    "objectID": "undocumented-apis.html#how-have-documented-apis-been-used",
    "href": "undocumented-apis.html#how-have-documented-apis-been-used",
    "title": "Finding Undocumented APIs",
    "section": "How have documented APIs been used?",
    "text": "How have documented APIs been used?\n\nGender Shades (Buolamwini and Gebru 2018) was an audit of three commercially-available facial recognition APIs (from Microsoft, IBM, and Face++) used to automate gender classification. The authors created a benchmark image dataset of faces, and tested each facial recognition models by sending the same images through each model‚Äôs API. The authors found that many models had high error rates for female and Black faces, with the worst performance on Black female faces.\nGoogle‚Äôs Perspective API was developed to filter out toxic comments for publishers such as The New York Times. Importantly, Perspective used ‚Äútraining data‚Äù sourced from human-labelled Wikipedia edits. An academic study (Sap et al. 2019) found racially biased classifications of Tweets. For example, the use of certain identifiers for minority groups would flag a comment as toxic. Because the Google had released the API publicly, researchers could access and audit this API directly."
  },
  {
    "objectID": "undocumented-apis.html#how-have-undocumented-apis-been-used",
    "href": "undocumented-apis.html#how-have-undocumented-apis-been-used",
    "title": "Finding Undocumented APIs",
    "section": "How have undocumented APIs been used?",
    "text": "How have undocumented APIs been used?\nUndocumented APIs are instrumental to investigations and audits ranging from uncovering keyword blocklists, to identifying unmarked private label products, to being able to collect massive of quantities data representative of residents across major American cities.\nOften time open-source web scraping projects rely on these APIs, see Pyktok (Python Tiktok collector).\nUsing undocumented APIs has three key strengths:\n\nRichness: APIs often contain information that is not visable on web pages. This information is also stored in JSON, which makes web parsing a breeze.\nReliability: Often times these APIs execute essential functions. For that reason, they don‚Äôt change often. This makes them a reliable data source over time. Some companies routinely change class names and tags to break web scrapers that save HTML. This is less of a problem when you use an API.\nScalability: You can collect more information in less time using this method compared to headless browsers, such as Selenium, Puppeteer, and Playwright. (Not throwing shade‚Äìthese tools have their purpose, which we‚Äôll go over in a future section.)\n\nWe will cover three case studies, each of which is intended to highlight one of these benefits.\n\nCase study on richness: Google‚Äôs blocklist for YouTube advertisers\nI‚Äôm not going to re-hash this case study, since we led with it in the introduction, but‚Ä¶\nUsing undocumented APIs can reveal rich metadata. This includes hidden fields that are not displayed to everyday users of a website, as well as subtle changes to the structural in how data is returned.\nThese produces receipts you can follow by deciphering the meaning of these hidden fields, finding traces left by missing data, and identifying patterns that are otherwise hidden from surface (front-end) world.\nCetainly this was the case with the YouTube investigation, and something that we‚Äôll brush on again in the hands-on tutorial at the end of this section.\n\n\nCase study on reliability: Amazon branded products\nIf you scrape HTML from a website over a long period of time, you‚Äôll likely find yourself with a broken scraper.\nThis occurs when class names, accessibility labels, text, or something else has changed and confused your scraper. In this sense, HTML scraping can be fragile and fickle, especially if your collecting data persistently.\nIf you look at large platforms such as Facebook‚Äôs homepage, you‚Äôll see elements are arbitrarily named, oddly-nested, and ever-changing.\nUsing undocumented APIs can often get you the same information with a higher success-rate. This is because these APIs interact with the same backend (fetching information before being rendered, named, and nestled neatly into a webpage), and are often essential to the operation of the website.\nIn the investigation ‚ÄúAmazon‚Äôs Advantage‚Äù, Adrianne Jeffries and I found a reliable method of identifying Amazon brands and exclusive products. At the time, these products were not clearly labelled, most Americans we surveyed were unable to identify Amazon‚Äôs top brands, and no source of truth existed.\nWe developed a approach to identify these products as Amazon private label using a filter found in the user interface of the Amazon site. The ‚ÄúOur brands‚Äù filter did a lot of heavy lifting in our investigation, and we found that it was powered by an undocumented API that listed all the Amazon branded products for a given search.\nThis method was essential to our investigation and analysis, especially while we collected data over a period of several months. To our surprise, the API continued to work even after we went to Amazon to comment on our detailed methodology, after we published our investigation, and even after Amazon executives were accused of perjury by the U.S. Senate.\nUsually the party gets shut down once you call the parents, but in this case it didn‚Äôt.\nBecause the API continued to work, we used it in a browser extension (Amazon Brand Detector) that we (inclusing Ritu Ghiya and Jeff Crouse) built to highlight Amazon brands for Amazon shoppers around the globe. Eventually, Amazon added an orange disclaimer of ‚ÄúAmazon brand‚Äù (mirroring the orange stain from our extension) to their branded products, but the API and extension still work at the time of writing, more than a year later.\nThis case study emphasizes the reliability of using undocumented APIs, not only for collecting datasets, but for persistant accountability efforts.\n\n\nCase Study on scalability: Collecting Internet Plans\nIn the investigation, ‚ÄúStill Loading‚Äù my reporting partner Aaron Sankin and I collected and analyzed internet service plans across major cities in the United States.\nWe learned a technique from a trio of researchers from Princeton, that used the lookup tools found on the internet service providers‚Äô websites to retrieve internet plans for a specific address.\nHowever, doing this using a browser (as a real person would) is incredibly slow. Even with 10 automated browsers (see below) with unique IP addresses, it would have taken months to collect a representative sample of a single major American city.\n\n\n\n\nScraping AT&T using way too many Selenium browsers\n\nBrowser automation is bulky. Not only do you need to load every asset of a web page, there is also the compute resources necessary to spin up a browser. When you can get away without having to mock user interactions, or use rendered page elements, finding the underlying API(s) can be quicker and more eloquent.\nInitially, the workflow for getting an internet plan seemed too complex to pull off using an API‚Äì there was user authentication that set a cookie, choosing an address from a list of suggestions, and adding an apartment number when prompted.\nHowever, we were able to keep track of cookies using a session (read about this advanced topic here), and speed things up by bundling the sequence of successive API calls into a function.\nNot only was this function easier to write, but it was able to be written and executed asyncronously. Meaning we could request internet plans from many addresses at the same time.\nThis allowed us to collect AT&T internet plans for a representative sample of 21 cities in two days, rather than two years. Timely data collection is key. We found our story looking at data for one city. If we were unable to collect that data we would never had published our investigation that revealed widespread inequities in internet plans across the country.\nWhen it comes to web scraping, undocumented APIs offer unmatched scalability to collect massive amounts of data. This is especially true when you combine the them with asynchronous and multi-threaded programming (another topic we plan to cover in a future section)."
  },
  {
    "objectID": "undocumented-apis.html#case-study-on-richness-googles-blocklist-for-youtube-advertisers",
    "href": "undocumented-apis.html#case-study-on-richness-googles-blocklist-for-youtube-advertisers",
    "title": "What are Undocumented APIs",
    "section": "Case study on richness: Google‚Äôs blocklist for YouTube advertisers",
    "text": "Case study on richness: Google‚Äôs blocklist for YouTube advertisers\nI‚Äôm not going to re-hash this case study, since we led with it in the introduction, but‚Ä¶\nUsing undocumented APIs can reveal rich metadata. This includes hidden fields that are not displayed to everyday users of a website, as well as subtle changes to the structural in how data is returned.\nThese produces receipts you can follow by deciphering the meaning of these hidden fields, finding traces left by missing data, and identifying patterns that are otherwise hidden from surface (front-end) world.\nCetainly this was the case with the YouTube investigation, and something that we‚Äôll brush on again in the hands-on tutorial at the end of this section."
  },
  {
    "objectID": "undocumented-apis.html#case-study-on-reliability-amazon-branded-products",
    "href": "undocumented-apis.html#case-study-on-reliability-amazon-branded-products",
    "title": "What are Undocumented APIs",
    "section": "Case study on reliability: Amazon branded products",
    "text": "Case study on reliability: Amazon branded products\nIf you scrape HTML from a website over a long period of time, you‚Äôll likely find yourself with a broken scraper.\nThis occurs when class names, accessibility labels, text, or something else has changed and confused your scraper. In this sense, HTML scraping can be fragile and fickle, especially if your collecting data over multiple days.\nIf you look at large platforms such as Facebook‚Äôs homepage, you‚Äôll see elements are arbitrarily named, oddly-nested, and ever-changing.\nUsing undocumented APIs can often get you the same information with a higher success-rate. This is because these APIs interact with the same backend (fetching information before being rendered, named, and nestled neatly into a webpage), and are often essential to the operation of the website.\nIn the investigation ‚ÄúAmazon‚Äôs Advantage‚Äù, Adrianne Jeffries and I found a reliable method of identifying Amazon brands and exclusive products. At the time, these products were not clearly labelled, most Americans we surveyed were unable to identify Amazon‚Äôs top brands, and no source of truth existed.\nWe developed a approach to identify these products as Amazon private label using a filter found in the user interface of the Amazon site. The ‚ÄúOur brands‚Äù filter did a lot of heavy lifting in our investigation, and we found that it was powered by an undocumented API that listed all the Amazon branded products for a given search.\nThis method was essential to our investigation and analysis, especially while we collected data over a period of several months. To our surprise, the API continued to work even after we went to Amazon to comment on our detailed methodology, after we published our investigation, and even after Amazon executives were questioned for perjury by the U.S. Senate.\nUsually the party gets shut down once you call the parents, but in this case it didn‚Äôt.\nBecause the API continued to work, we used it in a browser extension (Amazon Brand Detector) that my small team built to highlight Amazon brands for Amazon shoppers around the globe. Eventually, Amazon added an orange disclaimer of ‚ÄúAmazon brand‚Äù (mirroring the orange stain from our extension) to their branded products, but the API and extension still work at the time of writing, more than a year later.\nThis case study emphasizes the reliability of using undocumented APIs, not only for collecting datasets, but for persistant accountability efforts."
  },
  {
    "objectID": "undocumented-apis.html#case-study-on-scalability-collecting-internet-plans",
    "href": "undocumented-apis.html#case-study-on-scalability-collecting-internet-plans",
    "title": "What are Undocumented APIs",
    "section": "Case Study on scalability: Collecting Internet Plans",
    "text": "Case Study on scalability: Collecting Internet Plans\nIn the investigation, ‚ÄúStill Loading‚Äù my reporting partner Aaron Sankin and I collected and analyzed internet service plans across major cities in the United States.\nWe learned a technique from a trio of researchers from Princeton, that used the lookup tools found on the internet service providers‚Äô websites to retrieve internet plans for a specific address.\nHowever, doing this using a browser (as a real person would) is incredibly slow. Even with 10 automated browsers (see below) with unique IP addresses, it would have taken months to collect a representative sample of a single major American city.\n\n%%HTML\n<video width=100% controls loop>\n  <source src=\"assets/att-scraper-selenium.mp4\" type=\"video/mp4\">\n</video>\n\n\n  \n\n\n\nBrowser automation is bulky. Not only do you need to load every asset of a web page, there is also the compute resources necessary to spin up a browser. When you can get away without having to mock user interactions, or use rendered page elements, finding the underlying API(s) can be quicker and more eloquent.\nInitially, the workflow for getting an internet plan seemed too complex to pull off using an API‚Äì there was user authentication that set a cookie, choosing an address from a list of suggestions, and adding an apartment number when prompted.\nHowever, we were able to keep track of cookies using a session (read about this advanced topic here), and speed things up by bundling the sequence of successive API calls into a function.\nNot only was this function easier to write, but it was able to be written and executed asyncronously. Meaning we could request internet plans from many addresses at the same time.\nThis allowed us to collect AT&T internet plans for a representative sample of 21 cities in two days, rather than two years. Timely data collection is key. We found our story looking at data for one city. If we were unable to collect that data we would never had published our investigation that revealed widespread inequities in internet plans across the country.\nWhen it comes to web scraping, undocumented APIs offer unmatched scalability to collect massive amounts of data. This is especially true when you combine the them with asynchronous and multi-threaded programming (another topic we plan to cover in a future section)."
  },
  {
    "objectID": "undocumented-apis.html#related-readings",
    "href": "undocumented-apis.html#related-readings",
    "title": "What are Undocumented APIs",
    "section": "Related readings",
    "text": "Related readings\nMore tutorials on the same subject:\n\n‚ÄúScraping XHR‚Äù - Sam Lavigne\n‚ÄúWeb Scraping 201: finding the API‚Äù - Greg Reda\n\nTopical and timeless:\n\n‚ÄúComputational research in the post-API age‚Äù - Deen Freelon"
  },
  {
    "objectID": "finding-apis.html#how-to-find-apis",
    "href": "finding-apis.html#how-to-find-apis",
    "title": "Appendix A ‚Äî Finding Undocumented APIs",
    "section": "How to find APIs?",
    "text": "How to find APIs?\nMost APIs are undocumented, hidden in plain sight.\nYou can use these APIs to collect datasets for investigations, audits, and tools.\nYou can sniff them out using a web browser‚Äôs developer tools (shortened to dev tools).\nNote that if you‚Äôre in a workshop setting: hitting the example API at the same time will get us all blocked from the website!\n\n1. First open the developer console.\nSee how on Chrome or Firefox here.\nIn this tutorial, we‚Äôll see how Amazon.com autocomplete search suggestions work.\nMy go-to method, (regardless of browser,) is to right-click and ‚ÄúInspect‚Äù an element on the page.\n\n\n\nExample of inspecting an element on a page using a right-click\n\n\nThis will open the dev tools under the ‚ÄúElements‚Äù tab, which is used to explore the source code of a page.\nPage source code is useful because it reveals useful clues that are otherwise unseen by regular users. Often times, clues are in accessibility features known as ARIA elements.\nHowever, this tutorial is not about source code‚Ä¶ it‚Äôs about API requests that populate what we see on the page, and the hidden fields that we don‚Äôt see.\nLet‚Äôs try this!\nWith dev tools open, go to Amazon.com, select the search bar on the website, and start typing a query (such as ‚Äúspicy‚Äù).\n\n\n2. Click the ‚ÄúNetwork‚Äù tab.\nThis section of the dev tools is used to monitor network requests.\nBackground\nEverything on a page is retrieved from some outside source, likely a server. This includes things like images embedded on the page, JavaScript code running in the background, and all the bits of ‚Äúcontent‚Äù that populate the page before us.\nUsing the network tab, we can find out how this information is requested from a server, and intercept the response before it is rendered on the page.\nThese responses are information-rich, and contain fields that don‚Äôt end up in the source code or in the user interface that most people encounter when they visit a site.\nFurther, we can reverse-engineer how this request is made, and use it to collect structured data at scale. This is the power of finding undocumented APIs.\nBack to the console‚Ä¶\nThe network tab can look pretty hectic at first. It has many uses, and a lot of information. We‚Äôll cover some of the basics.\n\n\n\n\n\n3. Filter requests by fetch/XHR\nThis will reveal only API calls made to servers. This includes internal servers that are hosted by the folks who run the website we‚Äôre inspecting, as well as external servers. The latter often includes third-party trackers used in adtech, and verification services to authenticate user behavior.\n\n\n\nYou might see quite a few network requests that were loaded onto the page. Look at ‚ÄúDomain‚Äù and ‚ÄúFile‚Äù to narrow down where requests were sent, and whether the names are telling of the purpose of the request.\nüí°Pro tip: you can ‚ÄúFilter URLs‚Äù using many different properties (see how to do this for Chrome and Firefox).\nIn this example, notice that a request was sent to the ‚ÄúDomain‚Äù completion.amazon.com, using an API endpoint (in the ‚ÄúFile‚Äù column) named suggestions. This is likely the API being called to populate autocompleted search suggestions on the Amazon marketplace.\nWhen clicking the network request, you‚Äôll see ‚ÄúHeaders‚Äù. Those are the HTTP headers that were sent along with the network request. This is not useful for us just yet, instead we want to see what data gets transferred as a result of the API call.\nTo do this, we‚Äôll look at the request‚Äôs ‚ÄúResponse‚Äù attributes.\n\n\n4. Analyze the response\nThis might seem intimidating at first, but let me key you in on some tips. Responses are almost always JSON-formatted. JSON is made up of lists and key-value pairs. This means the information is stored like a dictionary, with words and their corresponding definitions.\nLooking at the JSON response, it looks like Amazon‚Äôs completion.amazon.com/suggestions API returns a list of ‚Äúsuggestions‚Äù. Each item in the list of suggestions has a ‚Äúvalue‚Äù, in the example above that ‚Äúvalue‚Äù is spicy ramen.\nCheck your work: confirm this interpretation is correct by cross-referencing the API response with what a user would see on the website.\n\n\n\nAmazon‚Äôs suggestions for ‚Äúspicy‚Äù.\n\n\nGetting these steps down, is your one way ticket to spicy town, and you don‚Äôt need to code at all.\nHowever, some rudimentary coding can help you figure out how to use the API for vast inputs to collect your own dataset.\n\n\n5. Copy as cURL\nIf you find an HTTP request that returns a response with useful information you can start to reverse-engineer it. To do that, we can isolate it by right-clicking the HTTP request and selecting ‚Äúcopy as cURL‚Äù. (cURL stands for client URL, and is a tool used to transfer data across networks.)\n\n\n\n6. Curl to requests\nWe can use a site like curlconverter.com to convert the cURL we copied into a reusable API call. In this example, we use the default conversion to a Python requests script. You can do the same for any language and framework, but we use this to demonstrate because it‚Äôs exactly what I do in practice.\nHere is what the converted cURL looks like after being converted to a Python request:\n\nimport requests\n\ncookies = {\n    'aws-ubid-main': '836-8365128-6734270',\n    'session-id-time': '2082787201l',\n    'ubid-main': '135-7086948-2591317',\n    'aws-priv': 'eyJ2IjoxLCJldSI6MCwic3QiOjB9',\n    'aws-target-static-id': '1593060129944-225088',\n    'lc-main': 'en_US',\n    'aws-userInfo': '%7B%22arn%22%3A%22arn%3Aaws%3Asts%3A%3A335340120612%3Aassumed-role%2FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%2Fleon.yin%40themarkup.org%22%2C%22alias%22%3A%22themarkup-journalist-sandbox%22%2C%22username%22%3A%22assumed-role%252FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%252Fleon.yin%2540themarkup.org%22%2C%22keybase%22%3A%22Fwu%2FzLIz0yD%2FyfrORX%2BSIpIQPXrzr0kt24uqa8mNs7g%5Cu003d%22%2C%22issuer%22%3A%22https%3A%2F%2Fd-90677ebe1e.awsapps.com%2Fstart%2F%23%2Fsaml%2Fcustom%2F335340120612%2520%2528AWS%2520Journalist%2520Sandbox%2529%2FMTI5NTE5MjY4NzI0X2lucy0wNzcxNmExOGZlNThiYjIyX3AtNDUyYTgyOTRhNTJlYWZjNQ%5Cu003d%5Cu003d%22%2C%22signinType%22%3A%22PUBLIC%22%7D',\n    'x-main': 'Oz3Tb5n2p0ic7OhF3cU5dc9B4ZR2gFjhKEsP4zikHHD3Gk2O7NpSmuShBxLFrhpZ',\n    'at-main': 'Atza|IwEBILB5ARQ_IgTCiBLam_XE2pyT76jXTbAXHOm2AJomLPmDgoJUJIIlUmyFeh_gChLHCycKjNlys-5CqqMabKieAzqSf607ChJsNevw-V06e7VKgcWjvoMaZRWlGiZ-c5wSJ-e4QzIWzAxTS1EI6sRUaRZRv-a0ZpOJQ-sHHB99006ytcrHhubdrXYPJRqEP5Q-_30JtESMpAkASoOs4vETSFp5BDBJfSWWETeotpIVXwA4NoC8E59bZb_5wHTW9cRBSWYGi1XL7CRl2xGbJaO2Gv3unuhGMB1tiq9iwxodSPBBTw',\n    'sess-at-main': '\"PUq9PW1TbO9CTYhGMo7l1Dz+wedh40Ki8Z9rPC+1TSI=\"',\n    'sst-main': 'Sst1|PQHsbeSFCMSY0X0_WgvTo5NUCaZkG2J9RPqWWy0fCpyWopJXgu6_drU_LstOdJB2cDmaVCXwkNpsF5yNPrBDj3Wtx-TC-AaYZn6WUdp8vNRPb6iYqxPAjRDnfK3pCnHqt19I0GoG7Bd1wnOxkAvnH0992IUq14kH6Ojm0J8noVPwMez0lltD-jxBwtDQ_EZYUkZG741RDVEojfziawJY9iKc-cLCnKmhi-ca1PPJnsimPV4lXRtMAGFbf9nMkKq4CbpkaRMdVtlPr20vF9eqg_V_-LY_V7S44WlO-_t_bFBnK8Q',\n    'i18n-prefs': 'USD',\n    'session-token': 'ptze73uznXExrMCSV9AklvNOKa1ND9F0rlQH2ioSM26Vr6hSheH8O4v4P8Lg3zuv7oDM+HZ+8f2TlyoPXUmPShprMXdvEpAQieXUw7+83PZOJvkkg1jwP0NiG0ZqksIYOr3Zuwt3omMcfCKRReWKxl5rGaDEM6AISpwI5aMDDCnA7fWbVO/QQYNxUZMifc599EZ5Fg3uGjCAhBlb6I7UO8ewRbXJ1bo9',\n    'session-id': '139-9925917-2023535',\n    'aws-userInfo-signed': 'eyJ0eXAiOiJKV1MiLCJrZXlSZWdpb24iOiJ1cy1lYXN0LTEiLCJhbGciOiJFUzM4NCIsImtpZCI6ImFhNDFkZjRjLTMxMzgtNGVkOC04YmU5LWYyMzUzYzNkOTEzYiJ9.eyJzdWIiOiJ0aGVtYXJrdXAtam91cm5hbGlzdC1zYW5kYm94Iiwic2lnbmluVHlwZSI6IlBVQkxJQyIsImlzcyI6Imh0dHBzOlwvXC9kLTkwNjc3ZWJlMWUuYXdzYXBwcy5jb21cL3N0YXJ0XC8jXC9zYW1sXC9jdXN0b21cLzMzNTM0MDEyMDYxMiUyMCUyOEFXUyUyMEpvdXJuYWxpc3QlMjBTYW5kYm94JTI5XC9NVEk1TlRFNU1qWTROekkwWDJsdWN5MHdOemN4Tm1FeE9HWmxOVGhpWWpJeVgzQXRORFV5WVRneU9UUmhOVEpsWVdaak5RPT0iLCJrZXliYXNlIjoiRnd1XC96TEl6MHlEXC95ZnJPUlgrU0lwSVFQWHJ6cjBrdDI0dXFhOG1OczdnPSIsImFybiI6ImFybjphd3M6c3RzOjozMzUzNDAxMjA2MTI6YXNzdW1lZC1yb2xlXC9BV1NSZXNlcnZlZFNTT19Qb3dlclVzZXJBY2Nlc3NfNGRjNTIzMjM5YTkwMTlkY1wvbGVvbi55aW5AdGhlbWFya3VwLm9yZyIsInVzZXJuYW1lIjoiYXNzdW1lZC1yb2xlJTJGQVdTUmVzZXJ2ZWRTU09fUG93ZXJVc2VyQWNjZXNzXzRkYzUyMzIzOWE5MDE5ZGMlMkZsZW9uLnlpbiU0MHRoZW1hcmt1cC5vcmcifQ.LWFZOJMDcYdu6od6Nk8TmhAFMGA9O98O4tIOsVlR7w5vAS_JgVixL8j75u6jTgjfWkdddhKqa5kgsXDmGNbjhzLIsD48ch1BUodlzxqeQfn0r8onIwLbUIHEnk6X-AJE',\n    'skin': 'noskin',\n}\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n    # 'Accept-Encoding': 'gzip, deflate, br',\n    'Origin': 'https://www.amazon.com',\n    'Connection': 'keep-alive',\n    'Referer': 'https://www.amazon.com/',\n   \n    'Sec-Fetch-Dest': 'empty',\n    'Sec-Fetch-Mode': 'cors',\n    'Sec-Fetch-Site': 'same-site',\n}\n\nparams = {\n    'limit': '11',\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'page-type': 'Gateway',\n    'alias': 'aps',\n    'site-variant': 'desktop',\n    'version': '3',\n    'event': 'onKeyPress',\n    'wc': '',\n    'lop': 'en_US',\n    'last-prefix': '\\0',\n    'avg-ks-time': '2486',\n    'fb': '1',\n    'session-id': '139-9925917-2023535',\n    'request-id': 'SVMTJXRDBQ9T8M7BRGNJ',\n    'mid': 'ATVPDKIKX0DER',\n    'plain-mid': '1',\n    'client-info': 'amazon-search-ui',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                        params=params, cookies=cookies, headers=headers)\n\nYou can run this Python code, as-is, and it should work.\n\n\n7. Strip it down\nYou might be overwhelmed with the parameters that go into this API request. Like the response output, the inputs are formatted like a JSON, too. Start removing these parameters one-by-one.\nI tend to keep a few essential ones for authentication, and also the parameters you care about changing for your own purposes. Notice that our example query of ‚Äúspicy‚Äù stored in the prefix parameter.\nüí°Pro tip: parameter values can expire, so test the request and each parameter to assure you only keep the shelf-stable parts.\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n}\n\nparams = {\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'alias': 'aps',\n    'plain-mid': '1',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', params=params, headers=headers)\n\n\n\n8. Recycle and reuse\nWith the stripped down request, try to submit a few‚Äî let‚Äôs say 10 or 20, requests with new parameters set by you.\nBelow, I converted the stripped down API call into a Python function that takes any keyword as input.\n\nimport pandas as pd\nimport time\n\ndef search_suggestions(keyword):\n    \"\"\"\n    Get autocompleted search suggestions for a `keyword` search on Amazon.com.\n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'Accept-Language': 'en-US,en;q=0.5',\n    }\n\n    params = {\n        'prefix': keyword,\n        'suggestion-type': [\n            'WIDGET',\n            'KEYWORD',\n        ],\n        'alias': 'aps',\n        'plain-mid': '1',\n    }\n\n    response = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                            params=params, headers=headers)\n    return response.json()\n\nHere we can set new input parameters in keyword, and make the an API call using each keyword.\n\nkeywords = [\n    'a', 'b', 'cookie', 'sock', 'zelda', '12'\n]\n\ndata = []\nfor keyword in keywords:\n    suggestions = search_suggestions(keyword)\n    suggestions['search_word'] = keyword # keep track of the seed keyword\n    time.sleep(1) # best practice to put some time between API calls.\n    data.extend(suggestions['suggestions'])\n\nWe save the API responses in a list called data, and put them into a Pandas DataFrame to analyze.\n\ndf = pd.DataFrame(data)\n\n# show 5 random auto suggestions\ndf.sample(5, random_state=303)\n\n\n\n\n\n  \n    \n      \n      suggType\n      type\n      value\n      refTag\n      candidateSources\n      strategyId\n      prior\n      ghost\n      help\n      queryUnderstandingFeatures\n    \n  \n  \n    \n      4\n      KeywordSuggestion\n      KEYWORD\n      asmanex twisthaler 30 inhaler\n      nb_sb_ss_i_5_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      13\n      KeywordSuggestion\n      KEYWORD\n      bathroom organizer\n      nb_sb_ss_i_4_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      19\n      KeywordSuggestion\n      KEYWORD\n      baby wipes\n      nb_sb_ss_i_10_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      12\n      KeywordSuggestion\n      KEYWORD\n      baby registry search\n      nb_sb_ss_i_3_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      16\n      KeywordSuggestion\n      KEYWORD\n      b013xkha4m b08xzrxczm b07xxphqzk b09rwjblc7\n      nb_sb_ss_i_7_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n  \n\n\n\n\nIf you look at the columns, you might be flooded with more questions:\n\nSome terms may be blackListed, what does that mean and what words, if any, are blackListed = True?\nAre some searches paid for, and not organic?\nWhat is ghost?\n\nThis metadata is only visable from the API, and can lead to new story ideas and directions to pursue.\nUnfortunately, because this API is undocumented, asking these questions and figuring out what everything represents is difficult. Use your curiousity and look at many examples. The feature of the API is being able to make many queries at scale, which should help answer these questions. Reporting this out with sources is also essential in this process."
  },
  {
    "objectID": "finding-apis.html#do-it-yourself",
    "href": "finding-apis.html#do-it-yourself",
    "title": "Appendix A ‚Äî Finding Undocumented APIs",
    "section": "Do it Yourself",
    "text": "Do it Yourself\nUse this space below to find another undocumented API and reuse it for a few other input parameters.\nRevisit the steps we outlined above, and apply them to a new website.\nIf you are not a coder, try to get steps 1-6.\nIf you are a coder, try some of the advanced usage below.\n\nFor advanced usage‚Ä¶\n\nHandle errors for bad requests, rate limiting, and other issues that could arise.\nyou can use session instead of pure requests. This is helpful if cookies and authentication are involved. Read more about that here.\nyou can make a request asyncronous to speed up data collection (without overloading the site‚Äôs servers, of course).\nImplement steps 6-onwards in another programming language."
  },
  {
    "objectID": "finding-apis.html#homework-assignment",
    "href": "finding-apis.html#homework-assignment",
    "title": "Appendix A ‚Äî Finding Undocumented APIs",
    "section": "Homework Assignment",
    "text": "Homework Assignment\nDry-run: Think about the websites you frequent and topics that interest you. Find an API in the wild, isolate it, and analyze some of its results.\nScoping: Determine how the API could be used to produce data to answer a reporting question or hypothesis.\nReporting: Figure out the ‚Äúsample‚Äù: how many inputs will you test and for what purpose. Determine the meaning and signifigance of hidden fields that are returned.\nUltimately undocumented APIs are a tool and data is useless without a purpose. Hopefully this worksheet helps you in your time of need."
  },
  {
    "objectID": "finding-apis.html#contact",
    "href": "finding-apis.html#contact",
    "title": "Appendix A ‚Äî Finding Undocumented APIs",
    "section": "Contact",
    "text": "Contact\nSuggestions, corrections, follow-ups, or cool work you want to share that uses undocumented APIs?\nemail me at leon@leonyin.org"
  },
  {
    "objectID": "_notebooks/finding-apis.html#how-to-find-apis",
    "href": "_notebooks/finding-apis.html#how-to-find-apis",
    "title": "Finding Undocumented APIs",
    "section": "How to find APIs?",
    "text": "How to find APIs?\nMost APIs are undocumented, hidden in plain sight.\nYou can use these API‚Äôs to collect datasets for investigations, audits, and tools.\nYou can sniff them out using a web browser‚Äôs developer tools (shortened to dev tools).\nNote that if you‚Äôre in a workshop setting: hitting the example API at the same time will get us all blocked from the website!\n\n1. First open the developer console.\nSee how on Chrome or Firefox here.\nIn this tutorial, we‚Äôll see how Amazon.com autocomplete search suggestions work.\nMy go-to method, (regardless of browser,) is to right-click and ‚ÄúInspect‚Äù an element on the page.\n\n\n\npanel\n\n\nThis will open the dev tools under the ‚ÄúElements‚Äù tab, which is used to explore the source code of a page.\nPage source code is useful because it reveals useful clues that are otherwise unseen by regular users. Often times, clues are in accessibility features known as ARIA elements.\nHowever, this tutorial is not about source code‚Ä¶ it‚Äôs about API requests that populate what we see on the page, and the hidden fields that we don‚Äôt see.\nLet‚Äôs try this!\nWith dev tools open, go to Amazon.com, select the search bar on the website, and start typing a query (such as ‚Äúspicy‚Äù).\n\n\n2. Click the ‚ÄúNetwork‚Äù tab.\nThis section of the dev tools is used to monitor network requests.\nBackground\nEverything on a page is retrieved from some outside source, likely a server. This includes things like images embedded on the page, JavaScript code running in the background, and all the bits of ‚Äúcontent‚Äù that populate the page before us.\nUsing the network tab, we can find out how this information is requested from a server, and intercept the response before it is rendered on the page.\nThese responses are information-rich, and contain fields that don‚Äôt end up in the source code or in the user interface that most people encounter when they visit a site.\nFurther, we can reverse-engineer how this request is made, and use it to collect structured data at scale. This is the power of finding undocumented APIs.\nBack to the console‚Ä¶\nThe network tab can look pretty hectic at first. It has many uses, and a lot of information. We‚Äôll cover some of the basics.\n\n%%HTML\n<video width=70% controls loop>\n  <source src=\"assets/dev-console.mov\" type=\"video/mp4\">\n</video>\n\n\n  \n\n\n\n\n\n3. Filter requests by fetch/XHR\nThis will reveal only API calls made to servers. This includes internal servers that are hosted by the folks who run the website we‚Äôre inspecting, as well as external servers. The latter often includes third-party trackers used in adtech, and verification services to authenticate user behavior.\n\n%%HTML\n<video width=70% controls loop>\n  <source src=\"assets/filter-network.mov\" type=\"video/mp4\">\n</video>\n\n\n  \n\n\n\nYou might see quite a few network requests that were loaded onto the page. Look at ‚ÄúDomain‚Äù and ‚ÄúFile‚Äù to narrow down where requests were sent, and whether the names are telling of the purpose of the request.\nüí°Pro tip: you can ‚ÄúFilter URLs‚Äù using many different properties (see how to do this for Chrome and Firefox).\nIn this example, notice that a request was sent to the ‚ÄúDomain‚Äù completion.amazon.com, using an API endpoint (in the ‚ÄúFile‚Äù column) named suggestions. This is likely the API being called to populate autocompleted search suggestions on the Amazon marketplace.\nWhen clicking the network request, you‚Äôll see ‚ÄúHeaders‚Äù. Those are the HTTP headers that were sent along with the network request. This is not useful for us just yet, instead we want to see what data gets transferred as a result of the API call.\nTo do this, we‚Äôll look at the request‚Äôs ‚ÄúResponse‚Äù attributes.\n\n\n4. Analyze the response\nThis might seem intimidating at first, but let me key you in on some tips. Responses are almost always JSON-formatted. JSON is made up of lists and key-value pairs. This means the information is stored like a dictionary, with words and their corresponding definitions.\nLooking at the JSON response, it looks like Amazon‚Äôs completion.amazon.com/suggestions API returns a list of ‚Äúsuggestions‚Äù. Each item in the list of suggestions has a ‚Äúvalue‚Äù, in the example above that ‚Äúvalue‚Äù is spicy ramen.\nCheck your work: confirm this interpretation is correct by cross-referencing the API response with what a user would see on the website.\n\nGetting these steps down, is your one way ticket to spicy town, and you don‚Äôt need to code at all.\nHowever, some rudimentary coding can help you figure out how to use the API for vast inputs to collect your own dataset.\n\n\n5. Copy as cURL\nIf you find an HTTP request that returns a response with useful information you can start to reverse-engineer it. To do that, we can isolate it by right-clicking the HTTP request and selecting ‚Äúcopy as cURL‚Äù. (cURL stands for client URL, and is a tool used to tranfer data across networks.)\n\n\n\n6. Curl to requests\nWe can use a site like curlconverter.com to convert the cURL we copied into a reusable API call. In this example, we use the default conversion to a Python requests script. You can do the same for any language and framework, but we use this to demonstrate because it‚Äôs exactly what I do in practice.\nHere is what the converted cURL looks like after being converted to a Python request:\n\nimport requests\n\ncookies = {\n    'aws-ubid-main': '836-8365128-6734270',\n    'session-id-time': '2082787201l',\n    'ubid-main': '135-7086948-2591317',\n    'aws-priv': 'eyJ2IjoxLCJldSI6MCwic3QiOjB9',\n    'aws-target-static-id': '1593060129944-225088',\n    'lc-main': 'en_US',\n    'aws-userInfo': '%7B%22arn%22%3A%22arn%3Aaws%3Asts%3A%3A335340120612%3Aassumed-role%2FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%2Fleon.yin%40themarkup.org%22%2C%22alias%22%3A%22themarkup-journalist-sandbox%22%2C%22username%22%3A%22assumed-role%252FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%252Fleon.yin%2540themarkup.org%22%2C%22keybase%22%3A%22Fwu%2FzLIz0yD%2FyfrORX%2BSIpIQPXrzr0kt24uqa8mNs7g%5Cu003d%22%2C%22issuer%22%3A%22https%3A%2F%2Fd-90677ebe1e.awsapps.com%2Fstart%2F%23%2Fsaml%2Fcustom%2F335340120612%2520%2528AWS%2520Journalist%2520Sandbox%2529%2FMTI5NTE5MjY4NzI0X2lucy0wNzcxNmExOGZlNThiYjIyX3AtNDUyYTgyOTRhNTJlYWZjNQ%5Cu003d%5Cu003d%22%2C%22signinType%22%3A%22PUBLIC%22%7D',\n    'x-main': 'Oz3Tb5n2p0ic7OhF3cU5dc9B4ZR2gFjhKEsP4zikHHD3Gk2O7NpSmuShBxLFrhpZ',\n    'at-main': 'Atza|IwEBILB5ARQ_IgTCiBLam_XE2pyT76jXTbAXHOm2AJomLPmDgoJUJIIlUmyFeh_gChLHCycKjNlys-5CqqMabKieAzqSf607ChJsNevw-V06e7VKgcWjvoMaZRWlGiZ-c5wSJ-e4QzIWzAxTS1EI6sRUaRZRv-a0ZpOJQ-sHHB99006ytcrHhubdrXYPJRqEP5Q-_30JtESMpAkASoOs4vETSFp5BDBJfSWWETeotpIVXwA4NoC8E59bZb_5wHTW9cRBSWYGi1XL7CRl2xGbJaO2Gv3unuhGMB1tiq9iwxodSPBBTw',\n    'sess-at-main': '\"PUq9PW1TbO9CTYhGMo7l1Dz+wedh40Ki8Z9rPC+1TSI=\"',\n    'sst-main': 'Sst1|PQHsbeSFCMSY0X0_WgvTo5NUCaZkG2J9RPqWWy0fCpyWopJXgu6_drU_LstOdJB2cDmaVCXwkNpsF5yNPrBDj3Wtx-TC-AaYZn6WUdp8vNRPb6iYqxPAjRDnfK3pCnHqt19I0GoG7Bd1wnOxkAvnH0992IUq14kH6Ojm0J8noVPwMez0lltD-jxBwtDQ_EZYUkZG741RDVEojfziawJY9iKc-cLCnKmhi-ca1PPJnsimPV4lXRtMAGFbf9nMkKq4CbpkaRMdVtlPr20vF9eqg_V_-LY_V7S44WlO-_t_bFBnK8Q',\n    'i18n-prefs': 'USD',\n    'session-token': 'ptze73uznXExrMCSV9AklvNOKa1ND9F0rlQH2ioSM26Vr6hSheH8O4v4P8Lg3zuv7oDM+HZ+8f2TlyoPXUmPShprMXdvEpAQieXUw7+83PZOJvkkg1jwP0NiG0ZqksIYOr3Zuwt3omMcfCKRReWKxl5rGaDEM6AISpwI5aMDDCnA7fWbVO/QQYNxUZMifc599EZ5Fg3uGjCAhBlb6I7UO8ewRbXJ1bo9',\n    'session-id': '139-9925917-2023535',\n    'aws-userInfo-signed': 'eyJ0eXAiOiJKV1MiLCJrZXlSZWdpb24iOiJ1cy1lYXN0LTEiLCJhbGciOiJFUzM4NCIsImtpZCI6ImFhNDFkZjRjLTMxMzgtNGVkOC04YmU5LWYyMzUzYzNkOTEzYiJ9.eyJzdWIiOiJ0aGVtYXJrdXAtam91cm5hbGlzdC1zYW5kYm94Iiwic2lnbmluVHlwZSI6IlBVQkxJQyIsImlzcyI6Imh0dHBzOlwvXC9kLTkwNjc3ZWJlMWUuYXdzYXBwcy5jb21cL3N0YXJ0XC8jXC9zYW1sXC9jdXN0b21cLzMzNTM0MDEyMDYxMiUyMCUyOEFXUyUyMEpvdXJuYWxpc3QlMjBTYW5kYm94JTI5XC9NVEk1TlRFNU1qWTROekkwWDJsdWN5MHdOemN4Tm1FeE9HWmxOVGhpWWpJeVgzQXRORFV5WVRneU9UUmhOVEpsWVdaak5RPT0iLCJrZXliYXNlIjoiRnd1XC96TEl6MHlEXC95ZnJPUlgrU0lwSVFQWHJ6cjBrdDI0dXFhOG1OczdnPSIsImFybiI6ImFybjphd3M6c3RzOjozMzUzNDAxMjA2MTI6YXNzdW1lZC1yb2xlXC9BV1NSZXNlcnZlZFNTT19Qb3dlclVzZXJBY2Nlc3NfNGRjNTIzMjM5YTkwMTlkY1wvbGVvbi55aW5AdGhlbWFya3VwLm9yZyIsInVzZXJuYW1lIjoiYXNzdW1lZC1yb2xlJTJGQVdTUmVzZXJ2ZWRTU09fUG93ZXJVc2VyQWNjZXNzXzRkYzUyMzIzOWE5MDE5ZGMlMkZsZW9uLnlpbiU0MHRoZW1hcmt1cC5vcmcifQ.LWFZOJMDcYdu6od6Nk8TmhAFMGA9O98O4tIOsVlR7w5vAS_JgVixL8j75u6jTgjfWkdddhKqa5kgsXDmGNbjhzLIsD48ch1BUodlzxqeQfn0r8onIwLbUIHEnk6X-AJE',\n    'skin': 'noskin',\n}\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n    # 'Accept-Encoding': 'gzip, deflate, br',\n    'Origin': 'https://www.amazon.com',\n    'Connection': 'keep-alive',\n    'Referer': 'https://www.amazon.com/',\n   \n    'Sec-Fetch-Dest': 'empty',\n    'Sec-Fetch-Mode': 'cors',\n    'Sec-Fetch-Site': 'same-site',\n}\n\nparams = {\n    'limit': '11',\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'page-type': 'Gateway',\n    'alias': 'aps',\n    'site-variant': 'desktop',\n    'version': '3',\n    'event': 'onKeyPress',\n    'wc': '',\n    'lop': 'en_US',\n    'last-prefix': '\\0',\n    'avg-ks-time': '2486',\n    'fb': '1',\n    'session-id': '139-9925917-2023535',\n    'request-id': 'SVMTJXRDBQ9T8M7BRGNJ',\n    'mid': 'ATVPDKIKX0DER',\n    'plain-mid': '1',\n    'client-info': 'amazon-search-ui',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                        params=params, cookies=cookies, headers=headers)\n\nYou can run this Python code, as-is, and it should work.\n\n\n7. Strip it down\nYou might be overwhelmed with the parameters that go into this API request. Like the response output, the inputs are formatted like a JSON, too. Start removing these parameters one-by-one.\nI tend to keep a few essential ones for authentication, and also the parameters you care about changing for your own purposes. Notice that our example query of ‚Äúspicy‚Äù stored in the prefix parameter.\nüí°Pro tip: parameter values can expire, so test the request and each parameter to assure you only keep the shelf-stable parts.\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n}\n\nparams = {\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'alias': 'aps',\n    'plain-mid': '1',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', params=params, headers=headers)\n\n\n\n8. Recycle and reuse\nWith the stripped down request, try to submit a few‚Äî let‚Äôs say 10 or 20, requests with new parameters set by you.\nBelow, I converted the stripped down API call into a Python function that takes any keyword as input.\n\nimport pandas as pd\nimport time\n\ndef search_suggestions(keyword):\n    \"\"\"\n    Get autocompleted search suggestions for a `keyword` search on Amazon.com.\n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'Accept-Language': 'en-US,en;q=0.5',\n    }\n\n    params = {\n        'prefix': keyword,\n        'suggestion-type': [\n            'WIDGET',\n            'KEYWORD',\n        ],\n        'alias': 'aps',\n        'plain-mid': '1',\n    }\n\n    response = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                            params=params, headers=headers)\n    return response.json()\n\nHere we can set new input parameters in keyword, and make the an API call using each keyword.\n\nkeywords = [\n    'a', 'b', 'cookie', 'sock', 'zelda', '12'\n]\n\ndata = []\nfor keyword in keywords:\n    suggestions = search_suggestions(keyword)\n    suggestions['search_word'] = keyword # keep track of the seed keyword\n    time.sleep(1) # best practice to put some time between API calls.\n    data.extend(suggestions['suggestions'])\n\nWe save the API responses in a list called data, and put them into a Pandas DataFrame to analyze.\n\ndf = pd.DataFrame(data)\n\n# show 5 random auto suggestions\ndf.sample(5, random_state=303)\n\n\n\n\n\n  \n    \n      \n      suggType\n      type\n      value\n      refTag\n      candidateSources\n      strategyId\n      prior\n      ghost\n      help\n      queryUnderstandingFeatures\n      spellCorrected\n      fallback\n      blackListed\n      xcatOnly\n    \n  \n  \n    \n      4\n      KeywordSuggestion\n      KEYWORD\n      asmanex twisthaler 30 inhaler\n      nb_sb_ss_i_5_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n      False\n      False\n      False\n      False\n    \n    \n      13\n      KeywordSuggestion\n      KEYWORD\n      backpack\n      nb_sb_ss_i_4_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n      False\n      False\n      False\n      False\n    \n    \n      19\n      KeywordSuggestion\n      KEYWORD\n      biker shorts women\n      nb_sb_ss_i_10_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n      False\n      False\n      False\n      False\n    \n    \n      12\n      KeywordSuggestion\n      KEYWORD\n      bathing suit for women\n      nb_sb_ss_i_3_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n      False\n      False\n      False\n      False\n    \n    \n      16\n      KeywordSuggestion\n      KEYWORD\n      blackout curtains\n      nb_sb_ss_i_7_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n      False\n      False\n      False\n      False\n    \n  \n\n\n\n\nIf you look at the columns, you might be flooded with more questions: - Some terms may be blackListed, what does that mean and what words, if any, are blackListed = True? - Are some searches paid for, and not organic? - What is ghost?\nThis metadata is only visable from the API, and can lead to new story ideas and directions to pursue.\nUnfortunately, because this API is undocumented, asking these questions and figuring out what everything represents is difficult. Use your curiousity and look at many examples. The feature of the API is being able to make many queries at scale, which should help answer these questions. Reporting this out with sources is also essential in this process."
  },
  {
    "objectID": "_notebooks/finding-apis.html#do-it-yourself",
    "href": "_notebooks/finding-apis.html#do-it-yourself",
    "title": "Finding Undocumented APIs",
    "section": "Do it Yourself",
    "text": "Do it Yourself\nUse this space below to find another undocumented API and reuse it for a few other input parameters.\nRevisit the steps we outlined above, and apply them to a new website.\nEven if you aren‚Äôt a coder, try to get steps 1-6 ;)\nIf you are a coder, try some of the advanced usage below.\n\nFor advanced usage‚Ä¶\n\nHandle errors for bad requests, rate limiting, and other issues that could arise.\nyou can use session instead of pure requests. This is helpful if cookies and authentication are involved. Read more about that here.\nyou can make a request asyncronous to speed up data collection (without overloading the site‚Äôs servers, of course).\nImplement steps 6-onwards in another programming language."
  },
  {
    "objectID": "_notebooks/finding-apis.html#homework-assignment",
    "href": "_notebooks/finding-apis.html#homework-assignment",
    "title": "Finding Undocumented APIs",
    "section": "Homework Assignment",
    "text": "Homework Assignment\nDry-run: Think about the websites you frequent and topics that interest you. Find an API in the wild, isolate it, and analyze some of its results.\nScoping: Determine how the API could be used to produce data to answer a reporting question or hypothesis.\nReporting: Figure out the ‚Äúsample‚Äù: how many inputs will you test and for what purpose. Determine the meaning and signifigance of hidden fields that are returned.\nUltimately undocumented APIs are a tool and data is useless without a purpose. Hopefully this worksheet helps you in your time of need.\nThanks!"
  },
  {
    "objectID": "_notebooks/finding-apis.html#contact",
    "href": "_notebooks/finding-apis.html#contact",
    "title": "Finding Undocumented APIs",
    "section": "Contact",
    "text": "Contact\nSuggestions, corrections, follow-ups, or cool work you want to share that uses undocumented APIs?\nemail me at leon@leonyin.org"
  },
  {
    "objectID": "undocumented-apis.html#how-to-find-apis",
    "href": "undocumented-apis.html#how-to-find-apis",
    "title": "Finding Undocumented APIs",
    "section": "How to find APIs?",
    "text": "How to find APIs?\nMost APIs are undocumented, hidden in plain sight.\nYou can use undocumented APIs to collect datasets for investigations, audits, and tools.\nYou can sniff them out using a web browser‚Äôs developer tools (shortened to dev tools).\n\n\n\n\n\n\nNote that if you‚Äôre in a workshop setting: hitting the example API at the same time will get us all blocked from the website!\n\n\n\n\n1. First open the developer console.\nSee how on Chrome or Firefox here.\nIn this tutorial, we‚Äôll see how Amazon.com autocomplete search suggestions work.\nMy go-to method, (regardless of browser,) is to right-click and ‚ÄúInspect‚Äù an element on the page.\n\n\n\nExample of inspecting an element on a page using a right-click\n\n\nThis will open the dev tools under the ‚ÄúElements‚Äù tab, which is used to explore the source code of a page.\nPage source code is useful because it reveals useful clues that are otherwise unseen by regular users. Often times, clues are in accessibility features known as ARIA elements.\nHowever, this tutorial is not about source code‚Ä¶ it‚Äôs about API requests that populate what we see on the page, and the hidden fields that we don‚Äôt see.\nLet‚Äôs try this!\nWith dev tools open, go to Amazon.com, select the search bar on the website, and start typing a query (such as ‚Äúspicy‚Äù).\n\n\n2. Click the ‚ÄúNetwork‚Äù tab.\nThis section of the dev tools is used to monitor network requests.\nBackground\nEverything on a page is retrieved from some outside source, likely a server. This includes things like images embedded on the page, JavaScript code running in the background, and all the bits of ‚Äúcontent‚Äù that populate the page before us.\nUsing the network tab, we can find out how this information is requested from a server, and intercept the response before it is rendered on the page.\nThese responses are information-rich, and contain fields that don‚Äôt end up in the source code or in the user interface that most people encounter when they visit a site.\nFurther, we can reverse-engineer how this request is made, and use it to collect structured data at scale. This is the power of finding undocumented APIs.\nBack to the console‚Ä¶\nThe network tab can look pretty hectic at first. It has many uses, and a lot of information. We‚Äôll cover some of the basics.\n\n\n\n\n\n3. Filter requests by fetch/XHR\nThis will reveal only API calls made to servers. This includes internal servers that are hosted by the folks who run the website we‚Äôre inspecting, as well as external servers. The latter often includes third-party trackers used in adtech, and verification services to authenticate user behavior.\n\n\n\nYou might see quite a few network requests that were loaded onto the page. Look at ‚ÄúDomain‚Äù and ‚ÄúFile‚Äù to narrow down where requests were sent, and whether the names are telling of the purpose of the request.\n\n\n\n\n\n\nPro tip:\n\n\n\nYou can ‚ÄúFilter URLs‚Äù using different properties (see how to do this for Chrome and Firefox).\n\n\nIn this example, notice that a request was sent to the ‚ÄúDomain‚Äù completion.amazon.com, using an API endpoint (in the ‚ÄúFile‚Äù column) named suggestions. This is likely the API being called to populate autocompleted search suggestions on the Amazon marketplace.\nWhen clicking the network request, you‚Äôll see ‚ÄúHeaders‚Äù. Those are the HTTP headers that were sent along with the network request. This is not useful for us just yet, instead we want to see what data gets transferred as a result of the API call.\nTo do this, we‚Äôll look at the request‚Äôs ‚ÄúResponse‚Äù attributes.\n\n\n4. Analyze the response\nThis might seem intimidating at first, but let me key you in on some tips. Responses are almost always JSON-formatted. JSON is made up of lists and key-value pairs. This means the information is stored like a dictionary, with words and their corresponding definitions.\nLooking at the JSON response, it looks like Amazon‚Äôs completion.amazon.com/suggestions API returns a list of ‚Äúsuggestions‚Äù. Each item in the list of suggestions has a ‚Äúvalue‚Äù, in the example above that ‚Äúvalue‚Äù is spicy ramen.\nCheck your work: confirm this interpretation is correct by cross-referencing the API response with what a user would see on the website.\n\n\n\nAmazon‚Äôs suggestions for ‚Äúspicy‚Äù.\n\n\nGetting these steps down, is your one way ticket to spicy town, and you don‚Äôt need to code at all.\nHowever, some rudimentary coding can help you figure out how to use the API for vast inputs to collect your own dataset.\n\n\n5. Copy as cURL\nIf you find an HTTP request that returns a response with useful information you can start to reverse-engineer it. To do that, we can isolate it by right-clicking the HTTP request and selecting ‚Äúcopy as cURL‚Äù. (cURL stands for client URL, and is a tool used to transfer data across networks.)\n\n\n\n6. Curl to requests\nWe can use a site like curlconverter.com to convert the cURL we copied into a reusable API call. In this example, we use the default conversion to a Python requests script. You can do the same for any language and framework, but we use this to demonstrate because it‚Äôs exactly what I do in practice.\nHere is what the converted cURL looks like after being converted to a Python request:\n\nimport requests\n\ncookies = {\n    'aws-ubid-main': '836-8365128-6734270',\n    'session-id-time': '2082787201l',\n    'ubid-main': '135-7086948-2591317',\n    'aws-priv': 'eyJ2IjoxLCJldSI6MCwic3QiOjB9',\n    'aws-target-static-id': '1593060129944-225088',\n    'lc-main': 'en_US',\n    'aws-userInfo': '%7B%22arn%22%3A%22arn%3Aaws%3Asts%3A%3A335340120612%3Aassumed-role%2FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%2Fleon.yin%40themarkup.org%22%2C%22alias%22%3A%22themarkup-journalist-sandbox%22%2C%22username%22%3A%22assumed-role%252FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%252Fleon.yin%2540themarkup.org%22%2C%22keybase%22%3A%22Fwu%2FzLIz0yD%2FyfrORX%2BSIpIQPXrzr0kt24uqa8mNs7g%5Cu003d%22%2C%22issuer%22%3A%22https%3A%2F%2Fd-90677ebe1e.awsapps.com%2Fstart%2F%23%2Fsaml%2Fcustom%2F335340120612%2520%2528AWS%2520Journalist%2520Sandbox%2529%2FMTI5NTE5MjY4NzI0X2lucy0wNzcxNmExOGZlNThiYjIyX3AtNDUyYTgyOTRhNTJlYWZjNQ%5Cu003d%5Cu003d%22%2C%22signinType%22%3A%22PUBLIC%22%7D',\n    'x-main': 'Oz3Tb5n2p0ic7OhF3cU5dc9B4ZR2gFjhKEsP4zikHHD3Gk2O7NpSmuShBxLFrhpZ',\n    'at-main': 'Atza|IwEBILB5ARQ_IgTCiBLam_XE2pyT76jXTbAXHOm2AJomLPmDgoJUJIIlUmyFeh_gChLHCycKjNlys-5CqqMabKieAzqSf607ChJsNevw-V06e7VKgcWjvoMaZRWlGiZ-c5wSJ-e4QzIWzAxTS1EI6sRUaRZRv-a0ZpOJQ-sHHB99006ytcrHhubdrXYPJRqEP5Q-_30JtESMpAkASoOs4vETSFp5BDBJfSWWETeotpIVXwA4NoC8E59bZb_5wHTW9cRBSWYGi1XL7CRl2xGbJaO2Gv3unuhGMB1tiq9iwxodSPBBTw',\n    'sess-at-main': '\"PUq9PW1TbO9CTYhGMo7l1Dz+wedh40Ki8Z9rPC+1TSI=\"',\n    'sst-main': 'Sst1|PQHsbeSFCMSY0X0_WgvTo5NUCaZkG2J9RPqWWy0fCpyWopJXgu6_drU_LstOdJB2cDmaVCXwkNpsF5yNPrBDj3Wtx-TC-AaYZn6WUdp8vNRPb6iYqxPAjRDnfK3pCnHqt19I0GoG7Bd1wnOxkAvnH0992IUq14kH6Ojm0J8noVPwMez0lltD-jxBwtDQ_EZYUkZG741RDVEojfziawJY9iKc-cLCnKmhi-ca1PPJnsimPV4lXRtMAGFbf9nMkKq4CbpkaRMdVtlPr20vF9eqg_V_-LY_V7S44WlO-_t_bFBnK8Q',\n    'i18n-prefs': 'USD',\n    'session-token': 'ptze73uznXExrMCSV9AklvNOKa1ND9F0rlQH2ioSM26Vr6hSheH8O4v4P8Lg3zuv7oDM+HZ+8f2TlyoPXUmPShprMXdvEpAQieXUw7+83PZOJvkkg1jwP0NiG0ZqksIYOr3Zuwt3omMcfCKRReWKxl5rGaDEM6AISpwI5aMDDCnA7fWbVO/QQYNxUZMifc599EZ5Fg3uGjCAhBlb6I7UO8ewRbXJ1bo9',\n    'session-id': '139-9925917-2023535',\n    'aws-userInfo-signed': 'eyJ0eXAiOiJKV1MiLCJrZXlSZWdpb24iOiJ1cy1lYXN0LTEiLCJhbGciOiJFUzM4NCIsImtpZCI6ImFhNDFkZjRjLTMxMzgtNGVkOC04YmU5LWYyMzUzYzNkOTEzYiJ9.eyJzdWIiOiJ0aGVtYXJrdXAtam91cm5hbGlzdC1zYW5kYm94Iiwic2lnbmluVHlwZSI6IlBVQkxJQyIsImlzcyI6Imh0dHBzOlwvXC9kLTkwNjc3ZWJlMWUuYXdzYXBwcy5jb21cL3N0YXJ0XC8jXC9zYW1sXC9jdXN0b21cLzMzNTM0MDEyMDYxMiUyMCUyOEFXUyUyMEpvdXJuYWxpc3QlMjBTYW5kYm94JTI5XC9NVEk1TlRFNU1qWTROekkwWDJsdWN5MHdOemN4Tm1FeE9HWmxOVGhpWWpJeVgzQXRORFV5WVRneU9UUmhOVEpsWVdaak5RPT0iLCJrZXliYXNlIjoiRnd1XC96TEl6MHlEXC95ZnJPUlgrU0lwSVFQWHJ6cjBrdDI0dXFhOG1OczdnPSIsImFybiI6ImFybjphd3M6c3RzOjozMzUzNDAxMjA2MTI6YXNzdW1lZC1yb2xlXC9BV1NSZXNlcnZlZFNTT19Qb3dlclVzZXJBY2Nlc3NfNGRjNTIzMjM5YTkwMTlkY1wvbGVvbi55aW5AdGhlbWFya3VwLm9yZyIsInVzZXJuYW1lIjoiYXNzdW1lZC1yb2xlJTJGQVdTUmVzZXJ2ZWRTU09fUG93ZXJVc2VyQWNjZXNzXzRkYzUyMzIzOWE5MDE5ZGMlMkZsZW9uLnlpbiU0MHRoZW1hcmt1cC5vcmcifQ.LWFZOJMDcYdu6od6Nk8TmhAFMGA9O98O4tIOsVlR7w5vAS_JgVixL8j75u6jTgjfWkdddhKqa5kgsXDmGNbjhzLIsD48ch1BUodlzxqeQfn0r8onIwLbUIHEnk6X-AJE',\n    'skin': 'noskin',\n}\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n    # 'Accept-Encoding': 'gzip, deflate, br',\n    'Origin': 'https://www.amazon.com',\n    'Connection': 'keep-alive',\n    'Referer': 'https://www.amazon.com/',\n   \n    'Sec-Fetch-Dest': 'empty',\n    'Sec-Fetch-Mode': 'cors',\n    'Sec-Fetch-Site': 'same-site',\n}\n\nparams = {\n    'limit': '11',\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'page-type': 'Gateway',\n    'alias': 'aps',\n    'site-variant': 'desktop',\n    'version': '3',\n    'event': 'onKeyPress',\n    'wc': '',\n    'lop': 'en_US',\n    'last-prefix': '\\0',\n    'avg-ks-time': '2486',\n    'fb': '1',\n    'session-id': '139-9925917-2023535',\n    'request-id': 'SVMTJXRDBQ9T8M7BRGNJ',\n    'mid': 'ATVPDKIKX0DER',\n    'plain-mid': '1',\n    'client-info': 'amazon-search-ui',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                        params=params, cookies=cookies, headers=headers)\n\nYou can run this Python code, as-is, and it should work.\n\n\n7. Strip it down\nYou might be overwhelmed with the parameters that go into this API request. Like the response output, the inputs are formatted like a JSON, too. Start removing these parameters one-by-one.\nI tend to keep a few essential ones for authentication, and also the parameters you care about changing for your own purposes. Notice that our example query of ‚Äúspicy‚Äù stored in the prefix parameter.\n\n\n\n\n\n\nPro tip:\n\n\n\nParameter values can expire, so periodically test the request and each parameter to assure you only keep the shelf-stable parts.\n\n\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n}\n\nparams = {\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'alias': 'aps',\n    'plain-mid': '1',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', params=params, headers=headers)\n\n\n\n8. Recycle and reuse\nWith the stripped down request, try to submit a few‚Äî let‚Äôs say 10 or 20, requests with new parameters set by you.\nBelow, I converted the stripped down API call into a Python function that takes any keyword as input.\n\nimport pandas as pd\nimport time\n\ndef search_suggestions(keyword):\n    \"\"\"\n    Get autocompleted search suggestions for a `keyword` search on Amazon.com.\n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'Accept-Language': 'en-US,en;q=0.5',\n    }\n\n    params = {\n        'prefix': keyword,\n        'suggestion-type': [\n            'WIDGET',\n            'KEYWORD',\n        ],\n        'alias': 'aps',\n        'plain-mid': '1',\n    }\n\n    response = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                            params=params, headers=headers)\n    return response.json()\n\nHere we can set new input parameters in keyword, and make the an API call using each keyword.\n\n# Here are our inputs (what searches we'll get autocompleted)\nkeywords = [\n    'a', 'b', 'cookie', 'sock', 'zelda', '12'\n]\n\n# Here we'll go through each input, get the suggestions, and then add the `suggestions` to a list.\ndata = []\nfor keyword in keywords:\n    suggestions = search_suggestions(keyword)\n    suggestions['search_word'] = keyword # keep track of the seed keyword\n    time.sleep(1) # best practice to put some time between API calls.\n    data.extend(suggestions['suggestions'])\n\nWe saved the API responses in a list called data, and put them into a Pandas DataFrame to analyze.\n\ndf = pd.DataFrame(data)\n\n# show 5 random auto suggestions\ndf.sample(5, random_state=303)\n\n\n\n\n\n  \n    \n      \n      suggType\n      type\n      value\n      refTag\n      candidateSources\n      strategyId\n      prior\n      ghost\n      help\n      queryUnderstandingFeatures\n    \n  \n  \n    \n      4\n      KeywordSuggestion\n      KEYWORD\n      asmanex twisthaler 30 inhaler\n      nb_sb_ss_i_5_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      13\n      KeywordSuggestion\n      KEYWORD\n      bathroom organizer\n      nb_sb_ss_i_4_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      19\n      KeywordSuggestion\n      KEYWORD\n      baby wipes\n      nb_sb_ss_i_10_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      12\n      KeywordSuggestion\n      KEYWORD\n      baby registry search\n      nb_sb_ss_i_3_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      16\n      KeywordSuggestion\n      KEYWORD\n      b013xkha4m b08xzrxczm b07xxphqzk b09rwjblc7\n      nb_sb_ss_i_7_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n  \n\n\n\n\nIf you look at the columns, you might be flooded with more questions:\n\nSome terms may be blackListed, what does that mean and what words, if any, are blackListed = True?\nAre some searches paid for, and not organic?\nWhat is ghost?\n\nThis metadata is only visable from the API, and can lead to new story ideas and directions to pursue.\nUnfortunately, because this API is undocumented, asking these questions and figuring out what everything represents is difficult. Use your curiousity and look at many examples. The feature of the API is being able to make many queries at scale, which should help answer these questions. Reporting this out with sources is also essential in this process."
  },
  {
    "objectID": "undocumented-apis.html#do-it-yourself",
    "href": "undocumented-apis.html#do-it-yourself",
    "title": "Finding Undocumented APIs",
    "section": "Do it Yourself",
    "text": "Do it Yourself\nUse this space below to find another undocumented API and reuse it for a few other input parameters.\nRevisit the steps we outlined above, and apply them to a new website. If you aren‚Äôt a coder, try to get steps 1-6 (I believe in you!).\nIf you are a coder, try some of the advanced usage below.\n\nFor advanced usage‚Ä¶\n\nHandle errors for bad requests, rate limiting, and other issues that could arise.\nyou can use session instead of pure requests. This is helpful if cookies and authentication are involved. Read more about that here.\nyou can make a request asyncronous to speed up data collection (without overloading the site‚Äôs servers, of course).\nImplement steps 6-onwards in another programming language."
  },
  {
    "objectID": "undocumented-apis.html#homework-assignment",
    "href": "undocumented-apis.html#homework-assignment",
    "title": "Finding Undocumented APIs",
    "section": "Homework Assignment",
    "text": "Homework Assignment\nDry-run: Think about the websites you frequent and topics that interest you. Find an API in the wild, isolate it, and analyze some of its results.\nScoping: Determine how the API could be used to produce data to answer a reporting question or hypothesis.\nReporting: Figure out the ‚Äúsample‚Äù. How many inputs will you test and for what purpose? Determine the meaning and signifigance of hidden fields that are returned.\nUltimately undocumented APIs are a tool, and data is useless without a purpose. Hopefully this worksheet helps you in your time of need."
  },
  {
    "objectID": "index.html#who-wrote-this",
    "href": "index.html#who-wrote-this",
    "title": "Inspect Element",
    "section": "Who wrote this?",
    "text": "Who wrote this?\nInspect Element is written and edited by Leon Yin, an investigative data journalist at The Markup.\nRead about his award-winning journalism on his personal website, at The Markup.org\nThis site was generated using the Quarto open-source web framework."
  },
  {
    "objectID": "build-your-own-datasets.html",
    "href": "build-your-own-datasets.html",
    "title": "Build your own datasets",
    "section": "",
    "text": "Building your own dataset will enable you to\nThis section of the guide will cover various strategies to collect datasets, as well as best-practices learned from data engineering‚Äì the professionalized version of data collection."
  },
  {
    "objectID": "undocumented-apis.html#artifacts",
    "href": "undocumented-apis.html#artifacts",
    "title": "Finding Undocumented APIs",
    "section": "Artifacts",
    "text": "Artifacts\nSlides from workshops can be found here:\n2023-03-16 @ Tow Center Columbia"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Buolamwini, Joy, and Timnit Gebru. 2018. ‚ÄúGender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.‚Äù In Proceedings of the 1st Conference on\nFairness, Accountability and Transparency, edited by Sorelle A.\nFriedler and Christo Wilson, 81:77‚Äì91. Proceedings of Machine Learning\nResearch. PMLR. https://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nSap, Maarten, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A.\nSmith. 2019. ‚ÄúThe Risk of Racial Bias in Hate Speech\nDetection.‚Äù In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 1668‚Äì78. Florence,\nItaly: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1163."
  }
]