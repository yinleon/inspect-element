[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inspect Element",
    "section": "",
    "text": "Video\n\nThis guide walks through in-depth case studies and hands-on tutorials to help you investigate opaque systems, systematically.\nYou will learn how to build your own datasets, find undocumented APIs, analyze missing data, measure disparate outcomes, bullet-proof findings, and write with precision.\nAs practitioners ourselves, we’ve had to learn on the job, hit dead ends, and seek external expertise across fields and industries. Here, we distill these experiences and include tips for veterans and new-comers alike.\nThe disciplines we’ll draw from include: investigative journalism, data science, social science, and other branches of computer and information science.\nDon’t code? No problem: the guide emphasizes underlying principles and uses plain-language (“pseudocode”) explainations to accompany any code.\n\nWho wrote this?\nInspect Element is written by investigative data journalist Leon Yin with contributions by Piotr Sapiezynski, and others TK.\nI’ll frequently reference past investigations I’ve worked on in this guide. You can read those investigations plus new stories at Bloomberg and The Markup.\nThis site was generated using the Quarto open-source publishing system.\n\nCorrections, comments, suggestions?\nEmail: inspectelement@leonyin.org File an issue: on GitHub"
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "Planning Data Investigations",
    "section": "",
    "text": "Data investigations can get complicated and you might not know where to start.\nIn this section we’ll introduce you to a planning checklist that we use for our own investigations.\nThe checklist will help you choose an accountability target, identify tangible harms, and form a testable hypothesis.\nMost importantly, the checklist covers questions you’ll need to answer in order to develop a defensible methodology and a bullet-proof story.\n\nbullet-proofing is hater-proofing\nYour readers and future self will thank you for being forthright about your investigation’s vulnerabilities. Moreover, you’ll get a sense of the experiment’s feasibility before you invest too much time into it.\nAlthough grim, the checklist can help expedite the decision to kill a story. Killing a story is a difficult process– there’s even a podcast on the topic called Killed Stories, but it’s better to catch fatal flaws and irreconcilable uncertainties early. Trust that doing this efficiently is a gift to yourself and your colleagues.\nThe questions in the checklist cover fundamental topics we’ll discuss throughout the practitioner’s guide such as:\n\nData collection\nViability (minimal viable) analyses\nClassification\nStatistical tests\nLimitations\nCommunicating findings for a general audience\n\nYou won’t need to fill out a checklist for every story, but it is necessary for projects with original data collection and/or analysis.\nLastly, view the checklist is a starting point. Add and edit questions with your team to assure you can publish your findings with certainty.\nGood luck."
  },
  {
    "objectID": "checklist.html#checklist",
    "href": "checklist.html#checklist",
    "title": "Pitching Hypothesis-Driven Data Investigations",
    "section": "Checklist",
    "text": "Checklist\n1. What is the hypothesis of the story?  This is a testable claim that can be proven or disproven. The hypothesis is revised and honed in the early stages of an investigation.\n2. Who is being harmed and at what scale?  Give us a sense of who is being harmed, and whether outcomes are proportionate?\n3. Who is causing the harm and what is the accountability angle?  What frameworks exist– legal, company policies, sworn testimonies, to hold them accountable?\n4. What is the evidence (anecdotal or otherwise) you’ve gathered that leads you to think you have a viable hypothesis?  If there is existing work on this topic (journalistic, academic, lived experience) how will you build off that work?\n5. What is a viability study you can perform?  How can you begin to collect and analyze data as a proof of concept?\n6. What data will you need to run an analysis? How will you gather the data?  Web scraping, public records requests, using open data? How will you decide what makes up your sample? Specificity is your friend.\n7. How complicated is the data collection?  Will you be able to do this alone? Do you need to use proxies, cloud instances? Do you need to collect data over time?\n8. Will you need to filter out records from that data?  What records will you throw out and why?\n9. What are the limitations of the dataset(s) you are proposing to use? How will you test its accuracy?  Every dataset is imperfect and carries inherent assumptions. How will you assure what you’re writing is aligned with your dataset? How will you bulletproof that dataset?\n10. Do you need to classify the data for your experiment? If so, please describe how you propose doing that. Are there outside classifications or experts you can lean on? What are the limitations of your classification method?  Seldom is the outcome you want to measure already encoded in a column. Instead, you need to make that assessment.\n11. How will you analyze the data? What statistical tests will run? Please list any limitations to your proposed method and any alternatives.  Start with simplicity.\n12. What specific sentences will you be able to write based on your findings? What’s the lede? What’s the nutgraph?  Use this space to brainstorm how you’ll frame your findings. This helps make your findings tangible. I use TK’s at this step.\n13. Can you imagine the charts or other visualizations this data will produce?  How will you communicate your findings visually?"
  },
  {
    "objectID": "checklist.html#acknowledgements",
    "href": "checklist.html#acknowledgements",
    "title": "Pitching Hypothesis-Driven Data Investigations",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis checklist is adapted from a checklist used by my editors Julia Angwin and Evelyn Larrubia at The Markup. Jeremy Singer-Vine provided feedback on the adapted list."
  },
  {
    "objectID": "build-your-own-datasets.html#public-data-sources-explained",
    "href": "build-your-own-datasets.html#public-data-sources-explained",
    "title": "Build your own datasets",
    "section": "Public data sources explained",
    "text": "Public data sources explained\nThere’s a difference between “open data” and “publicly available data”:\n\nOpen data is typically already combined into a spreadsheet or database. Additionally, open data is usually documented, and easily available for the public to use. See for example, climate data from NOAA, or the U.S. Census Bureau’s American Community Survey.\nPublicly available data lives on the open web, but has yet to be synthesized into a cohesive data set. It’s up to you to collect these data points, responsibly. Search engines and many other technology companies depend on “crawling” these sources.\n\nAt a minimum: only collect data with intention, do not overload websites’ servers, and abstain from collecting personally identifiable information without user consent."
  },
  {
    "objectID": "build-your-own-datasets.html#what-to-expect-in-this-section",
    "href": "build-your-own-datasets.html#what-to-expect-in-this-section",
    "title": "Build your own datasets",
    "section": "What to expect in this section?",
    "text": "What to expect in this section?\nPublicly available data is a useful tool to audit and investigate technologies and their underlying business practices.\nThe following sections will cover programmatic data collection and best-practices.\nWe’ll discuss data collection techniques such as:\n\nFinding undocumented APIs\nBrowser automation\nApp automation\nParsing HTML and JSON\n\nUse these techniques to build datasets that allow you to test original hypotheses, design clear experiments, and understand the limitations that come along with the decisions you make."
  },
  {
    "objectID": "build-your-own-datasets.html#scraping-is-not-a-crime",
    "href": "build-your-own-datasets.html#scraping-is-not-a-crime",
    "title": "Build your own datasets",
    "section": "Scraping is not a crime",
    "text": "Scraping is not a crime\nAlthough big tech giants and data brokers often depend on web scraping for their business models, they seldom use that data in the public interest or release data that could be used to hold themselves accountable.\nThis guide exists to teach you how to build evidence that leads to accountability. However, know that using data to investigate powerful entities is not without risks.\nIf you’re in the United States: know what violates the Computer Fraud and Abuses Act (CFAA), which primarily prohibits unauthorized access to a computer network.\nRecent cases such as Van Buren v. United States, hiQ v Linkedin, and Sandvig v. Barr helped shape interpretations of CFAA for collecting public data with automated means, such as web scraping.\nAlthough the legal landscape is changing to favor web scraping in the public interest, we still see governments and industry titans attempt to shut down accountability efforts. Take for example:\n\nA journalist in Missouri was called a hacker by the governor and threatened prosecution for identifying a flaw that revealed social security numbers of school employees after inspecting the page source.\nAcademic researchers at NYU received a cease-and-desist notice for crowdsourcing Political ads from Facebook.\n\n\n\n\n\n\n\nEven if your activity does not fall within CFAA’s purview or violate any other law, online services can suspend your account(s) for breaking their terms of service. For that reason, be careful involving your personal/institutional accounts in web scraping, and volunteers’ if you’re crowdsourcing data.\n\n\n\nIf you want more information on the topic, several of the field’s top researchers explore the legal and ethical considerations in Section 4.1 of Metaxa et al. (2021).\nThis is NOT legal advice. Discuss your intentions and your plan to collect data with your editor and legal counsel (if you’re a journalist), or your advisor and ethics board (if you’re a researcher).\nHaving institutional support is essential to make sure you are protected, and that you and your superiors are well-informed about the risks.\n\nAaron Swartz\nContemporary legal interpretations of CFAA and web scraping can be traced back to the late activist and engineer Aaron Swartz.\nIn 2008, Swartz was investigated by the F.B.I. for scraping 2.7 million public court records from PACER and sharing it with the public. Swartz redistributed information that is in the public domain, but hosted by a central entity that charges fees for accessing that public information.\nThe bureau concluded that Swartz did not violate any laws, but three years later, Swartz was arrested and federally indicted for mass-downloading academic articles from JSTOR using a laptop stored in an MIT closet. Although neither JSTOR, MIT, nor state prosecutors chose to litigate, federal prosecutors sought maximal penalties: Swartz faced $1 million in fees and 35 years in prison– charges that were deeply criticized by lawyers and experts.\nSwartz’s prosecution and untimely passing would have a chilling effect on web scraping in the academy for years to come. But attitudes are changing slowly, with journalists, researchers, and other public interest technologists receiving more legal and institutional protections to collect publicly available data.\nYou can learn more about Aaron Swartz in the documentary “The Internet’s Own Boy,“ directed by Brian Knappenberger, and on the website AaronSwartzDay.org.\n\n\n\n\nMetaxa, Danaë, Joon Sung Park, Ronald E. Robertson, Karrie Karahalios, Christo Wilson, Jeff Hancock, and Christian Sandvig. 2021."
  },
  {
    "objectID": "apis.html",
    "href": "apis.html",
    "title": "Finding Undocumented APIs",
    "section": "",
    "text": "Intro\nMore tutorials on the same subject:\nTopical and timeless:\nNotable investigations and audits using undocumented APIs:\nPlease reach out with more examples to add.\nTo cite this chapter, please use the following BibTex entry:"
  },
  {
    "objectID": "apis.html#what-is-an-a-p-i",
    "href": "apis.html#what-is-an-a-p-i",
    "title": "Finding Undocumented APIs",
    "section": "What is an A-P-I?",
    "text": "What is an A-P-I?\nIf you have tried to get a driver’s license or a travel visa, you have experienced bureaucracy at its finest– a series of lines, forms, credential-showing, and waiting.\nApplication Program Interfaces, or APIs, are digitized bureaucracy. You make a request, and then wait in a queue to be served by a server. However, instead of leaving with a driver’s license or a custom plate, what you’re waiting for is well-formatted data. As for making mistakes… well, you’ll get an automated rejection and zero sympathy.\nMost APIs are undocumented and hidden in plain sight. There is no set terminology for these APIs, so for simplicity’s sake we’ll refer to them as “undocumented APIs”.\nSome investigations are only possible after finding and reverse-engineering undocumented APIs. Our first case study illustrates this in detail, where my reporting partner Aaron Sankin and I discovered undocumented APIs out of necessity while reporting on YouTube."
  },
  {
    "objectID": "apis.html#a-key-tool-for-investigations",
    "href": "apis.html#a-key-tool-for-investigations",
    "title": "Finding Undocumented APIs",
    "section": "A key tool for investigations",
    "text": "A key tool for investigations\nYouTube is the largest video-hosting platform in the world, and plays a major role in the creator economy thanks to the YouTube Partner Program that shares ad revenue with eligible creators.\nAdvertising is central to YouTube, and major companies have boycotted the platform in the past in response to their ads appearing alongside extremist content. We wanted to better understand YouTube’s advertising system, especially how they treated hateful conspiracy theories, which at the time, seemed to thrive on the platform.\nTo start investigating this topic, we got acquainted with the Google Ads portal. Anyone can sign-up, and see all the tools that marketers can use to reach users across the Google adverse. YouTube has a special section of the ad portal, where marketers can target their ads based on user demographics and the content of videos.\nWe investigated a specific targeting tool that allows ad-buyers to use keyword searches to find videos and channels to place their ads on.\nRead the investigation along with its accompanying methodology.\nIn an initial test, we found that searching for the racist “White genocide” conspiracy theory returned no videos, but by simply removing spaces, we were shown relevant results.\n\n\n\n\nSearching the Google Ads portal for YouTube videos related to a conspiracy theory, and circumventing blocking measures. Source: Google.com/The Markup\n\n\nThis anecdotal test suggested that Google was using a keyword blocklist that hid results for certain search terms, but not others. We performed further tests and found that searching for swears and strings of gibberish also surfaced no results.\nWe wanted to verify if Google was using a keyword blocklist, and test how consistent that blocklist was with YouTube’s “advertiser friendly” guidelines. Unfortunately, the portal did not make it possible to discern between a blocked keyword and one that may have been too obscure to return any results.\nOur colleague Surya Mattu suggested using the web browser’s built-in developer tools to monitor network requests while we made searches in the portal. This proved to be a breakthrough that allowed us to isolate the API-endpoint being called during this process, reverse-engineer it to return results for any given keyword, and analyze the API’s response before its contents were displayed in the portal.\nBy looking closely at the API responses, we were able to identify clear structural differences based on Google’s verdict of the keyword. Blocked terms returned an empty JSON string {}, whereas obscure terms returned a JSON with labels but no results:\n{“videos”: [], “channels\": []}\nWith the categorization scheme established, we could confirm search terms were being blocked (read about this in detail here). Moreover, with the API at our service, we could test any set of keywords, so we tested well-known hate terms and phrases related to “racial justice and representation” that we asked independent advocacy groups to send us.\nAfter testing the two keyword lists, we saw a pattern of Google blocking racial justice terms (like “Black power”), while showing advertisers results for well-known hate terms (like “White power”).\n\n\n\nThis was my first time finding and using an undocumented API for an investigation. Doing so revealed essential information that was not visible to everyday users, and it allowed us to perform systematic tests and bring receipts.\nBefore we dive deeper into how to find undocumented APIs, it’s important to note that some APIs are “official” and well-documented to the public. These APIs can also be used to great effect."
  },
  {
    "objectID": "apis.html#documented-apis",
    "href": "apis.html#documented-apis",
    "title": "Finding Undocumented APIs",
    "section": "Documented APIs",
    "text": "Documented APIs\nMany businesses sell their services using APIs.\nThe benefit of documented APIs is self-explanatory, you know what you are going to get, and there are notes and examples to help developers use the tool as intended.\nSome documented APIs are also free to use, making them a great tool for teaching and research. One such API that journalists frequent is the Census Data API, which we use to retrieve statistical survey data from across the United States. Unfortunately, free APIs can often disappear or have their access severely limited– as we’ve seen with Twitter (no longer free), YouTube (severely restricted), and Facebook (deprecated entirely).\n\nHow have Documented APIs been used?\n\nGender Shades was an audit of three commercially-available facial recognition APIs used to automate gender classification (from Microsoft, IBM, and Face++). The authors created a benchmark image dataset of faces hand-labeled by gender and skin tone, and tested each facial recognition model by sending the benchmark dataset through each respective model’s API (Buolamwini and Gebru 2018). The authors found that many models had high error rates for female and Black faces, with the worst performance on Black female faces.\nGoogle’s Perspective API was developed to filter out toxic comments for publishers such as The New York Times. Importantly, Perspective used “training data” sourced from human-labeled Wikipedia edits. An academic study found racially biased classifications of Tweets. For example, the use of certain identifiers for minority groups would flag a comment as “toxic” (Sap et al. 2019). Because Google had released the API publicly, researchers could access and audit this technology directly through the API.\n\nNow, let’s get back to APIs that are undocumented and hidden."
  },
  {
    "objectID": "apis.html#undocumented-apis",
    "href": "apis.html#undocumented-apis",
    "title": "Finding Undocumented APIs",
    "section": "Undocumented APIs",
    "text": "Undocumented APIs\nThese are the unsung heroes making sure websites run, often times executing essential functions behind the scenes. Many of these functions are so mundane, you probably don’t even realize that something is happening.\nIf you spend time on social media platforms, you’ll find that the good times keep rolling, and you’ll never reach the end of the page. That is because “infinite scroll” is powered by an API that is called upon as you approach the bottom of the page to load more fun things to eat up your day.\nSometimes engineers find these API endpoints and build open source software to access public data programmatically. See Instaloader (for Instagram) as an example.\nLearning how to find and use these publicly available APIs can help you build evidence and test hypotheses that are otherwise unreachable due to lack of access."
  },
  {
    "objectID": "apis.html#how-have-undocumented-apis-been-used",
    "href": "apis.html#how-have-undocumented-apis-been-used",
    "title": "Finding Undocumented APIs",
    "section": "How have undocumented APIs been used?",
    "text": "How have undocumented APIs been used?\nJournalists and researchers have used undocumented APIs to catalog Amazon Ring’s sprawling surveillance network (Calacci, Shen, and Pentland 2022; Cameron and Mehrota 2019), measure inequities in internet access using Facebook’s advertising ecosystem (Garcia et al. 2018), and parse complex government documents listing presidential appointees (Willis 2013).\nUsing undocumented APIs has three key strengths:\n\nRichness: APIs often contain information that is not visible on web pages.\nReliability: APIs execute essential functions, so they don’t change often. This can make for a reliable data source over time.\nScalability: You can collect more information in less time using this method compared to headless browsers, such as Selenium, Puppeteer, and Playwright (Not throwing shade– these tools have their purpose).\n\nNext we will cover three case studies, each of which is intended to highlight one of these benefits."
  },
  {
    "objectID": "apis.html#case-study-on-richness-googles-blocklist-for-youtube-advertisers",
    "href": "apis.html#case-study-on-richness-googles-blocklist-for-youtube-advertisers",
    "title": "Finding Undocumented APIs",
    "section": "Case study on richness: Google’s blocklist for YouTube advertisers",
    "text": "Case study on richness: Google’s blocklist for YouTube advertisers\nI’m not going to rehash this case study, since we led with it in the introduction, but…\nUsing undocumented APIs can reveal rich metadata. This includes hidden fields that are not displayed to everyday users of a website, as well as subtle changes to the structural in how data is returned.\nUsing this metadata produces receipts you can follow by deciphering the meaning of these hidden fields, finding traces left by missing data, and identifying patterns that are otherwise hidden from the surface (front-end) world.\nCertainly this was the case with the YouTube investigation, and something that we’ll brush on again in the hands-on tutorial at the end of this section."
  },
  {
    "objectID": "apis.html#case-study-on-reliability-amazon-branded-products",
    "href": "apis.html#case-study-on-reliability-amazon-branded-products",
    "title": "Finding Undocumented APIs",
    "section": "Case study on reliability: Amazon branded products",
    "text": "Case study on reliability: Amazon branded products\nIf you have ever scraped HTML from a website, you’ve likely found yourself with a broken scraper.\nThis occurs when class names, accessibility labels, text, or something else has changed and confused your scraper. In this sense, HTML scraping can be fragile and fickle, especially if your collecting data over a prolonged period of time.\nA stark example is Facebook’s timeline, where you’ll find elements of the page are arbitrarily named, oddly-nested, and ever-changing.\nUsing undocumented APIs can often get you the same information with a higher success-rate. This is because these APIs interact with the same backend (fetching information before being rendered, named, and nestled neatly into a webpage), and are often essential to the operation of the website.\nIn the investigation “Amazon’s Advantage”, Adrianne Jeffries and I found a reliable method of identifying Amazon brands and exclusive products. At the time, these products were not clearly labeled, most Americans we surveyed were unable to identify Amazon’s top brands, and no source of truth existed.\nWe developed a approach to identify these products as Amazon private label using a filter found in the user interface of Amazon’s website. The “Our brands” filter did a lot of heavy lifting in our investigation, and we found that it was powered by a undocumented API that listed all the Amazon branded products for a given search.\nThis method was key to our investigation, which required persistent data collection over a period of several months. To our surprise, the API continued to work after we went to Amazon for comments on our detailed methodology, after we published our investigation, and even after Amazon executives were accused of perjury by members of the U.S. Congress.\nUsually the party gets shut down once you call the parents, but in this case it didn’t.\nBecause the API continued to work, we used it in a browser extension (Amazon Brand Detector) that we (including Ritu Ghiya and Jeff Crouse) built to highlight Amazon brands for shoppers around the globe. About half a year later, Amazon added an orange disclaimer of “Amazon brand” to their branded products, but the API and extension still work at the time of writing, more than a year later.\nThis case study emphasizes the reliability of using undocumented APIs, not only for collecting datasets, but for persistent accountability efforts."
  },
  {
    "objectID": "apis.html#case-study-on-scalability-collecting-internet-plans",
    "href": "apis.html#case-study-on-scalability-collecting-internet-plans",
    "title": "Finding Undocumented APIs",
    "section": "Case study on scalability: collecting Internet Plans",
    "text": "Case study on scalability: collecting Internet Plans\nIn the investigation, “Still Loading” my reporting partner Aaron Sankin and I collected and analyzed over 1 million internet service plans across major cities in the United States.\nWe learned a technique from a trio of researchers from Princeton, that used the lookup tools found on the internet service providers’ websites to retrieve internet plans for a specific address (Major, Teixeira, and Mayer 2020).\nHowever, doing this using a browser (as a real person would) is incredibly slow. Even with 10 automated browsers (see below) with unique IP addresses, it would have taken months to collect a representative sample of a single major American city.\n\n\n\n\n\nAutomating checking for internet plans from AT&T using Selenium browser automation.\n\n\nBrowser automation is bulky. Not only do you need to load every asset of a web page, there is also the compute resources necessary to spin up a browser. When you can get away without having to mock user interactions, or use rendered page elements, finding the underlying API(s) can be quicker and more eloquent.\nInitially, the workflow for getting an internet plan seemed too complex to pull off using an API– there was user authentication that set a cookie, choosing an address from a list of suggestions, and adding an apartment number when prompted.\nHowever, we were able to keep track of cookies using a session (read about this advanced topic here), and speed things up by bundling the sequence of successive API calls into a function.\nNot only was this function easier to write, but it was able to be written and executed asynchronously. Meaning we could request internet plans from many addresses at the same time.\nThis allowed us to collect AT&T internet plans for a representative sample of 21 cities in two days, rather than two years.\nTimely data collection is key. Solving this issue allowed us to be ambitious in the scope of our investigation, which ultimately found that Internet pricing disparities were common for lower-income, least-White, and historically redlined areas.\nWhen it comes to web scraping, undocumented APIs offer unmatched scalability to collect massive amounts of data. This is especially true when you orchestrate them with asynchronous and multi-threaded programming (another topic we plan to cover in a future section).\nAlthough the process of finding undocumented APIs is not too complicated (as you’ll see in the tutorial), the chances of finding one that is helpful for your investigation or research are still quite low. Don’t be deterred, that just makes finding a useful one more special."
  },
  {
    "objectID": "apis.html#how-to-find-and-use-undocumented-apis",
    "href": "apis.html#how-to-find-and-use-undocumented-apis",
    "title": "Finding Undocumented APIs",
    "section": "How to find and use undocumented APIs",
    "text": "How to find and use undocumented APIs\nIn this exercise, you’ll learn to sniff out undocumented APIs using the web browser’s developer tools (shortened to dev tools), figure out how they work, test different inputs, and analyze API responses.\nYou can do most of this tutorial with zero coding, but it’ll hold you back from using APIs to their fullest.\n\n\n\n\n\n\nNote that if you’re in a workshop setting: hitting the example API at the same time will get us all blocked from the website!\n\n\n\n\n1. First open the developer console.\nSee how on Chrome or Firefox here.\nIn this tutorial, we’ll see how Amazon.com autocomplete search suggestions work.\nOne way to get to the dev tools it to right-click and “Inspect” an element on the page.\n\n\n\nExample of inspecting an element on a page using a right-click\n\n\nThis will open the dev tools under the “Elements” tab, which is used to explore the source code of a page.\nPage source code is useful because it reveals clues that are otherwise unseen by regular users. Often times, clues are in accessibility features known as ARIA elements.\nHowever, this tutorial is not about source code… it’s about API requests that populate what we see on the page, and the hidden fields that we don’t see.\nLet’s try this!\nWith dev tools open, go to Amazon.com, select the search bar on the website, and start typing a query (such as “spicy”).\n\n\n2. Click the “Network” tab.\nThis section of the dev tools is used to monitor network requests.\nBackground\nEverything on a page is retrieved from some outside source, likely a server. This includes things like images embedded on the page, JavaScript code running in the background, and all the bits of “content” that populate the page before us.\nUsing the Network tab, we can find out how this information is requested from a server, and intercept the response before it is rendered on the page.\nThese responses are information-rich, and contain fields that don’t end up in the source code or in the user interface that most people encounter when they visit a site.\nFurther, we can reverse-engineer how this request is made, and use it to collect structured data at scale. This is the power of finding undocumented APIs.\nBack to the console…\nThe Network tab can look pretty hectic at first. It has many uses, and a lot of information. We’ll cover some of the basics.\n\n\n\n\n\n3. Filter requests by fetch/XHR\nThis will reveal only API calls made to servers. This includes internal servers that are hosted by the website we’re inspecting, as well as external servers. The latter often includes third-party trackers used in adtech, and verification services to authenticate user behavior.\n\n\n\nYou might see quite a few network requests that were loaded onto the page. Look at “Domain” and “File” to narrow down where requests were sent, and whether the names are telling of the purpose of the request.\n\n\n\n\n\n\nPro tip:\n\n\n\nYou can “Filter URLs” using different properties (see how to do this for Chrome and Firefox).\n\n\nIn this example, notice that a request was sent to the “Domain” completion.amazon.com, using an API endpoint (in the “File” column) named suggestions. This is likely the API being called to populate autocompleted search suggestions on the Amazon marketplace. Reading “File” names can help determine each API’s function.\nWhen clicking the network request, you’ll see “Headers”. Those are the HTTP headers that were sent along with the network request. This is not useful for us just yet, instead we want to see what data gets transferred as a result of the API call.\nTo do this, we’ll look at the request’s “Response” attributes.\n\n\n4. Analyze the response\nThis might seem intimidating at first, but let me key you in on some tips. Responses are almost always JSON-formatted. JSON is made up of lists and key-value pairs. This means the information is stored like a dictionary, with words and their corresponding definitions.\nLooking at the JSON response, it looks like Amazon’s completion.amazon.com/suggestions API returns a list of “suggestions”. Each item in the list of suggestions has a “value”, in the example above that “value” is spicy ramen.\nCheck your work: confirm this interpretation is correct by cross-referencing the API response with what a user would see on the website.\n\n\n\nAmazon’s suggestions for “spicy”.\n\n\nAnother check you can perform CTRL+F the JSON response for a unique string. This could be a string of text on the page (or something else) that serves as a unique tracer. Verifying its presence will help pinpoint the right API call.\nGetting these steps down, is your one way ticket to spicy town, and you don’t need to code at all.\nHowever, some rudimentary coding can help you figure out how to use the API for vast inputs to collect your own dataset.\n\n\n5. Copy as cURL\nIf you find an HTTP request that returns a response with useful information you can start to reverse-engineer it. To do that, we can isolate it by right-clicking the HTTP request and selecting “copy as cURL”. (cURL stands for client URL, and is a tool used to transfer data across networks.)\n\n\n\n6. Curl to requests\nWe can use a site like curlconverter.com to convert the cURL we copied into a reusable API call. In this example, we use the default conversion to a Python requests script. You can do the same for any language and framework.\nHere is what the converted cURL looks like after being converted to a Python request:\n\nimport requests\n\ncookies = {\n    'aws-ubid-main': '836-8365128-6734270',\n    'session-id-time': '2082787201l',\n    'ubid-main': '135-7086948-2591317',\n    'aws-priv': 'eyJ2IjoxLCJldSI6MCwic3QiOjB9',\n    'aws-target-static-id': '1593060129944-225088',\n    'lc-main': 'en_US',\n    'x-main': 'Oz3Tb5n2p0ic7OhF3cU5dc9B4ZR2gFjhKEsP4zikHHD3Gk2O7NpSmuShBxLFrhpZ',\n    'at-main': 'Atza|IwEBILB5ARQ_IgTCiBLam_XE2pyT76jXTbAXHOm2AJomLPmDgoJUJIIlUmyFeh_gChLHCycKjNlys-5CqqMabKieAzqSf607ChJsNevw-V06e7VKgcWjvoMaZRWlGiZ-c5wSJ-e4QzIWzAxTS1EI6sRUaRZRv-a0ZpOJQ-sHHB99006ytcrHhubdrXYPJRqEP5Q-_30JtESMpAkASoOs4vETSFp5BDBJfSWWETeotpIVXwA4NoC8E59bZb_5wHTW9cRBSWYGi1XL7CRl2xGbJaO2Gv3unuhGMB1tiq9iwxodSPBBTw',\n    'sess-at-main': '\"PUq9PW1TbO9CTYhGMo7l1Dz+wedh40Ki8Z9rPC+1TSI=\"',\n    'sst-main': 'Sst1|PQHsbeSFCMSY0X0_WgvTo5NUCaZkG2J9RPqWWy0fCpyWopJXgu6_drU_LstOdJB2cDmaVCXwkNpsF5yNPrBDj3Wtx-TC-AaYZn6WUdp8vNRPb6iYqxPAjRDnfK3pCnHqt19I0GoG7Bd1wnOxkAvnH0992IUq14kH6Ojm0J8noVPwMez0lltD-jxBwtDQ_EZYUkZG741RDVEojfziawJY9iKc-cLCnKmhi-ca1PPJnsimPV4lXRtMAGFbf9nMkKq4CbpkaRMdVtlPr20vF9eqg_V_-LY_V7S44WlO-_t_bFBnK8Q',\n    'i18n-prefs': 'USD',\n    'session-token': 'ptze73uznXExrMCSV9AklvNOKa1ND9F0rlQH2ioSM26Vr6hSheH8O4v4P8Lg3zuv7oDM+HZ+8f2TlyoPXUmPShprMXdvEpAQieXUw7+83PZOJvkkg1jwP0NiG0ZqksIYOr3Zuwt3omMcfCKRReWKxl5rGaDEM6AISpwI5aMDDCnA7fWbVO/QQYNxUZMifc599EZ5Fg3uGjCAhBlb6I7UO8ewRbXJ1bo9',\n    'session-id': '139-9925917-2023535',\n    'aws-userInfo-signed': 'eyJ0eXAiOiJKV1MiLCJrZXlSZWdpb24iOiJ1cy1lYXN0LTEiLCJhbGciOiJFUzM4NCIsImtpZCI6ImFhNDFkZjRjLTMxMzgtNGVkOC04YmU5LWYyMzUzYzNkOTEzYiJ9..LWFZOJMDcYdu6od6Nk8TmhAFMGA9O98O4tIOsVlR7w5vAS_JgVixL8j75u6jTgjfWkdddhKqa5kgsXDmGNbjhzLIsD48ch1BUodlzxqeQfn0r8onIwLbUIHEnk6X-AJE',\n    'skin': 'noskin',\n}\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n    # 'Accept-Encoding': 'gzip, deflate, br',\n    'Origin': 'https://www.amazon.com',\n    'Connection': 'keep-alive',\n    'Referer': 'https://www.amazon.com/',\n   \n    'Sec-Fetch-Dest': 'empty',\n    'Sec-Fetch-Mode': 'cors',\n    'Sec-Fetch-Site': 'same-site',\n}\n\nparams = {\n    'limit': '11',\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'page-type': 'Gateway',\n    'alias': 'aps',\n    'site-variant': 'desktop',\n    'version': '3',\n    'event': 'onKeyPress',\n    'wc': '',\n    'lop': 'en_US',\n    'last-prefix': '\\0',\n    'avg-ks-time': '2486',\n    'fb': '1',\n    'session-id': '139-9925917-2023535',\n    'request-id': 'SVMTJXRDBQ9T8M7BRGNJ',\n    'mid': 'ATVPDKIKX0DER',\n    'plain-mid': '1',\n    'client-info': 'amazon-search-ui',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                        params=params, cookies=cookies, headers=headers)\n\nYou can run this Python code, as-is, and it should work.\nJust a reminder that you can run this code interactively on Google Colab by copying this tutorial (it’s written as a Jupyter Notebook), or running a “new notebook” and pasting the code in.\nPress the little play button on the left to run the code. You should see something that looks similar to what you saw in the inspector.\n\n# to see the response, run this cell:\nresponse.json()\n\n\n\n7. Strip it down\nYou might be overwhelmed with the parameters that go into this API request. Like the response output, the inputs are formatted like a JSON, too. Start removing these parameters one-by-one.\nKeep parameters for authentication, and also the input parameters that you can change for your own purposes. Notice that the example query of “spicy” stored in the prefix parameter.\n\n\n\n\n\n\nPro tip:\n\n\n\nParameter values can expire, so periodically test the request and each parameter to assure you only keep the shelf-stable parts.\n\n\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n    'Accept': 'application/json, text/javascript, */*; q=0.01',\n    'Accept-Language': 'en-US,en;q=0.5',\n}\n\nparams = {\n    'prefix': 'spicy',\n    'suggestion-type': [\n        'WIDGET',\n        'KEYWORD',\n    ],\n    'alias': 'aps',\n    'plain-mid': '1',\n}\n\nresponse = requests.get('https://completion.amazon.com/api/2017/suggestions', params=params, headers=headers)\n\n\n\n8. Recycle and reuse\nWith the stripped down request, try to submit a few— let’s say 10 or 20, requests with new parameters set by you.\nFor convenience, we can write the stripped down API call as a function that takes any keyword as input.\n\nimport pandas as pd\nimport time\n\ndef search_suggestions(keyword):\n    \"\"\"\n    Get autocompleted search suggestions for a `keyword` search on Amazon.com.\n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'Accept-Language': 'en-US,en;q=0.5',\n    }\n\n    params = {\n        'prefix': keyword,\n        'suggestion-type': [\n            'WIDGET',\n            'KEYWORD',\n        ],\n        'alias': 'aps',\n        'plain-mid': '1',\n    }\n\n    response = requests.get('https://completion.amazon.com/api/2017/suggestions', \n                            params=params, headers=headers)\n    return response.json()\n\nIn this step the code gets refactored to make it repeatable and reusable, but it’s a bit of a jump if you’re not a coder. If that’s you, still try to read the code – you should be able to get a rough idea of what does what, and how it’s similar to the code you had in the last step.\nHere we can set new input parameters in keyword, and make the an API call using each keyword. Try changing some of the code (eg. the keywords) and rerunning it to check your understanding.\n\n# Here are our inputs (what searches we'll get autocompleted)\nkeywords = [\n    'a', 'b', 'cookie', 'sock', 'zelda', '12'\n]\n\n# Here we'll go through each input, get the suggestions, and then add the `suggestions` to a list.\ndata = []\nfor keyword in keywords:\n    suggestions = search_suggestions(keyword)\n    suggestions['search_word'] = keyword # keep track of the seed keyword\n    time.sleep(1) # best practice to put some time between API calls.\n    data.extend(suggestions['suggestions'])\n\nWe saved the API responses in a list called data, and put them into a Pandas DataFrame to analyze.\n\ndf = pd.DataFrame(data)\n\n# show 5 random auto suggestions\ndf.sample(5, random_state=303)\n\n\n\n\n\n  \n    \n      \n      suggType\n      type\n      value\n      refTag\n      candidateSources\n      strategyId\n      prior\n      ghost\n      help\n      queryUnderstandingFeatures\n    \n  \n  \n    \n      4\n      KeywordSuggestion\n      KEYWORD\n      asmanex twisthaler 30 inhaler\n      nb_sb_ss_i_5_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      13\n      KeywordSuggestion\n      KEYWORD\n      bathroom organizer\n      nb_sb_ss_i_4_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      19\n      KeywordSuggestion\n      KEYWORD\n      baby wipes\n      nb_sb_ss_i_10_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      12\n      KeywordSuggestion\n      KEYWORD\n      baby registry search\n      nb_sb_ss_i_3_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n    \n      16\n      KeywordSuggestion\n      KEYWORD\n      b013xkha4m b08xzrxczm b07xxphqzk b09rwjblc7\n      nb_sb_ss_i_7_1\n      local\n      organic\n      0.0\n      False\n      False\n      [{'source': 'QU_TOOL', 'annotations': []}]\n    \n  \n\n\n\n\nIf you look at the columns, you might be flooded with more questions:\n\nSome terms may be blackListed, what does that mean and what words, if any, are blackListed = True?\nAre some searches paid for, and not organic?\nWhat is ghost?\n\nThis metadata is only visible from the API, and can lead to new story ideas and directions to pursue.\nUnfortunately, because this API is undocumented, asking these questions and figuring out what everything represents is difficult. Use your curiosity and look at many examples. The feature of the API is being able to make many queries at scale, which should help answer these questions. Reporting this out with sources is also essential in this process."
  },
  {
    "objectID": "apis.html#do-it-yourself",
    "href": "apis.html#do-it-yourself",
    "title": "Finding Undocumented APIs",
    "section": "Do it yourself",
    "text": "Do it yourself\nFind an API in the wild, isolate it, strip it down, reverse-engineer it and analyze some of its results.\nIf a website has a search bar or a text box that queries a server or database, there’s a good chance that you can find an API.\nRevisit the steps we outlined above, and apply them to a new website. If you aren’t a coder, try to get steps 1-6 (I believe in you!).\nIf you are a coder, try some of the advanced usage below.\n\nFor advanced usage…\n\nHandle errors for bad requests, rate limiting, and other issues that could arise.\nRestructure the API response to better analyze (called “parsing” the data).\nyou can use session instead of pure requests. This is helpful if cookies and authentication are involved. Read more about that here.\nyou can make a request asynchronous to speed up data collection (without overloading the site’s servers, of course).\nImplement steps 6-onwards in another programming language."
  },
  {
    "objectID": "apis.html#predetermined-prompts",
    "href": "apis.html#predetermined-prompts",
    "title": "Finding Undocumented APIs",
    "section": "Predetermined prompts",
    "text": "Predetermined prompts\nDon’t know where to look? Here are some ideas:\n\nYouTube recommendations.\nBlacklight’s API to find third-party trackers on a website.\nAmtrak’s train statuses."
  },
  {
    "objectID": "apis.html#homework-assignment",
    "href": "apis.html#homework-assignment",
    "title": "Finding Undocumented APIs",
    "section": "Homework assignment",
    "text": "Homework assignment\nPractice: Keep trying to find APIs in the wild. Think about the websites you frequent, topics that interest you, or stories you’re currently working on. You won’t always find an API, and that’s OK.\nScoping: Determine how the API could be used to produce data to answer a reporting question or hypothesis. What will be your sample for a quick test, i.e. how many data points are enough to know if you have something?\nReporting: Determine the meaning and significance of hidden fields that are returned.\nUltimately APIs are a tool, and data is useless without a purpose. Hopefully this worksheet helps you in your time of need."
  },
  {
    "objectID": "apis.html#artifacts",
    "href": "apis.html#artifacts",
    "title": "Finding Undocumented APIs",
    "section": "Artifacts",
    "text": "Artifacts\nSlides from workshops can be found here:\n2023-02-24 @ Tow Center Columbia 2023-03-04 @ NICAR 2023-06-12 @ FAccT 2023-06-14 @ Journocoders 2023-06-22 @ C+J DATAJ"
  },
  {
    "objectID": "apis.html#acknowledgements",
    "href": "apis.html#acknowledgements",
    "title": "Finding Undocumented APIs",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThank you Max Harlow for suggestions in making the tutorial easier to understand, and a well-compressed thank you to Simon Fondrie-Teitler for helping optimize this page.\n\n\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, edited by Sorelle A. Friedler and Christo Wilson, 81:77–91. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nCalacci, Dan, Jeffrey J. Shen, and Alex Pentland. 2022. “The Cop in Your Neighbor’s Doorbell: Amazon Ring and the Spread of Participatory Mass Surveillance.” Proc. ACM Hum.-Comput. Interact. 6 (CSCW2). https://doi.org/10.1145/3555125.\n\n\nCameron, Dell, and Dhruv Mehrota. 2019. “Ring’s Hidden Data Let Us Map Amazon’s Sprawling Home Surveillance Network.” Gizmodo, December. https://gizmodo.com/ring-s-hidden-data-let-us-map-amazons-sprawling-home-su-1840312279.\n\n\nGarcia, David, Yonas Mitike Kassa, Angel Cuevas, Manuel Cebrian, Esteban Moro, Iyad Rahwan, and Ruben Cuevas. 2018. “Analyzing gender inequality through large-scale Facebook advertising data.” Proceedings of the National Academy of Science 115 (27): 6958–63. https://doi.org/10.1073/pnas.1717781115.\n\n\nMajor, David, Ross Teixeira, and Jonathan Mayer. 2020. “No WAN’s Land: Mapping u.s. Broadband Coverage with Millions of Address Queries to ISPs.” In Proceedings of the ACM Internet Measurement Conference, 393–419. IMC ’20. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3419394.3423652.\n\n\nSap, Maarten, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. “The Risk of Racial Bias in Hate Speech Detection.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 1668–78. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1163.\n\n\nWillis, Derek. 2013. “Freeing the Plum Book.” Source, April. https://source.opennews.org/articles/freeing-plum-book/."
  },
  {
    "objectID": "browser_automation.html",
    "href": "browser_automation.html",
    "title": "Browser Automation",
    "section": "",
    "text": "Intro\nIf you’ve tried to buy concert tickets to a popular act lately, you’ve probably watched in horror as the blue “available” seats evaporate before your eyes the instant tickets are released. Part of that may be pure ✨star power✨, but more than likely, bots were programmed to buy tickets to be resold at a premium.\nThese bots are programmed to act like an eager fan: waiting in the queue, selecting a seat, and paying for the show. These tasks can all be executed using browser automation.\nBrowser automation is used to programmatically interact with web applications.\nThe most frequent use case for browser automation is to run tests on websites by simulating user behavior (mouse clicks, scrolling, and filling out forms). This is routine and invisible work that you wouldn’t remember, unlike seeing your dream of crowd surfing with your favorite musician disappear thanks to ticket-buying bots.\nBut browser automation has another use, one which may make your dreams come true: web scraping.\nBrowser automation isn’t always the best solution for building a dataset, but it is necessary when you need to:\nThese reasons are often interrelated. We will walk through case studies (below) that highlight at least one of these strengths, as well as why browser automation was a necessary choice.\nSome popular browser automation tools are Puppeteer, Playwright, and Selenium.\nIn the hands-on tutorial we will attempt to study personalization on TikTok with a mock experiment.\nWe’re going to teach you the basics of browser automation in Selenium, but the techniques we’ll discuss could be used to study any other website using any other automation tool.\nWe will try to replicate elements of the WSJ investigation and see if we can trigger a personalized “For You” page. Although the WSJ ran their investigation using an Android on a Raspberry Pi, we will try our luck with something you can run locally on a personal computer using browser automation.\nIn this tutorial we’ll use Selenium to watch TikTok videos where the description mentions keywords of our choosing, while skipping all others. In doing so, you will learn practical skills such as:\nImportantly, we’ll be watching videos with lighter topics than depression (the example chosen in the WSJ investigation.).\nMore tutorials on the same subject:\nNotable investigations, audits, and tools using browser automation:\nPlease reach out with more examples to add.\nTo cite this chapter, please use the following BibTex entry:"
  },
  {
    "objectID": "browser_automation.html#headless-browsing",
    "href": "browser_automation.html#headless-browsing",
    "title": "Browser Automation",
    "section": "Headless Browsing",
    "text": "Headless Browsing\nBrowser automation can be executed in a “headless” state by some tools.\nThis doesn’t mean that the browser is a ghost or anything like that, it just means that the user interface is not visible.\nOne benefit of headless browsing is that it is less resource intensive, however there is no visibility into what the browser is doing, making headless scrapers difficult to debug.\nLuckily, some browser automation tools (such as Selenium) allow you to toggle headless browsing on and off. Other tools, such as Puppeteer only allow you to use headless browsing.\nIf you’re new to browser automation, we suggest not using headless browsing off the bat. Instead try Selenium (or Playwright), which is exactly what we’ll do in the tutorial below.\n\n\n\nUsing Selenium to automate browsing TikTok’s “For You” page for food videos."
  },
  {
    "objectID": "browser_automation.html#case-study-1-google-search",
    "href": "browser_automation.html#case-study-1-google-search",
    "title": "Browser Automation",
    "section": "Case Study 1: Google Search",
    "text": "Case Study 1: Google Search\nIn the investigation “Google the Giant,” The Markup wanted to measure how much of a Google Search page is “Google.” Aside from the daunting task of classifying what is “Google,” and what is “not Google,” the team of two investigative journalists– Adrianne Jeffries and Leon Yin (a co-author of this section) needed to measure real estate on a web page.\nThe team developed a targeted staining technique inspired by the life sciences, originally used to highlight the presence of chemicals, compounds, or cancers.\n\n\n\nSource: The Markup\n\n\nThe reporters wrote over 68 web parsers to identify elements on trending Google Search results as “Google,” or three other categories. Once an element was identified, they could find the coordinates of each element along with its corresponding bounding box. Using the categorization and bounding box, The Markup were able to measure how many pixels were allocated to Google properties, as well as where they were placed on a down the page for a mobile phone.\n\n\n\nSource: The Markup\n\n\nBrowser automation tools’ ability to collect and analyze rendered HTML pages can be essential. This is especially the case for search results, since most search results contain modules, carousels, and other non-standardized rows and columns that are more complex than lists.\nRendered HTML can be used to analyze the allocation of real estate on a website, which can be a useful metric to gauge self-preferencing and anti-competitive business practices relevant to antitrust. Take for example this case study, which was placed above the others because one of this section’s co-authors happened to work on it."
  },
  {
    "objectID": "browser_automation.html#case-study-2-deanonymizing-googles-ad-network",
    "href": "browser_automation.html#case-study-2-deanonymizing-googles-ad-network",
    "title": "Browser Automation",
    "section": "Case Study 2: Deanonymizing Google’s Ad Network",
    "text": "Case Study 2: Deanonymizing Google’s Ad Network\nGoogle ad sellers offer space on websites like virtual billboards, and are compensated by Google after an ad is shown. However, unlike physical ad sellers, almost all of the ~1.3 million ad sellers on Google are anonymous. To limit transparency further, multiple websites and apps can be monetized by the same seller, and it’s not clear which websites are part of Google’s ad network in the first place.\nAs a result, advertisers and the public do not know who is making money from Google ads. Fortunately, watchdog groups, industry analysts, and reporters have developed methods to hold Google accountable for this oversight.\nThe methods boil down to triggering a JavaScript function that sends a request to Google to show an ad on a loaded web page. Importantly, the request reveals the seller ID used to monetize the website displaying the ad, and in doing so, links the seller ID to the website.\nIn 2022, reporters from ProPublica used Playwrite to automated this process to visit 7 million websites and deanonymize over 900,000 Google ad sellers. Their investigation found some websites were able to monetize advertisements, despite breaking Google’s policies.\nProPublica’s investigation used browser automation tools to trigger event execution to successfully load ads. Often, this required waiting a page to fully render, scrolling down to potential ad space, and browsing multiple pages. The reporters used a combination of network requests, rendered HTML, and cross-referencing screenshots to confirm that each website monetized ads from Google’s ad network.\nBrowser automation can help you trawl for clues, especially when it comes to looking for specific network requests sent to a central player by many different websites."
  },
  {
    "objectID": "browser_automation.html#case-study-3-tiktok-personalization",
    "href": "browser_automation.html#case-study-3-tiktok-personalization",
    "title": "Browser Automation",
    "section": "Case Study 3: TikTok Personalization",
    "text": "Case Study 3: TikTok Personalization\nAn investigation conducted by the Wall Street Journal, “Inside TikTok’s Algorithm” found that even when a user does not like, share, or follow any creators, TikTok still personalizes their “For You” page based on how long they watch the recommended videos.\nIn particular, the WSJ investigation found that users who watch content related to depression and skip other content are soon presented with mental health content and little else. Importantly, this effect happened even when the users did not explicitly like or share any videos, nor did they follow any creators.\nYou can watch the WSJ’s video showing how they mimic user behavior to study the effects of personalization:\n\n\n\n\n\nSource: WSJ\n\n\nThis investigation was possible only after simulating user behavior and triggering personalization from TikTok’s “For You” recommendations."
  },
  {
    "objectID": "browser_automation.html#step-1-setting-up-the-browser",
    "href": "browser_automation.html#step-1-setting-up-the-browser",
    "title": "Browser Automation",
    "section": "Step 1: Setting up the browser",
    "text": "Step 1: Setting up the browser\nOur setup will consist of a real browser and an interface that will allow us to control that browser using Python. We chose Google Chrome because it’s the most popular browser and easy enough (famous last words) to set up.\n\n1.1 Installing Google Chrome\nPlease download the most recent version here.\nIf you already have Google Chrome installed, make sure it’s a latest version by opening Chrome and pasting this address in the address bar: chrome://settings/help. Now verify that there are no pending updates.\n\n\n\n1.2 Installing the webdriver\nThe webdriver is our interface between Python and the browser. It is specific to the browser (there are different webdrivers for Firefox [called Gecko], Safari, etc) and even to the particular version of the browser. It’s easier to ensure we are working with the correct version by installing a webdriver that automatically detects the current version of Chrome.\nRun the code in the cell below to download the Python package chromedriver-binary-auto. Adding an exclamation mark before code in Jupyter notebook allows you to run commands as if you were in your computer terminal’s command line\n\n!pip install chromedriver-binary-auto\n\nCollecting chromedriver-binary-auto\n  Downloading chromedriver-binary-auto-0.2.6.tar.gz (5.2 kB)\n  Preparing metadata (setup.py) ... done\nBuilding wheels for collected packages: chromedriver-binary-auto\n  Building wheel for chromedriver-binary-auto (setup.py) ... done\n  Created wheel for chromedriver-binary-auto: filename=chromedriver_binary_auto-0.2.6-py3-none-any.whl size=8652851 sha256=1ccd18edd04cf5e1c63e0305676dc1c9c0c0532c8dc09842f6cf963a910e4f04\n  Stored in directory: /Users/leon/Library/Caches/pip/wheels/2a/4e/a6/e342ab457a4cd1642a94bbc8f132e56e90a7a320d08d6bfeb2\nSuccessfully built chromedriver-binary-auto\nInstalling collected packages: chromedriver-binary-auto\nSuccessfully installed chromedriver-binary-auto-0.2.6\n\n\nLet’s see if the installation worked correctly! Run the cell below to import the correct webdriver and open a new Chrome window.\n\nfrom selenium import webdriver\nimport chromedriver_binary # adds the chromedriver binary to the path\n\ndriver = webdriver.Chrome()\n\nThe chrome-driver-auto package should have installed a driver that’s suitable for your current Chrome version running the line of code above should have opened a new Chrome window.\nThis step is notoriously hard, and you might get a version mismatch error:\nSessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 112\nCurrent browser version is 113 with binary path /Applications/Google Chrome.app/Contents/MacOS/Google Chrome\nIt means that you probably updated your Chrome in the meantime. To fix it, reinstall the Python package:\n\n!pip install --upgrade --force-reinstall chromedriver-binary-auto\n\nCollecting chromedriver-binary-auto\n  Using cached chromedriver_binary_auto-0.2.6-py3-none-any.whl\nInstalling collected packages: chromedriver-binary-auto\n  Attempting uninstall: chromedriver-binary-auto\n    Found existing installation: chromedriver-binary-auto 0.2.6\n    Uninstalling chromedriver-binary-auto-0.2.6:\n      Successfully uninstalled chromedriver-binary-auto-0.2.6\nSuccessfully installed chromedriver-binary-auto-0.2.6\n\n\nIf everything works fine and you have the window open, our setup is complete and you can now close the Chrome window:\n\ndriver.close()"
  },
  {
    "objectID": "browser_automation.html#step-2-hiding-typical-tells-of-an-automated-browser",
    "href": "browser_automation.html#step-2-hiding-typical-tells-of-an-automated-browser",
    "title": "Browser Automation",
    "section": "Step 2: Hiding typical tells of an automated browser",
    "text": "Step 2: Hiding typical tells of an automated browser\nWhen you open Chrome with Selenium you’ll notice that the window displays a warning about being an “automated session”. Even though the warning is only displayed to you, the webdriver leaves behind other red flags that inform website administrators that you are using browser automation.\nThe website admins will use these red flags to refuse service to your browser.\nLet’s remove those.\n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"start-maximized\")\n\n# remove all signs of this being an automated browser\noptions.add_argument('--disable-blink-features=AutomationControlled')\noptions.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\noptions.add_experimental_option('useAutomationExtension', False)\n\n# open the browser with the new options\ndriver = webdriver.Chrome(options=options)\ndriver.get('https://tiktok.com/foryou')\n\nThis should open a new window without those warnings and navigate to tiktok.com:"
  },
  {
    "objectID": "browser_automation.html#step-3-finding-elements-on-page-and-interacting-with-them",
    "href": "browser_automation.html#step-3-finding-elements-on-page-and-interacting-with-them",
    "title": "Browser Automation",
    "section": "Step 3: Finding elements on page and interacting with them",
    "text": "Step 3: Finding elements on page and interacting with them\nWe will perform our mock experiment without logging in (but we will also learn how to create multiple accounts and how to log in later).\nInstead of logging in, our first interaction will be dismissing this login window. Doing this programmatically has two steps:\n\nWe need to identify that [X] button in the page source\nAnd then click it\n\nLet’s inspect the button element: \nIn my case, the particular element that the Developer Tools navigated to is just the graphic on the button, not the button itself, but you can still find the actual button by hovering your mouse over different elements in the source and seeing what elements on page are highlighted:\n\nOur close button is a <div> element, whose data-e2e attribute is \"modal-close-inner-button\".\nThere are many ways to fish for the exact element you want, and many of those methods are built into Selenium. One way to find it would be using a CSS_SELECTOR, like so:\n\nfrom selenium.webdriver.common.by import By\n\nclose_button = driver.find_element(By.CSS_SELECTOR, '[data-e2e=\"modal-close-inner-button\"]')\nclose_button\n\n<selenium.webdriver.remote.webelement.WebElement (session=\"710ad56950f0c245bc25a21b57ccf110\", element=\"CD049116A9F70D8AA3FE1F0BC1BBEB15_element_298\")>\n\n\nIf Selenium successfully finds an element, you’ll get a WebElement object of the first match. However, if Selenium does not find the element– for example because the element hasn’t loaded yet, you will get an empty object in return. This will crash your script if you try to interact with the empty element.\nOne thing you can do is to tell Selenium to wait up to X_seconds for that particular element before trying to click on it, like this:\n\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# let's wait up to 20 seconds\nX_seconds = 20\nwait = WebDriverWait(driver, timeout = X_seconds)\nwait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-e2e=\"modal-close-inner-button\"]')))\n\n# this line will only execute whenever the element was found (or after 20 seconds it it wasn't)\nclose_button = driver.find_element(By.CSS_SELECTOR, '[data-e2e=\"modal-close-inner-button\"]')\nclose_button\n\n<selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_45\")>\n\n\nWe seem to have found something, let’s click it! WebElements come equipped with special functions you can use to interact with them:\n\nclose_button.click()\n\nDid you notice a change on the page? Congratulations! You just automated the browser to click something."
  },
  {
    "objectID": "browser_automation.html#step-4-scrolling",
    "href": "browser_automation.html#step-4-scrolling",
    "title": "Browser Automation",
    "section": "Step 4: Scrolling",
    "text": "Step 4: Scrolling\nWe now have a browser instance open and displaying the For You page. Let’s scroll through the videos.\nIf you are a real person who (for whatever reason) visits TikTok on their computer, you could press the down key the keyboard to see new videos. We will do that programmatically instead:\n\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.keys import Keys\n\nactions = ActionChains(driver)\nactions.send_keys(Keys.ARROW_DOWN)\nactions.perform()\n\nWhen you run the cell above you will see that your browser scrolls down to the next video. You just automated scrolling!"
  },
  {
    "objectID": "browser_automation.html#step-5-finding-tiktok-videos-on-the-page",
    "href": "browser_automation.html#step-5-finding-tiktok-videos-on-the-page",
    "title": "Browser Automation",
    "section": "Step 5: Finding TikTok videos on the page",
    "text": "Step 5: Finding TikTok videos on the page\nNow that the site loaded and you can browse it, let’s find all the TikTok videos that are displayed and extract the information (called metadata) from each of them.\n\nRight click on the white space around a TikTok video and choose “Inspect”. \nHover your mouse over the surrounding <div> elements and observe the highlighted elements on the page to see which ones correspond to each TikTok video. \nYou will see that each video is in a separate <div> container but each of these containers has the same data-e2e attribute with the value of recommend-list-item-container.\nSimilarly to how we found the close button, we can now use this to find all videos on page:\n\n\nvideos = driver.find_elements(By.CSS_SELECTOR, '[data-e2e=\"recommend-list-item-container\"]')\n\nWhen we searched for the “dismiss” button we used the driver.find_element() function because we were only interested in the first element that matched our CSS selector.\nNow we’re trying to find all videos on page, so we use the driver.find_elements() function instead - it returns the complete list of elements that match the selector.\n\nvideos\n\n[<selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_73\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_78\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_101\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_160\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_161\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_162\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_163\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_164\")>,\n <selenium.webdriver.remote.webelement.WebElement (session=\"64b92d16043e500c3630742e6fa746d2\", element=\"7B4533C51CF631265A8665122C637150_element_165\")>]"
  },
  {
    "objectID": "browser_automation.html#step-6-parsing-tiktok-metadata",
    "href": "browser_automation.html#step-6-parsing-tiktok-metadata",
    "title": "Browser Automation",
    "section": "Step 6: Parsing TikTok metadata",
    "text": "Step 6: Parsing TikTok metadata\nNow that we found all the TikTok videos on the page, let’s extract the description from each - this is how we will decide whether to watch the video, or to skip it. The process of extracting a specific field from a webpage is “parsing”.\n\nPick any description, right click, “Inspect”.\nLet’s locate the <div> that contains the whole description (including any hashtags) and make note of its data-e2s attribute.\nNow let’s write the code that, extracts the description from a single video (note that you can get the text content of any element by calling element.text)\n\n\nfor video in videos:\n    print(video.find_element(By.CSS_SELECTOR, '[data-e2e=\"video-desc\"]').text)\n\n\nThe last one 😂😂 #pet #cat #dog #cute #animals #funny #foryou #fyp\nالرد على @hadeelalsamare #اكسبلور #fyp #fypシ\nBEST MAGIC TRICKS REVEALED 😱😳 #magician #learnfromme #foru #popular\nThe most Useful Toy ever! 2 😂 #fun #play #fyp\nIphone 13 pro max #repair #tamarshabi🥰 תיקון\nHerb-Crusted Rack of Lamb 😍 #lamb #easyrecipe #easyrecipes #asmrfood #foodtok #cooktok #dinnerwithme #homecook #homecooking #dinnerideas #dinnerparty\n#fyp #halsey #geazy #scandal\nشو رأيكم كان فيها تكفي اللقمة اللي بتمها؟ 😐#hasanandhawraa #ramdan2023 #رمضان_يجمعنا #رمضان\n\n\n\n\n\n\n\n\nNote: We previously searched for elements using driver.find_element() and driver.find_elements(). That allowed us to search the whole page. Notice that here, instead of driver, we’re using a particular element which we called video: this way we can search for elements within an element, rather than on the whole page."
  },
  {
    "objectID": "browser_automation.html#step-7-finding-the-tiktok-video-thats-currently-playing",
    "href": "browser_automation.html#step-7-finding-the-tiktok-video-thats-currently-playing",
    "title": "Browser Automation",
    "section": "Step 7: Finding the TikTok video that’s currently playing",
    "text": "Step 7: Finding the TikTok video that’s currently playing\nWe know how to scroll to the next video, and we know how to find all videos that are loaded. At this point we could either:\n\nAssume that at the beginning, the 0th video is playing, and then every time we press arrow down, the next video is being displayed\nAssume that the arrow down does not always work and each time verify which video is actually playing\n\nThe problem with the first approach is that even if scrolling fails just once, our experiment will be compromised (after it happens we will be watching and skipping different videos that our script tells us). This is why we will go with the second approach and verify which video is actually playing. Back to our favorite tool- inspect element!\nWhen you right click on the playing video, you will see that instead of our familiar UI we get a custom TikTok menu, so that won’t work. Try right-clicking on the description of the video instead, then hovering over different elements in the inspector and expanding the one that highlights the video in the browser. Dig deep until you get to the div that only contains the video.\nStill in the inspector try looking at the video below. You will see that the div that contains the video is missing and there is no element with the tag name video. That’s how we can find if the video is currently playing - its div will contain the video element that we can find by TAG_NAME:\n\nfor video in videos:\n    description = video.find_element(By.CSS_SELECTOR, '[data-e2e=\"video-desc\"]').text\n    if video.find_elements(By.TAG_NAME, 'video'):\n        playing = 'playing'\n    else:\n        playing = 'not playing'\n    print(playing, description)\n\nplaying \nnot playing The last one 😂😂 #pet #cat #dog #cute #animals #funny #foryou #fyp\nnot playing الرد على @hadeelalsamare #اكسبلور #fyp #fypシ\nnot playing BEST MAGIC TRICKS REVEALED 😱😳 #magician #learnfromme #foru #popular\nnot playing The most Useful Toy ever! 2 😂 #fun #play #fyp\nnot playing Iphone 13 pro max #repair #tamarshabi🥰 תיקון\nnot playing Herb-Crusted Rack of Lamb 😍 #lamb #easyrecipe #easyrecipes #asmrfood #foodtok #cooktok #dinnerwithme #homecook #homecooking #dinnerideas #dinnerparty\nnot playing #fyp #halsey #geazy #scandal\nnot playing شو رأيكم كان فيها تكفي اللقمة اللي بتمها؟ 😐#hasanandhawraa #ramdan2023 #رمضان_يجمعنا #رمضان"
  },
  {
    "objectID": "browser_automation.html#step-8-taking-screenshots-and-saving-page-source",
    "href": "browser_automation.html#step-8-taking-screenshots-and-saving-page-source",
    "title": "Browser Automation",
    "section": "Step 8: Taking screenshots and saving page source",
    "text": "Step 8: Taking screenshots and saving page source\nThe presentation of your results might be more compelling, when its accompanied by screenshots, rather than just data. Selenium allows you to take screenshots of the whole screen, or just a particular element (though the latter is a bit cumbersome):\n\n# take a screenshot of the whole browser\ndriver.save_screenshot('full_screenshot.png')\n\n# take a screenshot of just one video\nscreenshot = video.screenshot_as_png\nwith open('element_screenshot.png', 'wb') as output:\n    output.write(screenshot)\n\nIn the spirit of bringing receipts, you can also save the entire webpage to parse it later.\n\n# save the source of the entire page\npage_html = driver.page_source\nwith open('webpage.html', 'w') as output:\n    output.write(page_html)\n\n\n\n\n\n\n\nPro tip: Keep these records to sanity check your results\n\n\n\nTaking a screenshot and saving the page source is a useful practice for checking your work. Use the two to cross-reference what was visible in the browser and whatever data you end up extracting during the parsing step.\n\n\nLet’s close the browser for now, and kick this workflow up a notch.\n\ndriver.close()"
  },
  {
    "objectID": "browser_automation.html#step-9-putting-it-all-together",
    "href": "browser_automation.html#step-9-putting-it-all-together",
    "title": "Browser Automation",
    "section": "Step 9: Putting it all together",
    "text": "Step 9: Putting it all together\nAt this point, we can read the description of TikTok videos and navigate the “For You” page.\nThat’s most of the setup we need to try our mock experiment: let’s watch all TikTok videos that mention food in the description and skip videos that do not mention food.\nAfter one hundred videos, we will see whether we are served videos from FoodTok more frequently than other topics.\n\n\n\n\n\n\nPro tip: Use functions!\n\n\n\nSo far we wrote code to open the browser, close the dialog, and find videos as separate cells in the notebook. We could copy that code over here to use it, but it will be much easier to understand and maintain the code if we write clean, well-documented functions with descriptive names.\n\n\n\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nimport chromedriver_binary\n\n\n\ndef open_browser():\n    \"\"\"\n    Opens a new automated browser window with all tell-tales of automated browser disabled\n    \"\"\"\n    options = webdriver.ChromeOptions()\n    options.add_argument(\"start-maximized\")\n\n    # remove all signs of this being an automated browser\n    options.add_argument('--disable-blink-features=AutomationControlled')\n    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n    options.add_experimental_option('useAutomationExtension', False)\n\n    # open the browser with the new options\n    driver = webdriver.Chrome(options=options)\n    return driver\n\ndef close_login_dialog(driver):\n    \"\"\"\n    Waits for the login dialog to appear, then closes it\n    \"\"\"\n    \n    # rather than trying to click a button that might have not loaded yet, we will \n    # wait up to 20 seconds for it to actually appear first\n    wait = WebDriverWait(driver, timeout = 20)\n    wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-e2e=\"modal-close-inner-button\"]')))\n   \n    close_button = driver.find_element(By.CSS_SELECTOR, '[data-e2e=\"modal-close-inner-button\"]')\n    if close_button:\n        close_button.click()\n\ndef arrow_down(driver):\n    \"\"\"\n    Sends the ARROW_DOWN key to a webdriver instance.\n    \"\"\"\n    actions = ActionChains(driver)\n    actions.send_keys(Keys.ARROW_DOWN)\n    actions.perform()\n    \ndef find_videos(driver):\n    \"\"\"\n    Finds all tiktoks loaded in the browser\n    \"\"\"\n    videos = driver.find_elements(By.CSS_SELECTOR, '[data-e2e=\"recommend-list-item-container\"]')\n    return videos\n\ndef get_description(video):\n    \"\"\"\n    Extracts the video description along with any hashtags\n    \"\"\"\n    try:\n        description = video.find_element(By.CSS_SELECTOR, '[data-e2e=\"video-desc\"]').text\n    except:\n        # if the description is missing, just get any text from the video\n        description = video.text\n    return description\n\ndef get_current(videos):\n    \"\"\"\n    Given the list of videos it returns the one that's currently playing\n    \"\"\"\n    for video in videos:\n        if video.find_elements(By.TAG_NAME, 'video'):\n            # this one has the video, we can return it and that ends the function.\n            return video\n    \n    return None\n\ndef is_target_video(description, keywords):\n    \"\"\"\n    Looks for keywords in the given description. \n    NOTE: only looks for the substring IE partial match is enough.\n    Returns `True` if there are any or `False` when there are none.\n    \"\"\"\n    # check in any of the keywords is in the description\n    for keyword in keywords:\n        if keyword in description:\n            # we have a video of interest, let's watch it \n            return True\n    \n    # if we're still here it means no keywords were found\n    return False\n\ndef screenshot(video, filename=\"screenshot.png\"):\n    \"\"\"\n    Saves a screenshot of a given video to a specified file\n    \"\"\"\n    screenshot = video.screenshot_as_png\n    with open(filename, 'wb') as output:\n        output.write(screenshot)\n    \ndef save_source(driver, filename=\"screenshot.html\"):\n    \"\"\"\n    Saves the browser HTML to a file\n    \"\"\"\n    page_html = driver.page_source\n    with open('webpage.html', 'w') as output:\n        output.write(page_html)\n\nOk, with that out of the way, let’s set up our first data collection.\nFirst, let’s make a directory to save screenshots. We will save screenshots here whenever we find a video related to food.\n\nimport os\n\nos.makedirs('data/screenshots/', exist_ok=True)\n\n\nimport time\n\n# if the description has any one these words, we will watch the video\nkeywords = ['food', 'dish', 'cook', 'pizza', 'recipe', 'mukbang', 'dinner', 'foodie', 'restaurant']\n\n# this is where will we store decisions we take\ndecisions = []\n\n# open a browser, and go to TikTok's For You page.\ndriver = open_browser()\ndriver.get('https://tiktok.com/foryou')\nclose_login_dialog(driver)\n\nfor tiktok_index in range(0, 100):\n    # get all videos\n    tiktoks = find_videos(driver)\n    \n    # the current tiktok is the one that's currently showing the video player\n    current_video = get_current(tiktoks)\n    \n    if current_video is None:\n        print('no more videos')\n        break\n              \n    # read the description of the video\n    description = get_description(current_video)\n    \n    # categorize the video as relevant to `keywords` or not.\n    contains_keyword = is_target_video(description, keywords)\n    decisions.append(contains_keyword )\n            \n    print(tiktok_index, contains_keyword, description)\n    \n    if contains_keyword:\n        # we have a video of interest, let's take a screenshot\n        ## here we declare the files we'll save. they're named according to their order.\n        fn_screenshot = f\"data/screenshots/screenshot_{tiktok_index:05}.png\"\n        fn_page_soure = fn_screenshot.replace('.png', '.html')\n        screenshot(current_video, fn_screenshot)\n        save_source(driver, fn_page_source)\n        # and now watch it for 30 seconds\n        time.sleep(30)\n    \n    # move to the next video\n    arrow_down(driver)\n    time.sleep(2)\n    \ndriver.close()\n\n0 False ДО КОНЦА😂 а какой у тебя рост?\n1 False • Reprodução: (SBT/Programa Raul Gil) 🇧🇷\n#combateaosuicidio\n2 False #stitch #이어찍기 #추천 #fyp #viral #xyzbca #korean #おすすめ\n3 True Cuando hago papas de esta manera, todos me preguntan por la receta😋😱#viral #parati #recetas #cocina #recetasfaciles #papa #queso #jamon #food #saborestiktok\n4 False #ومش_هزود_في_الملام #explore\n#fypシ #foryoupage #fyp #viral\n#مش_هنظبط_الريتش_بقي🖤 #حزين\n#حالات_واتس_حزينه💔 #foryou\n5 False #PasiondeGavilanes #telenovelacolombiana\n6 False #accident a veces pasa de todo 👉 sigueme para PARTE 2.\n7 False Zjedzcie se tez cos fajnego dzis #gotowaniezdominika\n8 False كيف تكتب اسم يوسف بخط جميل♥️🌹-\n-\n-\n-\n9 False بنت الجنوب 🔥🤍🇹🇳#مطماطة_قابس_تونس #اكسبلور\n10 False Game on\n11 False Чи бачите різницю між фото? Чи бачите які кадри зроблені на дорогу , а які на дешеву камеру? ☺️ #фотограф #фотоапарат #обзор #фотографія\n12 False #bendiciones #mideseo #TikTok #viral #\n13 False The most Useful Toy ever! 2 😂 #fun #play #fyp\n14 False Replying to @user4034722293618\n15 False jajeczniczka z kielbasiana\n16 False كام مره بكيت ؟ 🥺💔🎧 #المصمم_sheko🎧 #الرتش_فى_زمه_الله💔 #حالات_واتس #شاشه_سوداء #مصمم_حالات_واتس_شاشه_سوداء #fypシ #foryou #fyp #viral #music #tiktok\n17 False #movie #movieclip #fyp\n18 False Я ПРОТИВ КУРЕНИЯ, А ВЫ?\n19 False Uno de nuestros trends favoritos 😍🍭 @SHICKO 💊 @N E N A 🍓\n20 False Esse final me quebrou…🥺💛\n\n🎥Filme: Extraordinário\n\n#disciplina #motivacional #trechosvisionarios #extraordinar\n21 False Parece que o Vin Diesel curtiu “Vai Sentando” 😅\n22 False Para mi mama una niña valiente ♥️💕🇺🇸#parati #parati #parati #parati #parati #fyp #fyp #viral #viral #viral #viral #viral #viral #vistas #vistas #vistas ##vistas #vistas #muyviral @TikTok\n23 False #drawing #viralvideo🔥 #fypシ゚viral\n24 False شو رأيكم كان فيها تكفي اللقمة اللي بتمها؟ 😐#hasanandhawraa #ramdan2023 #رمضان_يجمعنا #رمضان\n25 False Brock is always there to save the day 🦈💪🏼 #wwe #wrestling #wrestlingmemes #brocklesnar #wweisfake #fakesituation #sharkattack #sharks #wwe2023 #nextgen #wwenetwork #smackdown #wwefan #bodybuilding #beach #holiday #pool #sea\n26 False HONEY, I SEE YOU #foryou #omspxd #music #mashonda #fyp #lyrics #speed #spedup #🎧\n27 False #fyp #mrbeast #foryou wow 😳😲\n28 False \n29 False Sometimes its better to save your breathe. #trauma #traumahealing #awakining #love #relationship #relatable #loveyourself #men #women #healing #problems #girltalk #therapy #couple #mom #fyp #fypシ #emotion\n30 False I love my body 🥰💜.. Dc: @Dance God 🦅🇬🇭 #purplespeedy\n31 False raye mi camioneta por ustedes jajajajajajaja\n32 False \n33 False My new car #catsoftiktok #fyp #fypシ\n34 False \n35 False Fiz um almoço insano! @Mateus ASMR\n36 False #bajonesemocionales #🥀💔\n37 False Don't mess with Cristiano 😤|| #cristianoronaldo #cr7 #mufc #intermilan #manutd #viral #ucl #tiktoktrending\n38 False This Small Town Farmer Better Buckle Up! - End #dealornodeal #show #fyp #deal\n39 False Genius, billionaire, playboy, philanthropist... and a great dancer🕺#downey #rdj #robertdowneyjr #ironman #tonystark #unrealdowneyjr #unrealrobertdowneyjr\n40 False Celebre as suas vitórias, amiga! 😍 Fazer 1% todos os dias vai te levar a lugares que você nem imagina.\nEu treino com a @Queima Diária 🔥 desde novembro de 2022 e fico muito feliz com esses resultados. Quem vem nessa comigo? Clica no link da bio ou nos stories e experimente por 30 dias!\n41 False اكتب شيء تؤجر عليه ✨🤍 #fyp #قران #عبدالرحمن_مسعد\n42 False I ❤️ Michael Jordan 🏀 #mercuri_88 #funny #littlebrother #tiktok #mom #CapCut #basketball #nba #jordan\n43 False Estavam com saudade? Nao me deixa sem graça nao caraaaa kkkkkk\n44 False يعني ارسمها علشان افرحها ويحصل معايا كدة 🤦‍♂️ #علي_الراوي\n45 False Ролик уже на канале💋\n46 False What k-drama do you think this is? #kdrama #드라마 #seoul #theglory\n47 False cat #cat #catsoftiktok #fun #foryou #fyp #viral #funny #😂😂😂 #🤣🤣🤣\n48 False #korea #seoul #socialexperiment #fyp\n49 False \n50 False #fyp #foryou #طيران\n51 False الماء والنار… 🥀💔 #lebrany #viral #foryou #explor\n52 False #foryou #recovery #homecare #gloves\n53 False Салат из одного ингредиента\n54 False #blog #vacuna Hoy tocó hacer vacunar a Salchipapu contra la rabia 🥺🐶\n55 False Song name: Jegi Jegi\nWatch full song on youtube ( Barbud Music )\n\n#lailakhan #newsong #rejarahish #tiktokpakistan\n56 False Putting automatic stickers on manual doors 😂 #rosscreations #prank\n57 False Abril 11 parte 7 “Comida Turka”\n58 True recipe: @ファビオ飯(イタリア料理人)🇮🇹Fabio #tiktokfood #asmr\n59 False Metallic silver epoxy floor🔥 #fyp #epoxyresin #garagegoals #epoxypour #polyasparticfloors #polyaspartic #theepoxypros\n60 False Enter the homepage to watch more wonderful videos#movieclips\n61 False Respuesta a @RZㅤＧＯＬＯＳＡღ -😅 @Duhsein\n62 False Почему «Титаник» до сих пор не подняли со дна океана? #титаник\n63 False Funny homework!✨✨#asmr #home #goodthing #foryou\n64 False 😂😂@도윤 #주전 #fyp\n65 False #parati #fyp #foryou #foryoupage #viral #trump #trump2024 #biden #teamtrump #donaldtrump\n66 False Não acreditei no resultado🥺🙌🏼\n67 False Atât de vrednică sunt… 😂\nM-am făcut de negreală pe obraz🤦🏻‍♀️😂 #soferițadecamion🚛😍 #AGLogistics #oriundeîneuropa #truckgirl\n68 False Gatinho Resgatado na chuva 🙏🏻 #jesus #jesuscristo #deus #resgateanimal #resgate #gato #gatinho #cat #viraliza\n69 False #pegar un video de\n@Yohary Michell Rios #maestra #maestros #universidad #universidad #clases #clasesvirtuales #profesora #profesor #fyp #parati #fouryou #fouyoupage #escuela #escuelatiktok #viral #\n70 False So cuteee😂\n71 False بوظتلهم الدنيا 😂\n72 False #pourtoi #foryou #cpl #bracelet #trend\n73 False What’s one way He’s held you as you’ve stepped out in faith? 🌊 #UNITED #fyp #christiantiktok #worship #Oceans\n74 False Antwort auf @🍇Wallah Krise🍇 I am going out tonight 💚 #bumpyride\n75 False #ليلياناا_نحن_عنوان_الجمال👑😍 #viral #fipシ #foryou #foryoupage #جمال #مكياج #شنيون #عرايس #لف #ميش #اكسبلور #لايك #هشتاك #مشاهير_تيك_توك #تخصيل\n76 False Full Episode 293 on YT & Spotify | ShxtsnGigs Podcast\n77 False GAME DE RUA COM LARRIKA! #gamederua #viral #fy #fypシ #pravoce #foryoupage\n78 False The smallest phone #CapCut #oppo #infinix #Motorola #zte #huawei #vivo #samsung\n79 False \n80 False Respect Moment in Football ❤️#footballeur #surprise #fan #respectmoment #respectinfootball #moment #respect #foryou #pourtoi #football\n81 False I think I got it in my pants 😧 #learnfromkhaby #comic\n82 False Respondendo a @hg_11236 ta aqui a reacao dela ❤️❤️❤️❤️ fofa demais! #fypシ #diadasmaes #surpresa\n83 False Наступ на Белгород. Що роблять добровольці там #війна #грайворон #белгород #українськийтікток #андрійковаленко\n84 False Have you ever eaten a cappuccino croissant? ☕️🥐\n.\n.\n.\n#pastry #pasticceria #italia #croissant\n85 False #recetas #facil whatia en tierrra\n86 False seyran inşallah gidersin feritinn bı kazimdan tokat yemediği kalmamisti#yalıcapkınıxferit #feritkorhan #seyrankorhan #mertramazandemir #afrasaraçoğlu #seyfer #yalıçapkını #keşfet #fypシ #foryoupage #foryou #viral\n87 False \n88 False La puissance de l’eau #pourtoi #meteo #inondation #eau #vigilance\n89 False Olha a aranha\n#alegriaquecontagia #comedia #viral #rireomelhorremedio #rireprosfortes #rirrenovaalma #gargalhada #fypシ #viralvideo #comediante #trolagem\n90 False Se puede ser infiel por chat? VIDEO COMPLETO EN EL LINK DE MI PERFIL ✅ #juliosinfiltros #relaciones #pareja #relacionessanas #infidelidad #infieles #microinfidelidades\n91 False Replying to @MC Codër\n92 False #kamalaghalan❣\n93 False Лобода про детей\n94 False Відмічай друга😅#українськийтікток #футболкизпринтами #подарунокхлопцю #подарунокдругу\n95 False Find your self worth.#real #loyalty #love #sad #sadquotes #relatable #betryal #foryou #scrolling #mindset #reality #xyzbca #fyp\n96 False #київ #вибух #нло #метеорит #ракета #сяйво #спалах #сніданокз1плюс1\n97 False اكثر مسلسل حبيتوها برمضان ؟#مهند_رفل #explore\n98 False المنتج اللي قالب التيك توك .. أسفنجة التنضيف السحرية 🧐 #حركة_لاكسبلورر #fyp #gym #عبدالرحمن_وابتسام #trendingtiktok #challenge #fypシ\n99 True Scotch Egg 😍🥚 #scotchegg #egg #easyrecipe #easyrecipes #caviar #eggs #asmrfood #bacon #cooktok #foodtok #recipesoftiktok #homecook #dinnerideas #eggrecipe #breakfastideas #fancy\n\n\n\n\n\n\n\n\nPro tip: Be careful about keywords\n\n\n\nFor experiments that use keywords, the choices we make will directly shape our results. In the field, you can mitigate your own predisposition and biases by working with domain experts to curate keyword lists.\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(decisions, ds='steps')\nplt.xlabel('Video Number')\nplt.ylabel('Watched')\nplt.yticks([0, 1], ['False', 'True']);\n\n\n\n\nThe figure above shows when during our 100-videos-long session we were recommended a video about food (from keywords). The x-axis is chronological, the 1st video displayed is on the left, and the most recent video is on the right. The y-axis is “yes” or “no,” depending on if the video was related to food.\n\nResults\nYou can look back to the data/screenshots folder we created to check whether the videos we watched appear to be food-related.\nIf the feed was indeed increasingly filled with food videos, we would see more lines towards the right of the graph. At least here it does not appear to be the case.\nDoes it mean that the WSJ investigation was wrong, or that TikTok stopped personalizing content?\nThe answer is “No,” for several reasons:\n\nWe only scrolled through 100 videos, this is likely too few to observe any effects. Try re-running with a higher number!\nWhen studying personalization you should use an account per profile and make sure you’re logged in, rather than relying on a fresh browser. So, instead of closing the login dialog, try actually logging in! You know how to find and click buttons, and this is how you put text in text fields.\nWhen you’re not logged in, you will be presented with content from all over the world, in all languages. If you filtered keywords in just one language, you will miss plenty of target content in other languages.\nYou should always have a baseline to compare to. In this case, you should probably run two accounts at the same time - one that watches food videos and one that doesn’t. Then you compare the prevalence of food videos between these two.\nThe WSJ investigation was run on the mobile app rather than on a desktop browser. Perhaps TikTok’s personalization works differently based on device or operating system."
  },
  {
    "objectID": "browser_automation.html#advanced-usage",
    "href": "browser_automation.html#advanced-usage",
    "title": "Browser Automation",
    "section": "Advanced Usage",
    "text": "Advanced Usage\nAbove we highlighted some ideas to make your investigation or study more robust, some are methodological choices, but others are technical.\nThere are some advanced use-cases and tasks you can perform with browser automation that include\n\nAuthentication using the browser and storing cookies for later use.\nIntercept background API calls and combine browser automation with API calls. See selenium-wire as an example.\nSigning in with one or more email addresses.\n\nWe may cover some or all of these topics in subsequent tutorials, but you should feel free to experiment.\nLet us know what you’re interested in learning more about!"
  },
  {
    "objectID": "browser_automation.html#acknowledgements",
    "href": "browser_automation.html#acknowledgements",
    "title": "Browser Automation",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThank you to Ruth Talbot and John West for answering questions about their two respective investigations."
  },
  {
    "objectID": "live_webscraping.html",
    "href": "live_webscraping.html",
    "title": "Live Scraping With R",
    "section": "",
    "text": "Intro\nBrowser automation can feel like magic, but it often requires thinking about a page from the perspective of a browser. R takes a human-centered approach. Behind the scenes, the code we write here will run an automated browser much like those demonstrated in the browser automation tutorial. But, unless called for, it will remain invisible, and we’ll be able to read in data more intuitively.\nTo cite this chapter, please use the following BibTex entry:"
  },
  {
    "objectID": "live_webscraping.html#installations",
    "href": "live_webscraping.html#installations",
    "title": "Live Scraping With R",
    "section": "Installations",
    "text": "Installations\nBefore getting started, make sure you have a copy of the Google Chrome browser installed in order to run Chromote. If using the R language for the first time, install the latest version of R here. I recommend also installing RStudio, an R-focused IDE."
  },
  {
    "objectID": "live_webscraping.html#getting-started",
    "href": "live_webscraping.html#getting-started",
    "title": "Live Scraping With R",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started, do one of the following:\n\nOpen RStudio (recommended)\n\nCreate a new project titled live-scraping-demo\nCreate an R script called scrape.R\n\nOr, open a new Jupyter Notebook and create an R cell\nOr, (Mac users) open an R session from the Terminal by typing the ‘R’ command\n\nWe’re now ready to run our first commands, which will install the libraries we need to complete the rest of the project. These libraries are large packages of pre-written code, largely created by Posit Chief Scientist and R developer Hadley Wickham. This tutorial is in part based on Hadley’s excellent NICAR session on live scraping, which you can find here.\n\n# General tools for the R language\ninstall.packages(\"tidyverse\")\n# Web scraping tools\ninstall.packages(\"rvest\")\n\nTo access these libraries, we’ll have to call them from the body of our R script. Add these lines to your scrape.R file.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nNow that we have the libraries we need, we’re ready to start scraping. For demonstration purposes, we’ll be collecting data from the Forbes rankings of U.S. colleges, but these same techniques can be applied to a wide range of sites. Let’s start with the simplest possible scraper: specifying a URL, and reading in that page so that we can work with it. We’ll progressively revise this code throughout the tutorial until we’ve collected all the data we need.\n\n# retrieve the page\npage <- read_html(\"https://www.forbes.com/top-colleges/\")\n\nLet’s print out the first few lines of the page to verify that we have the right one.\n\npage\n\n{html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<div id=\"__next\">\\n<div class=\"ForbesHeader_mainHeader__XuFcZ\"><h ...\n\n\nEverything looks good here. Time to start retrieving our data."
  },
  {
    "objectID": "live_webscraping.html#retrieving-the-first-table",
    "href": "live_webscraping.html#retrieving-the-first-table",
    "title": "Live Scraping With R",
    "section": "Retrieving the First Table",
    "text": "Retrieving the First Table\nBefore writing any more code, we’ll need to identify the HTML elements that contain our data. Open your browser’s developer tools. (In most standard browsers, this can be done by clicking the dot menu in the upper right and selecting “Developer Tools.”) I recommend keeping the developer panel open in a split-screen window in order to compare the page source to its visible elements. Using the selector tool in the upper left, click on the central table that contains college listings.\n\n\n\nThe table we want has the class ListTable_listTable__-N5U5. Typically, it’s best practice to use the most specific tag available when selecting elements to avoid mixups. There are tradeoffs to that approach, however. The designation -N5U5 looks largely random — if Forbes updates this page to use some other signifier, our code will stop working.\nInstead, I’ll recommend a simpler approach. Since the table we’re interested in is the only one on the page (that we can see), we can simply select the tag table. Let’s try it.\n\npage |> html_nodes(\"table\")\n\n{xml_nodeset (0)}\n\n\nThe code returns no results. Why? Despite the fact that we can see the table in the browser, it’s not present in the HTML we accessed with the read_html command. That’s because this is a “live”, or dynamic, page, one that renders significant parts of its layout — including our data — using JavaScript. Normally, scraping such a page would require browser automation. But the power of rvest will allow us to fix the problem by changing only one command.\n\npage <- read_html_live(\"https://www.forbes.com/top-colleges/\")\npage |> html_nodes(\"table\")\n\n{xml_nodeset (1)}\n[1] <table class=\"ListTable_listTable__-N5U5\">\\n<thead><tr>\\n<th class=\"ListT ...\n\n\nWhen we use the experimental read_html_live command instead of the traditional read_html, the code works as expected. We can now access the table, and all other data on this dynamic page, as a human user would. Let’s save the data we retrieved as an R dataframe.\n\ndf <- page |> html_node(\"table\") |> html_table()\n\nWe’ve now retrieved the data we set out to obtain in only five lines. The complete code for this scraper can be written as:\n\nlibrary(tidyverse)\nlibrary(rvest)\npage <- read_html_live(\"https://www.forbes.com/top-colleges/\")\nSys.sleep(1)\ndf <- page |> html_node(\"table\") |> html_table()"
  },
  {
    "objectID": "live_webscraping.html#data-cleaning",
    "href": "live_webscraping.html#data-cleaning",
    "title": "Live Scraping With R",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nUnfortunately, the table we’ve retrieved isn’t easy to work with. While largely correct, it contains the long-form callouts that Forbes includes for some institutions, such as Princeton. While these are interesting, they don’t belong in our dataframe. Let’s filter them out.\nThere are many possible approaches to removing junk data, but we’ll seek the simplest. By closely examining the table, we can see that the listings we’re interested in all have a numeric rank in the ‘Rank’ column, while the callout has a rank of NA. Let’s filter the dataframe to only include listings with a non-NA rank. We’ll use the filter command, which returns the rows of a dataframe for which the provided expression is True. We’ll specify the Rank column, and require that the Rank value for each row evaluate to some non-NA value.\n\ndf <- df |> filter(!is.na(Rank))"
  },
  {
    "objectID": "live_webscraping.html#retrieving-all-the-tables",
    "href": "live_webscraping.html#retrieving-all-the-tables",
    "title": "Live Scraping With R",
    "section": "Retrieving all the Tables",
    "text": "Retrieving all the Tables\nOur data table is now complete. Unfortunately, we’ve only collected the first 50 entries from Forbes’ list. The Forbes site is paginated, meaning we’ll have to click to a new page to get more data. Luckily, rvest provides a simple syntax for this.\n\npage$click('button[aria-label=\"Next\"]')\n\nWith only one line of code, we can select the ‘Next’ button at the bottom of the page and click it. If we rerun our code for scraping the data table, we’ll see that we’re now collecting the next page of data.\n\npage |> html_node(\"table\") |> html_table() |> filter(!is.na(Rank)) |> head()\n\n\n\nA tibble: 6 × 8\n\n    RankNameStateTypeAv. Grant AidAv. DebtMedian 10-year SalaryFinancial Grade\n    <int><chr><chr><chr><chr><chr><chr><chr>\n\n\n    51Middlebury College VTPrivate not-for-profit$56,639$8,541 $139,500A-\n    52Tufts University   MAPrivate not-for-profit$51,613$7,034 $144,400B+\n    53Boston University  MAPrivate not-for-profit$51,565$10,425$144,400B \n    54Wesleyan UniversityCTPrivate not-for-profit$59,825$9,716 $149,200B+\n    55William & Mary     VAPublic                $15,134$8,882 $134,200  \n    56Barnard College    NYPrivate not-for-profit$53,817$7,339 $149,300A-\n\n\n\n\nUnfortunately, our new data table has the same data quality problems as the first one. Let’s define a simple function for collecting ranking data from a page so that we don’t have to repeat ourselves.\n\nget_college_rankings <- function(page) {\n    df <- page |> html_node(\"table\") |> html_table()\n    df <- df |> filter(!is.na(Rank))\n    return(df)\n}\nget_college_rankings(page) |> head()\n\n\n\nA tibble: 6 × 8\n\n    RankNameStateTypeAv. Grant AidAv. DebtMedian 10-year SalaryFinancial Grade\n    <int><chr><chr><chr><chr><chr><chr><chr>\n\n\n    51Middlebury College VTPrivate not-for-profit$56,639$8,541 $139,500A-\n    52Tufts University   MAPrivate not-for-profit$51,613$7,034 $144,400B+\n    53Boston University  MAPrivate not-for-profit$51,565$10,425$144,400B \n    54Wesleyan UniversityCTPrivate not-for-profit$59,825$9,716 $149,200B+\n    55William & Mary     VAPublic                $15,134$8,882 $134,200  \n    56Barnard College    NYPrivate not-for-profit$53,817$7,339 $149,300A-\n\n\n\n\nWe can now retrieve data from each page of results with only one command. Let’s use that new shortcut to scrape in all ten pages of data. Start by scraping in the first page.\n\npage <- read_html_live(\"https://www.forbes.com/top-colleges/\")\nSys.sleep(1)\ndf <- get_college_rankings(page)\n\nNow, we’ll loop through the remaining pages, adding each new dataset to the main table before clicking the next button. We’ll stop the first time we find that the next button has been disabled, indicating that there are no more pages available.\n\nwhile (is.na(page |> html_node('button[aria-label=\"Next\"]') |> html_attr(\"disabled\"))) {\n    page$click('button[aria-label=\"Next\"]')\n    Sys.sleep(1)\n    df <- rbind(df, get_college_rankings(page))\n}\n\nAs a final step, we’ll export the data we collected to a CSV file for later use. Our complete program for scraping the Forbes college rankings can be written in only fifteen lines:\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nget_college_rankings <- function(page) {\n    df <- page |> html_node(\"table\") |> html_table()\n    df <- df |> filter(!is.na(Rank))\n    return(df)\n}\n\npage <- read_html_live(\"https://www.forbes.com/top-colleges/\")\nSys.sleep(1)\ndf <- get_college_rankings(page)\n\nwhile (is.na(page |> html_node('button[aria-label=\"Next\"]') |> html_attr(\"disabled\"))) {\n    page$click('button[aria-label=\"Next\"]')\n    Sys.sleep(1)\n    df <- rbind(df, get_college_rankings(page))\n}\n\nwrite_csv(df,\"forbes-rankings.csv\")\n\nAs I wrote above, read_html_live is an experimental tool, and more features are in active development. For more information, see Hadley Wickham’s documentation, including a summary of live scraping in rvest and an overview of commands for interacting with a live page’s UI. Feel free to reach out to me at declanrjb@gmail.com with any questions or suggestions for future tutorials."
  },
  {
    "objectID": "live_webscraping.html#acknowledgements",
    "href": "live_webscraping.html#acknowledgements",
    "title": "Live Scraping With R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThank you to Leon Yin for publishing this tutorial, and to Hadley Wickham for developing these tools and taking the time to teach journalists like me how to use them."
  },
  {
    "objectID": "best-practices-data-collection.html#dont-repeat-work",
    "href": "best-practices-data-collection.html#dont-repeat-work",
    "title": "Best practices for data collection",
    "section": "Don’t repeat work",
    "text": "Don’t repeat work\nBefore you collect data, check if you’ve already collected it.\nCreate a programmatic naming structure for a “target”– this could be a filename or a unique ID in a database, and check if it exists.\nIf it already exists, move on.\nBelow is a dummy example of a scraper for video metadata that checks if a file with the same video_id has already been saved.\n\nimport os\nimport time\n\ndef collect_video_metadata(video_id):\n    \"\"\"\n    This is an example of a data collection function\n    that checks if a video_id has already been collected.\n    \"\"\"\n    # consistently structure the target filename (fn_out)\n    fn_out = f\"video_metadata_{video_id}.csv\"\n    \n    # check if the file exists, if it does: move on\n    if os.path.exists(fn_out):\n        print(\"already collected\")\n        return\n        \n    # collect the data (not actually implemented)\n    print(\"time to do some work!\")\n    \n    # save the file. Instead of real data, we'll save text that says, \"Collected\".\n    with open(fn_out, 'w') as f:\n        f.write(\"Collected\")\n    return\n\nLet’s try to collect some video metadata for a video_id of our choosing.\n\nvideo_id = \"schfiftyfive\"\n\n\ncollect_video_metadata(video_id = video_id)\n\ntime to do some work!\n\n\nLet’s try to run the same exact function with the same input:\n\ncollect_video_metadata(video_id = video_id)\n\nalready collected\n\n\nThe second time you call it, the function ends early.\nWhen collecting a large dataset, these steps are essential to make the best use of time."
  },
  {
    "objectID": "best-practices-data-collection.html#make-a-todo-list",
    "href": "best-practices-data-collection.html#make-a-todo-list",
    "title": "Best practices for data collection",
    "section": "Make a todo list",
    "text": "Make a todo list\nIn addition to not repeating yourself, keep tabs on what needs to be done. That could be a simple CSV file, or something more advanced like a queuing system such as AWS SQS. For queuing systems, you can clear tickets that have been finished, and re-do tickets that might have failed."
  },
  {
    "objectID": "best-practices-data-collection.html#save-receipts",
    "href": "best-practices-data-collection.html#save-receipts",
    "title": "Best practices for data collection",
    "section": "Save receipts",
    "text": "Save receipts\nSave the output of every step, especially the earliest steps of collecting a JSON response from a server, or the HTML of a website.\nYou can always re-write parsers that turn that “raw” data into something neat and actionable.\nWebsites and API responses can change, so web parsers can break easily. It is safer to just save the data straight from the source, and process it later.\nIf you’re collecting a web page through browser automation, save a screenshot. It’s helpful to have reference material of what the web page looked like when you captured it.\nThis is something we did at the Markup when we collected Facebook data from a national panel over several months, and again, when we collected Google search results.\nThese receipts don’t just play a role in the underlying analysis, they can be used as powerful exhibits in your investigation.\n\n\n\nA screenshot of a Google Search for “Ida Tarbell” that was saved, and stained using a web parsing tool created for an investigation that found Google gives its own properties 41% of the first page of Search results."
  },
  {
    "objectID": "best-practices-data-collection.html#break-up-the-work-and-make-it-as-small-as-possible",
    "href": "best-practices-data-collection.html#break-up-the-work-and-make-it-as-small-as-possible",
    "title": "Best practices for data collection",
    "section": "Break up the work, and make it as small as possible",
    "text": "Break up the work, and make it as small as possible\nBreak scraping tasks into the smallest units of work. This makes scaling up easier, and it also prevents a single point of failure disrupting your entire workflow.\nCertain components of a scraper can be slower than others. By dividing the tasks, you can better identify bottlenecks to optimize the pipeline. Use to-do lists, and check for existing files to help communicate between tasks.\nRemember that big problems can be broken up into smaller problems. Being smart can help you get to the finish line faster and debug issues quicker."
  },
  {
    "objectID": "best-practices-data-collection.html#bigger-isnt-always-better",
    "href": "best-practices-data-collection.html#bigger-isnt-always-better",
    "title": "Best practices for data collection",
    "section": "Bigger isn’t always better",
    "text": "Bigger isn’t always better\nBe smart with how you use data, rather than depend on big numbers. Data isn’t in-itself valuable.\nIt’s better to start off smaller, with a trial analysis (we often call it a quick-sniff in the newsroom) to make sure you have a testable hypothesis.\nThis is always a step I use at my newsroom to plan longer data investigations, and see what kind of story we could write if we spent more time on the data collection and honing the methodology."
  },
  {
    "objectID": "best-practices-data-collection.html#spotcheck-everything",
    "href": "best-practices-data-collection.html#spotcheck-everything",
    "title": "Best practices for data collection",
    "section": "Spotcheck everything",
    "text": "Spotcheck everything\nManually check your programmatically saved results with the live results. Small errors can be systematic errors if you don’t catch them manually. Choose a reasonable sample size (such as N=100), to assure what you’re analyzing is exactly what you think you are.\nThis is something we did to bullet-proof almost every investigation, even if we didn’t publish the results of that hand-check."
  },
  {
    "objectID": "best-practices-data-collection.html#conclusion",
    "href": "best-practices-data-collection.html#conclusion",
    "title": "Best practices for data collection",
    "section": "Conclusion",
    "text": "Conclusion\nThese tips are not definitive. If you want to share tips, please make a suggestion via email or GitHub."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Buolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Proceedings of the 1st Conference on\nFairness, Accountability and Transparency, edited by Sorelle A.\nFriedler and Christo Wilson, 81:77–91. Proceedings of Machine Learning\nResearch. PMLR. https://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nCalacci, Dan, Jeffrey J. Shen, and Alex Pentland. 2022. “The Cop\nin Your Neighbor’s Doorbell: Amazon Ring and the Spread of Participatory\nMass Surveillance.” Proc. ACM Hum.-Comput. Interact. 6\n(CSCW2). https://doi.org/10.1145/3555125.\n\n\nCameron, Dell, and Dhruv Mehrota. 2019. “Ring’s Hidden Data Let Us\nMap Amazon’s Sprawling Home Surveillance Network.”\nGizmodo, December. https://gizmodo.com/ring-s-hidden-data-let-us-map-amazons-sprawling-home-su-1840312279.\n\n\nGarcia, David, Yonas Mitike Kassa, Angel Cuevas, Manuel Cebrian, Esteban\nMoro, Iyad Rahwan, and Ruben Cuevas. 2018. “Analyzing gender inequality through large-scale Facebook\nadvertising data.” Proceedings of the National Academy\nof Science 115 (27): 6958–63. https://doi.org/10.1073/pnas.1717781115.\n\n\nMajor, David, Ross Teixeira, and Jonathan Mayer. 2020. “No WAN’s\nLand: Mapping u.s. Broadband Coverage with Millions of Address Queries\nto ISPs.” In Proceedings of the ACM Internet Measurement\nConference, 393–419. IMC ’20. New York, NY, USA: Association for\nComputing Machinery. https://doi.org/10.1145/3419394.3423652.\n\n\nMetaxa, Danaë, Joon Sung Park, Ronald E. Robertson, Karrie\nKarahalios, Christo Wilson, Jeff Hancock, and Christian Sandvig.\n2021.\n\n\nSap, Maarten, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A.\nSmith. 2019. “The Risk of Racial Bias in Hate Speech\nDetection.” In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 1668–78. Florence,\nItaly: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1163.\n\n\nWillis, Derek. 2013. “Freeing the Plum Book.”\nSource, April. https://source.opennews.org/articles/freeing-plum-book/."
  }
]