{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Finding Undocumented APIs\"\n",
    "author: \"Leon Yin\"\n",
    "date-modified: \"03-10-2023\"\n",
    "bibliography: references.bib\n",
    "execute: \n",
    "  enabled: false\n",
    "href: undocumented-api\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is [@pmlr-v81-buolamwini18a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undocumented APIs provide a rich, reliable, and scalable method of building your own datasets. [^1]\n",
    "\n",
    "Learn about undocumented APIs, how they've been used in past investigations, and how to find them in the wild.\n",
    "\n",
    "üëâ[Click here to jump to the tutorial](#tutorial).\n",
    "\n",
    "# Intro\n",
    "\n",
    "When I pitched my first story at The Markup, I was (and still am) obsessed with YouTube. Not only is YouTube the [largest video-hosting website worldwide](https://www.statista.com/topics/2019/youtube/), but it also paved the way for the creator economy. \n",
    "\n",
    "I wanted to better understand YouTube's advertising system, especially how they treated hateful conspiracy theories, which at the time, thrived on the platform.\n",
    "\n",
    "To do this, I got acqainted with the Google Ads portal. Anyone can sign-up, and see all the tools marketers use to reach users across the Google adverse. YouTube has a special section of the ad portal, where marketers can target their ads based on user demographics and the content of videos.\n",
    "\n",
    "Along with Aaron Sankin, we investigated a specific targetting tool that allows ad-buyers to find and select specific videos and channels to place ads on, based on a search term. \n",
    "\n",
    "We found that the racist \"White Genocide\" conspiracy theory returned no videos, but by removing spaces, we were returned results.\n",
    "\n",
    "<video src=\"https://mrkp-static-production.themarkup.org/graphics/youtube-hate-video/1617820765826/assets/white-genocide--compressed.mp4\" autoplay=true loop=true width=75%></video>\n",
    "\n",
    "These initial tests suggested a keyword block list that hid results for certain keywords. However, we saw that swears and gibberish (\"asdfoiasf\") also would surface no results. Based on what we saw, it was difficult to discern a clear signal between something that was blocked, and something that may have been too obscure to return any results.\n",
    "\n",
    "My collegue Surya Mattu suggested using my web browser's built-in `developer tools` to watch the network requests while we made searches in the portal. Doing so helped us isolate the API-endpoint being called during this process, and reverse-engineer it to return results for any given keyword. We found structural differences in the data returned from the API depending on Google's verdict of the keyword.\n",
    "\n",
    "Blocked terms, returned an empty JSON packet of data `{}`, whereas obscure terms returned a JSON with labels but no results:\n",
    "\n",
    "```{‚Äúvideos‚Äù: [], ‚Äúchannels‚Äù: []}```\n",
    "\n",
    "With this method in tow, we solicited help from civil rights groups and researchers to build keyword lists to test. We found found YouTube's uneven enforcement of their advertising guidelines--blocking social and racial justice terms, while showing advertisers results for well-known hate terms.\n",
    "\n",
    "This was my first time finding and using `undocumented APIs`, and it is an essential tool for data collection. Here we will introduce you to APIs, explain the difference between \"undocumented API\"s and those that are documented, and we'll also share the key strengths that come with this technology through several case studies.\n",
    "\n",
    "Lastly, we will go through a hands-on excercise finding and using an undocumented API in the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an A-P-I?\n",
    "\n",
    "If you ever tried to get your driver's license at the Department of Motor Vehicles (DMV), needed a travel visa, or reimbursement for a business expense, you have experienced bureaucracy at its finest-- a series of lines, forms, credential-showing, and waiting.\n",
    "\n",
    "You bring the proper paperwork, wait in line, order, pay, and then, hopefully, you get what you came for. That is, only if you did everything exactly right.\n",
    "\n",
    "Application program interfaces, or APIs, are digitzed bureaucracy. You make a request, and then wait in a queue to be served. However, instead of leaving with a driver's license or a custom plate, what you‚Äôre waiting for is well-formatted data. As for making mistakes... well, you'll get an automated response and zero sympathy.\n",
    "\n",
    "A great deal of the actions you might perform on digital applications or websites depend on APIs to authorize and record your requests, put you in a queue, and send display freshly queried information on a screen.\n",
    "\n",
    "Some APIs are well-documented.\n",
    "\n",
    "### Documented APIs\n",
    "\n",
    "Many businesses sell their services using APIs.\n",
    "\n",
    "The benefit of documented APIs is self-explanitory, you know what you're going to get, and there‚Äôs notes and examples to help other developers use the tool as intended.\n",
    "\n",
    "Some documented APIs are also free to use, making them a great tool for teaching and research. Unfortunately, these free APIs can often disappear, or their access severely limited -- as we've seen with Twitter, YouTube, and Facebook.\n",
    "\n",
    "Other APIs are undocumented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undocumented APIs\n",
    "\n",
    "These are the silent heros making sure websites run, often times executing essential functions behind the scenes.\n",
    "Many of these essential functions are so mundane, you probably don't even realize that something is happening.\n",
    "\n",
    "If you kill time on social media platforms, you'll have noticed that the good times keep rolling. That is because \"infinate scroll\" is powered by an API that is called upon as you approach the bottom of the page to load more things to eat up your day.\n",
    "\n",
    "Similarly, when you upload an image onto Instagram, an API will recieve that image, run a series of machine-learning models to categorize it, assure it is \"OK\" (an oversimplification, I know), and store that image and its metadata into a database.\n",
    "\n",
    "Learning how to find and use these hidden APIs opens up endless possibilies for reporting on and researching technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How have documented APIs been used?\n",
    "\n",
    "- [Gender Shades](http://gendershades.org/) was an audit of three commercially-available facial recognition APIs (from Microsoft, IBM, and Face++) used to automate gender classification. The authors created a benchmark image dataset of faces, and tested each facial recognition models by sending the same images through each model's API. The authors found that many models had high error rates for female and Black faces, with the worst performance on Black female faces.\n",
    "\n",
    "- Google's Perspective API was developed to filter out toxic comments for publishers such as [The New York Times](https://www.nytimes.com/2017/06/13/insider/have-a-comment-leave-a-comment.htm). Importantly, Perspective used \"[training data](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)\" sourced from human-labelled [Wikipedia](https://en.wikipedia.org/wiki/Artificial_intelligence_in_Wikimedia_projects) edits. [An academic study](https://maartensap.com/pdfs/sap2019risk.pdf) found racially biased classifications of Tweets. For example, the use of certain identifiers for minority groups would flag a comment as toxic. Because the Google had released the API publicly, researchers could access and audit this API directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How have undocumented APIs been used?\n",
    "\n",
    "Undocumented APIs are instrumental to investigations and audits ranging from uncovering keyword blocklists, to identifying unmarked private label products, to being able to collect massive of quantities data representative of residents across major American cities.\n",
    "\n",
    "Often time open-source web scraping projects rely on these APIs, see [Pyktok](https://github.com/dfreelon/pyktok) (Python Tiktok collector).\n",
    "\n",
    "Using undocumented APIs has three key strengths:\n",
    "\n",
    "1. **Richness**: APIs often contain information that is not visable on web pages. This information is also stored in JSON, which makes web parsing a breeze.<br>\n",
    "2. **Reliability**: Often times these APIs execute essential functions. For that reason, they don't change often. This makes them a reliable data source over time. Some companies routinely change class names and tags to break web scrapers that save HTML. This is less of a problem when you use an API.<br>\n",
    "3. **Scalability**: You can collect more information in less time using this method compared to [headless browsers](https://en.wikipedia.org/wiki/Headless_browser), such as Selenium, Puppeteer, and Playwright. (Not throwing shade--these tools have their purpose, which we'll go over in a future section.)\n",
    "\n",
    "\n",
    "We will cover three case studies, each of which is intended to highlight one of these benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study on richness: Google's blocklist for YouTube advertisers\n",
    "\n",
    "I'm not going to re-hash this case study, since we led with it in the introduction, but...\n",
    "\n",
    "Using undocumented APIs can reveal **rich** metadata. This includes hidden fields that are not displayed to everyday users of a website, as well as subtle changes to the structural in how data is returned. \n",
    "\n",
    "These produces receipts you can follow by deciphering the meaning of these hidden fields, finding traces left by missing data, and identifying patterns that are otherwise hidden from surface (front-end) world.\n",
    "\n",
    "Cetainly this was the case with the YouTube investigation, and something that we'll brush on again in the hands-on tutorial at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study on reliability: Amazon branded products\n",
    "\n",
    "If you scrape HTML from a website over a long period of time, you'll likely find yourself with a broken scraper. \n",
    "\n",
    "This occurs when class names, accessibility labels, text, or something else has changed and confused your scraper. In this sense, HTML scraping can be fragile and fickle, especially if your collecting data persistently.\n",
    "\n",
    "If you look at large platforms such as Facebook's homepage, you'll see elements are arbitrarily named, oddly-nested, and ever-changing.\n",
    "\n",
    "Using undocumented APIs can often get you the same information with a higher success-rate. This is because these APIs interact with the same backend (fetching information before being rendered, named, and nestled neatly into a webpage), and are often essential to the operation of the website.\n",
    "\n",
    "In the investigation \"[Amazon's Advantage](https://themarkup.org/amazons-advantage/2021/10/14/how-we-analyzed-amazons-treatment-of-its-brands-in-search-results)\", Adrianne Jeffries and I found a **reliable** method of identifying Amazon brands and exclusive products. At the time, these products were not clearly labelled, most Americans we surveyed were unable to identify Amazon's top brands, and no source of truth existed. \n",
    "\n",
    "We developed a approach to identify these products as Amazon private label using a filter found in the user interface of the Amazon site. The \"Our brands\" filter did a lot of heavy lifting in our investigation, and we found that it was powered by an undocumented API that listed all the Amazon branded products for a given search.\n",
    "\n",
    "This method was essential to our investigation and analysis, especially while we collected data over a period of several months. To our surprise, the API continued to work even after we went to Amazon to comment on our detailed methodology, after we published our investigation, and even after Amazon executives were accused of perjury by the U.S. Senate.\n",
    "\n",
    "Usually the party gets shut down once you call the parents, but in this case it didn't.\n",
    "\n",
    "Because the API continued to work, we used it in a browser extension (_Amazon Brand Detector_) that we (inclusing Ritu Ghiya and Jeff Crouse) built to highlight Amazon brands for Amazon shoppers around the globe. Eventually, Amazon added an orange disclaimer of \"Amazon brand\" (mirroring the orange stain from our extension) to their branded products, but the API and extension still work at the time of writing, more than a year later.\n",
    "\n",
    "This case study emphasizes the reliability of using undocumented APIs, not only for collecting datasets, but for persistant accountability efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study on scalability: Collecting Internet Plans\n",
    "\n",
    "In the investigation, \"[Still Loading](https://themarkup.org/show-your-work/2022/10/19/how-we-uncovered-disparities-in-internet-deals)\" my reporting partner Aaron Sankin and I collected and analyzed internet service plans across major cities in the United States. \n",
    "\n",
    "We learned a technique from a trio of researchers from Princeton, that used the lookup tools found on the internet service providers' websites to retrieve internet plans for a specific address.\n",
    "\n",
    "However, doing this using a browser (as a real person would) is incredibly slow. Even with 10 automated browsers (see below) with unique IP addresses, it would have taken months to collect a representative sample of a single major American city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=100% controls loop>\n",
    "  <source src=\"assets/att-scraper-selenium.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "<p><small>Scraping AT&T using way too many Selenium browsers</small></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browser automation is bulky. Not only do you need to load every asset of a web page, there is also the compute resources necessary to spin up a browser. When you can get away without having to mock user interactions, or use rendered page elements, finding the underlying API(s) can be quicker and more eloquent.\n",
    "\n",
    "Initially, the workflow for getting an internet plan seemed too complex to pull off using an API-- there was user authentication that set a cookie, choosing an address from a list of suggestions, and adding an apartment number when prompted.\n",
    "\n",
    "However, we were able to keep track of cookies using a `session` (read about this advanced topic [here](https://requests.readthedocs.io/en/latest/user/advanced/#session-objects)), and speed things up by bundling the sequence of successive API calls into a function.\n",
    "\n",
    "Not only was this function easier to write, but it was able to be written and executed [asyncronously](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming)). Meaning we could request internet plans from many addresses at the same time.\n",
    "\n",
    "This allowed us to collect AT&T internet plans for a representative sample of 21 cities in two days, rather than two years. Timely data collection is key. We found our story looking at data for one city. If we were unable to collect that data we would never had published our investigation that revealed widespread inequities in internet plans across the country.\n",
    "\n",
    "When it comes to web scraping, undocumented APIs offer unmatched **scalability** to collect massive amounts of data. This is especially true when you combine the them with asynchronous and multi-threaded programming (another topic we plan to cover in a future section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *How to find APIs?*\n",
    "\n",
    "Most APIs are *undocumented*, hidden in plain sight.\n",
    "\n",
    "You can use undocumented APIs to collect [datasets for investigations, audits, and tools](#how-have-documented-apis-been-used).\n",
    "\n",
    "You can sniff them out using a web browser‚Äôs developer tools (shortened to dev tools).\n",
    "\n",
    "::: {.callout-note}\n",
    "Note that if you're in a workshop setting: hitting the example API at the same time will get us all blocked from the website!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First open the developer console. \n",
    "\n",
    "See how on [Chrome](https://developer.chrome.com/docs/devtools/open/) or [Firefox](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_are_browser_developer_tools) here.  \n",
    "\n",
    "In this tutorial, we'll see how Amazon.com autocomplete search suggestions work.\n",
    "\n",
    "My go-to method, (regardless of browser,) is to right-click and ‚ÄúInspect‚Äù an element on the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"assets/inspect-panel.png\" style=\"width:50%\">\n",
    "<figcaption align = \"center\"> Example of inspecting an element on a page using a right-click </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This will open the dev tools under the ‚ÄúElements‚Äù tab, which is used to explore the source code of a page. \n",
    "\n",
    "Page source code is useful because it reveals useful clues that are otherwise unseen by regular users. Often times, clues are in accessibility features known as ARIA elements.\n",
    "\n",
    "However, this tutorial is not about source code... it's about API requests that populate what we see on the page, and the hidden fields that we don't see.\n",
    "\n",
    "Let's try this!\n",
    "\n",
    "With dev tools open, go to Amazon.com, select the search bar on the website, and start typing a query (such as \"spicy\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Click the ‚ÄúNetwork‚Äù tab.\n",
    "\n",
    "This section of the dev tools is used to monitor network requests.\n",
    "\n",
    "*Background*\n",
    "\n",
    "Everything on a page is retrieved from some outside source, likely a server. This includes things like images embedded on the page, JavaScript code running in the background, and all the bits of ‚Äúcontent‚Äù that populate the page before us.\n",
    "\n",
    "Using the network tab, we can find out how this information is requested from a server, and intercept the response before it is rendered on the page.\n",
    "\n",
    "These responses are information-rich, and contain fields that don‚Äôt end up in the source code *or* in the user interface that most people encounter when they visit a site.\n",
    "\n",
    "Further, we can reverse-engineer how this request is made, and use it to collect structured data at scale. This is the power of finding *undocumented APIs.*\n",
    "\n",
    "*Back to the console...*\n",
    "\n",
    "The network tab can look pretty hectic at first. It has many uses, and a lot of information. We'll cover some of the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=100% controls loop>\n",
    "  <source src=\"assets/dev-console.mov\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filter requests by fetch/XHR\n",
    "\n",
    "This will reveal only API calls made to servers. This includes internal servers that are hosted by the folks who run the website we‚Äôre inspecting, as well as external servers. The latter often includes third-party trackers used in adtech, and verification services to authenticate user behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=100% controls loop>\n",
    "  <source src=\"assets/filter-network.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see quite a few network requests that were loaded onto the page. Look at \"Domain\" and \"File\" to narrow down where requests were sent, and whether the names are telling of the purpose of the request. \n",
    "\n",
    "::: {.callout-tip}\n",
    "#### Pro tip:\n",
    "You can \"Filter URLs\" using different properties (see how to do this for [Chrome](https://developer.chrome.com/docs/devtools/network/reference/#filter-by-property) and [Firefox](https://firefox-source-docs.mozilla.org/devtools-user/network_monitor/request_list/index.html#filtering-requests)).\n",
    ":::\n",
    "\n",
    "In this example, notice that a request was sent to the \"Domain\" `completion.amazon.com`, using an API endpoint (in the \"File\" column) named `suggestions`. This is  likely the API being called to populate autocompleted search suggestions on the Amazon marketplace.\n",
    "\n",
    "When clicking the network request, you'll see \"Headers\". Those are the [HTTP headers](https://developer.mozilla.org/en-US/docs/Glossary/Request_header) that were sent along with the network request. This is not useful for us _just yet_, instead we want to see what data gets transferred as a result of the API call.\n",
    "\n",
    "To do this, we'll look at the request's \"Response\" attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze the response\n",
    "This might seem intimidating at first, but let me _key_ you in on some tips. Responses are almost always JSON-formatted. JSON is made up of lists and [key-value](https://en.wikipedia.org/wiki/Name%E2%80%93value_pair) pairs. This means the information is stored like a dictionary, with words and their corresponding definitions.\n",
    "\n",
    "Looking at the JSON response, it looks like Amazon's `completion.amazon.com/suggestions` API returns a list of \"suggestions\". Each item in the list of suggestions has a \"value\", in the example above that \"value\" is `spicy ramen`. \n",
    "\n",
    "**Check your work**: confirm this interpretation is correct by cross-referencing the API response with what a user would see on the website.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"assets/spicy.png\" width=50%>\n",
    "<figcaption align = \"center\"> Amazon's suggestions for \"spicy\".</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting these steps down, is your one way ticket to spicy town, and you don't need to code at all.\n",
    "\n",
    "However, some rudimentary coding can help you figure out how to use the API for vast inputs to collect your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Copy as cURL\n",
    "\n",
    "If you find an HTTP request that returns a response with useful information you can start to reverse-engineer it. To do that, we can isolate it by right-clicking the HTTP request and selecting ‚Äúcopy as cURL‚Äù. ([cURL](https://developer.ibm.com/articles/what-is-curl-command/) stands for client URL, and is a tool used to transfer data across networks.)\n",
    "\n",
    "<img src=\"assets/copy-curl.png\" width=90%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Curl to requests\n",
    "We can use a site like [curlconverter.com](https://curlconverter.com/) to convert the cURL we copied into a reusable API call. In this example, we use the default conversion to a Python `requests` script. You can do the same for any language and framework, but we use this to demonstrate because it's exactly what I do in practice.\n",
    "\n",
    "Here is what the converted cURL looks like after being converted to a Python request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "cookies = {\n",
    "    'aws-ubid-main': '836-8365128-6734270',\n",
    "    'session-id-time': '2082787201l',\n",
    "    'ubid-main': '135-7086948-2591317',\n",
    "    'aws-priv': 'eyJ2IjoxLCJldSI6MCwic3QiOjB9',\n",
    "    'aws-target-static-id': '1593060129944-225088',\n",
    "    'lc-main': 'en_US',\n",
    "    'aws-userInfo': '%7B%22arn%22%3A%22arn%3Aaws%3Asts%3A%3A335340120612%3Aassumed-role%2FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%2Fleon.yin%40themarkup.org%22%2C%22alias%22%3A%22themarkup-journalist-sandbox%22%2C%22username%22%3A%22assumed-role%252FAWSReservedSSO_PowerUserAccess_4dc523239a9019dc%252Fleon.yin%2540themarkup.org%22%2C%22keybase%22%3A%22Fwu%2FzLIz0yD%2FyfrORX%2BSIpIQPXrzr0kt24uqa8mNs7g%5Cu003d%22%2C%22issuer%22%3A%22https%3A%2F%2Fd-90677ebe1e.awsapps.com%2Fstart%2F%23%2Fsaml%2Fcustom%2F335340120612%2520%2528AWS%2520Journalist%2520Sandbox%2529%2FMTI5NTE5MjY4NzI0X2lucy0wNzcxNmExOGZlNThiYjIyX3AtNDUyYTgyOTRhNTJlYWZjNQ%5Cu003d%5Cu003d%22%2C%22signinType%22%3A%22PUBLIC%22%7D',\n",
    "    'x-main': 'Oz3Tb5n2p0ic7OhF3cU5dc9B4ZR2gFjhKEsP4zikHHD3Gk2O7NpSmuShBxLFrhpZ',\n",
    "    'at-main': 'Atza|IwEBILB5ARQ_IgTCiBLam_XE2pyT76jXTbAXHOm2AJomLPmDgoJUJIIlUmyFeh_gChLHCycKjNlys-5CqqMabKieAzqSf607ChJsNevw-V06e7VKgcWjvoMaZRWlGiZ-c5wSJ-e4QzIWzAxTS1EI6sRUaRZRv-a0ZpOJQ-sHHB99006ytcrHhubdrXYPJRqEP5Q-_30JtESMpAkASoOs4vETSFp5BDBJfSWWETeotpIVXwA4NoC8E59bZb_5wHTW9cRBSWYGi1XL7CRl2xGbJaO2Gv3unuhGMB1tiq9iwxodSPBBTw',\n",
    "    'sess-at-main': '\"PUq9PW1TbO9CTYhGMo7l1Dz+wedh40Ki8Z9rPC+1TSI=\"',\n",
    "    'sst-main': 'Sst1|PQHsbeSFCMSY0X0_WgvTo5NUCaZkG2J9RPqWWy0fCpyWopJXgu6_drU_LstOdJB2cDmaVCXwkNpsF5yNPrBDj3Wtx-TC-AaYZn6WUdp8vNRPb6iYqxPAjRDnfK3pCnHqt19I0GoG7Bd1wnOxkAvnH0992IUq14kH6Ojm0J8noVPwMez0lltD-jxBwtDQ_EZYUkZG741RDVEojfziawJY9iKc-cLCnKmhi-ca1PPJnsimPV4lXRtMAGFbf9nMkKq4CbpkaRMdVtlPr20vF9eqg_V_-LY_V7S44WlO-_t_bFBnK8Q',\n",
    "    'i18n-prefs': 'USD',\n",
    "    'session-token': 'ptze73uznXExrMCSV9AklvNOKa1ND9F0rlQH2ioSM26Vr6hSheH8O4v4P8Lg3zuv7oDM+HZ+8f2TlyoPXUmPShprMXdvEpAQieXUw7+83PZOJvkkg1jwP0NiG0ZqksIYOr3Zuwt3omMcfCKRReWKxl5rGaDEM6AISpwI5aMDDCnA7fWbVO/QQYNxUZMifc599EZ5Fg3uGjCAhBlb6I7UO8ewRbXJ1bo9',\n",
    "    'session-id': '139-9925917-2023535',\n",
    "    'aws-userInfo-signed': 'eyJ0eXAiOiJKV1MiLCJrZXlSZWdpb24iOiJ1cy1lYXN0LTEiLCJhbGciOiJFUzM4NCIsImtpZCI6ImFhNDFkZjRjLTMxMzgtNGVkOC04YmU5LWYyMzUzYzNkOTEzYiJ9.eyJzdWIiOiJ0aGVtYXJrdXAtam91cm5hbGlzdC1zYW5kYm94Iiwic2lnbmluVHlwZSI6IlBVQkxJQyIsImlzcyI6Imh0dHBzOlwvXC9kLTkwNjc3ZWJlMWUuYXdzYXBwcy5jb21cL3N0YXJ0XC8jXC9zYW1sXC9jdXN0b21cLzMzNTM0MDEyMDYxMiUyMCUyOEFXUyUyMEpvdXJuYWxpc3QlMjBTYW5kYm94JTI5XC9NVEk1TlRFNU1qWTROekkwWDJsdWN5MHdOemN4Tm1FeE9HWmxOVGhpWWpJeVgzQXRORFV5WVRneU9UUmhOVEpsWVdaak5RPT0iLCJrZXliYXNlIjoiRnd1XC96TEl6MHlEXC95ZnJPUlgrU0lwSVFQWHJ6cjBrdDI0dXFhOG1OczdnPSIsImFybiI6ImFybjphd3M6c3RzOjozMzUzNDAxMjA2MTI6YXNzdW1lZC1yb2xlXC9BV1NSZXNlcnZlZFNTT19Qb3dlclVzZXJBY2Nlc3NfNGRjNTIzMjM5YTkwMTlkY1wvbGVvbi55aW5AdGhlbWFya3VwLm9yZyIsInVzZXJuYW1lIjoiYXNzdW1lZC1yb2xlJTJGQVdTUmVzZXJ2ZWRTU09fUG93ZXJVc2VyQWNjZXNzXzRkYzUyMzIzOWE5MDE5ZGMlMkZsZW9uLnlpbiU0MHRoZW1hcmt1cC5vcmcifQ.LWFZOJMDcYdu6od6Nk8TmhAFMGA9O98O4tIOsVlR7w5vAS_JgVixL8j75u6jTgjfWkdddhKqa5kgsXDmGNbjhzLIsD48ch1BUodlzxqeQfn0r8onIwLbUIHEnk6X-AJE',\n",
    "    'skin': 'noskin',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n",
    "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    # 'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Origin': 'https://www.amazon.com',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.amazon.com/',\n",
    "   \n",
    "    'Sec-Fetch-Dest': 'empty',\n",
    "    'Sec-Fetch-Mode': 'cors',\n",
    "    'Sec-Fetch-Site': 'same-site',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'limit': '11',\n",
    "    'prefix': 'spicy',\n",
    "    'suggestion-type': [\n",
    "        'WIDGET',\n",
    "        'KEYWORD',\n",
    "    ],\n",
    "    'page-type': 'Gateway',\n",
    "    'alias': 'aps',\n",
    "    'site-variant': 'desktop',\n",
    "    'version': '3',\n",
    "    'event': 'onKeyPress',\n",
    "    'wc': '',\n",
    "    'lop': 'en_US',\n",
    "    'last-prefix': '\\0',\n",
    "    'avg-ks-time': '2486',\n",
    "    'fb': '1',\n",
    "    'session-id': '139-9925917-2023535',\n",
    "    'request-id': 'SVMTJXRDBQ9T8M7BRGNJ',\n",
    "    'mid': 'ATVPDKIKX0DER',\n",
    "    'plain-mid': '1',\n",
    "    'client-info': 'amazon-search-ui',\n",
    "}\n",
    "\n",
    "response = requests.get('https://completion.amazon.com/api/2017/suggestions', \n",
    "                        params=params, cookies=cookies, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this Python code, as-is, and it should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Strip it down\n",
    "\n",
    "You might be overwhelmed with the parameters that go into this API request. Like the response output, the inputs are formatted like a JSON, too. Start removing these parameters one-by-one. \n",
    "\n",
    "I tend to keep a few essential ones for authentication, and also the parameters you care about changing for your own purposes. Notice that our example query of \"spicy\" stored in the `prefix` parameter.\n",
    "\n",
    "::: {.callout-tip}\n",
    "#### Pro tip:\n",
    "Parameter values can expire, so periodically test the request and each parameter to assure you only keep the shelf-stable parts.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n",
    "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'prefix': 'spicy',\n",
    "    'suggestion-type': [\n",
    "        'WIDGET',\n",
    "        'KEYWORD',\n",
    "    ],\n",
    "    'alias': 'aps',\n",
    "    'plain-mid': '1',\n",
    "}\n",
    "\n",
    "response = requests.get('https://completion.amazon.com/api/2017/suggestions', params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Recycle and reuse\n",
    "\n",
    "With the stripped down request, try to submit a few‚Äî let‚Äôs say 10 or 20, requests with new parameters set by you.\n",
    "\n",
    "Below, I converted the stripped down API call into a Python function that takes any `keyword` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def search_suggestions(keyword):\n",
    "    \"\"\"\n",
    "    Get autocompleted search suggestions for a `keyword` search on Amazon.com.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:100.0) Gecko/20100101 Firefox/100.0',\n",
    "        'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'prefix': keyword,\n",
    "        'suggestion-type': [\n",
    "            'WIDGET',\n",
    "            'KEYWORD',\n",
    "        ],\n",
    "        'alias': 'aps',\n",
    "        'plain-mid': '1',\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://completion.amazon.com/api/2017/suggestions', \n",
    "                            params=params, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can set new input parameters in `keyword`, and make the an API call using each keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are our inputs (what searches we'll get autocompleted)\n",
    "keywords = [\n",
    "    'a', 'b', 'cookie', 'sock', 'zelda', '12'\n",
    "]\n",
    "\n",
    "# Here we'll go through each input, get the suggestions, and then add the `suggestions` to a list.\n",
    "data = []\n",
    "for keyword in keywords:\n",
    "    suggestions = search_suggestions(keyword)\n",
    "    suggestions['search_word'] = keyword # keep track of the seed keyword\n",
    "    time.sleep(1) # best practice to put some time between API calls.\n",
    "    data.extend(suggestions['suggestions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved the API responses in a list called `data`, and put them into a [Pandas](https://pandas.pydata.org/) DataFrame to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggType</th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>refTag</th>\n",
       "      <th>candidateSources</th>\n",
       "      <th>strategyId</th>\n",
       "      <th>prior</th>\n",
       "      <th>ghost</th>\n",
       "      <th>help</th>\n",
       "      <th>queryUnderstandingFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>KeywordSuggestion</td>\n",
       "      <td>KEYWORD</td>\n",
       "      <td>asmanex twisthaler 30 inhaler</td>\n",
       "      <td>nb_sb_ss_i_5_1</td>\n",
       "      <td>local</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'source': 'QU_TOOL', 'annotations': []}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>KeywordSuggestion</td>\n",
       "      <td>KEYWORD</td>\n",
       "      <td>bathroom organizer</td>\n",
       "      <td>nb_sb_ss_i_4_1</td>\n",
       "      <td>local</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'source': 'QU_TOOL', 'annotations': []}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>KeywordSuggestion</td>\n",
       "      <td>KEYWORD</td>\n",
       "      <td>baby wipes</td>\n",
       "      <td>nb_sb_ss_i_10_1</td>\n",
       "      <td>local</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'source': 'QU_TOOL', 'annotations': []}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>KeywordSuggestion</td>\n",
       "      <td>KEYWORD</td>\n",
       "      <td>baby registry search</td>\n",
       "      <td>nb_sb_ss_i_3_1</td>\n",
       "      <td>local</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'source': 'QU_TOOL', 'annotations': []}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>KeywordSuggestion</td>\n",
       "      <td>KEYWORD</td>\n",
       "      <td>b013xkha4m b08xzrxczm b07xxphqzk b09rwjblc7</td>\n",
       "      <td>nb_sb_ss_i_7_1</td>\n",
       "      <td>local</td>\n",
       "      <td>organic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'source': 'QU_TOOL', 'annotations': []}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             suggType     type                                        value  \\\n",
       "4   KeywordSuggestion  KEYWORD                asmanex twisthaler 30 inhaler   \n",
       "13  KeywordSuggestion  KEYWORD                           bathroom organizer   \n",
       "19  KeywordSuggestion  KEYWORD                                   baby wipes   \n",
       "12  KeywordSuggestion  KEYWORD                         baby registry search   \n",
       "16  KeywordSuggestion  KEYWORD  b013xkha4m b08xzrxczm b07xxphqzk b09rwjblc7   \n",
       "\n",
       "             refTag candidateSources strategyId  prior  ghost   help  \\\n",
       "4    nb_sb_ss_i_5_1            local    organic    0.0  False  False   \n",
       "13   nb_sb_ss_i_4_1            local    organic    0.0  False  False   \n",
       "19  nb_sb_ss_i_10_1            local    organic    0.0  False  False   \n",
       "12   nb_sb_ss_i_3_1            local    organic    0.0  False  False   \n",
       "16   nb_sb_ss_i_7_1            local    organic    0.0  False  False   \n",
       "\n",
       "                    queryUnderstandingFeatures  \n",
       "4   [{'source': 'QU_TOOL', 'annotations': []}]  \n",
       "13  [{'source': 'QU_TOOL', 'annotations': []}]  \n",
       "19  [{'source': 'QU_TOOL', 'annotations': []}]  \n",
       "12  [{'source': 'QU_TOOL', 'annotations': []}]  \n",
       "16  [{'source': 'QU_TOOL', 'annotations': []}]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# show 5 random auto suggestions\n",
    "df.sample(5, random_state=303)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the columns, you might be flooded with more questions:\n",
    "\n",
    "- Some terms may be `blackListed`, what does that mean and what words, if any, are `blackListed = True`?<br>\n",
    "- Are some searches paid for, and not `organic`?<br>\n",
    "- What is `ghost`?<br>\n",
    "\n",
    "This metadata is only visable from the API, and can lead to new story ideas and directions to pursue. \n",
    "\n",
    "Unfortunately, because this API is undocumented, asking these questions and figuring out what everything represents is difficult. Use your curiousity and look at many examples. The feature of the API is being able to make many queries at scale, which should help answer these questions. Reporting this out with sources is also essential in this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it Yourself\n",
    "Use this space below to find another undocumented API and reuse it for a few other input parameters.\n",
    "\n",
    "Revisit the steps we outlined above, and apply them to a new website.\n",
    "If you aren't a coder, try to get steps 1-6 (I believe in you!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are a coder, try some of the advanced usage below.\n",
    "\n",
    "### For advanced usage...\n",
    "- Handle errors for bad requests, rate limiting, and other issues that could arise.<br>\n",
    "- you can use `session` instead of pure requests. This is helpful if cookies and authentication are involved. Read more about that [here](https://requests.readthedocs.io/en/latest/user/advanced/#session-objects).<br>\n",
    "- you can make a request asyncronous to speed up data collection (without overloading the site's servers, of course).<br>\n",
    "- Implement steps 6-onwards in another programming language.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Assignment\n",
    "\n",
    "Dry-run: Think about the websites you frequent and topics that interest you. Find an API in the wild, isolate it, and analyze some of its results.\n",
    "\n",
    "Scoping: Determine how the API could be used to produce data to answer a reporting question or hypothesis.\n",
    "\n",
    "Reporting: Figure out the \"sample\". How many inputs will you test and for what purpose? Determine the meaning and signifigance of hidden fields that are returned.\n",
    "\n",
    "Ultimately undocumented APIs are a tool, and data is useless without a purpose. Hopefully this worksheet helps you in your time of need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related readings\n",
    "More tutorials on the same subject:\n",
    "\n",
    "- [\"Scraping XHR\"](https://scrapism.lav.io/scraping-xhr/) - Sam Lavigne<br>\n",
    "- [\"Web Scraping 201: finding the API\"](http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/) - Greg Reda\n",
    "\n",
    "Topical and timeless:\n",
    "\n",
    "- [\"Computational research in the post-API age\"](http://dfreelon.org/publications/2018_Computational_research_in_the_postAPI_age.pdf) - Deen Freelon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "::: {#refs}\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts\n",
    "Slides from workshops can be found here:\n",
    "\n",
    "[2023-03-16 @ Tow Center Columbia]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
