{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1481a66b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Parsing HTML with XPath\"\n",
    "pagetitle: \"XPath\"\n",
    "description-meta: \"Why aren't you using XPath? Get familar now!\"\n",
    "description-title: \"Why aren't you using XPath? Get familar now!\"\n",
    "author: \"Leon Yin\"\n",
    "author-meta: \"Leon Yin\"\n",
    "date: \"07-26-2024\"\n",
    "date-modified: \"07-07-2025\"\n",
    "execute: \n",
    "  enabled: false\n",
    "twitter-card:\n",
    "  title: XPath to Success\n",
    "  description: Why aren't you using XPath? Get familar now!\n",
    "  image: assets/inspect-element-logo.jpg\n",
    "open-graph:\n",
    "  title: XPath to Success\n",
    "  description: Why aren't you using XPath? Get familar now!\n",
    "  locale: us_EN\n",
    "  site-name: Inspect Element\n",
    "  image: assets/inspect-element-logo.jpg\n",
    "href: xpath\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e7a010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "<a type=\"button\" class=\"btn btn-outline-primary btn-sm\" target=\"_blank\" href=\"https://inspectelement.org/xpath\">üìñ Read online</a>\n",
       "<a type=\"button\" class=\"btn btn-outline-primary btn-sm\" target=\"_blank\" href=\"https://colab.research.google.com/github/yinleon/inspect-element/blob/main/xpath.ipynb\">üñ•Ô∏è  Interactive version</a>\n",
       "<a type=\"button\" class=\"btn btn-outline-primary btn-sm\" target=\"_blank\" href=\"https://github.com/yinleon/inspect-element/blob/main/xpath.ipynb\">‚öôÔ∏è GitHub</a>\n",
       "<a type=\"button\" class=\"btn btn-outline-primary btn-sm\" target=\"_self\" href=\"#citation\">üèõ Citation</a>\n",
       "<br>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "from utils import build_buttons\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "utils.build_buttons(link= 'xpath', \n",
    "                    github= 'https://github.com/yinleon/inspect-element/blob/main/xpath.ipynb',\n",
    "                    citation= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f6b0e",
   "metadata": {},
   "source": [
    "[XPath](https://en.wikipedia.org/wiki/XPath) is an expression language used to parse and navigate [XML](https://en.wikipedia.org/wiki/XML)-formatted documents such as HTML.\n",
    "\n",
    "XPath is useful for web scraping and can be used in place of the popular [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) Python package. \n",
    "\n",
    "A key advantage of XPath is its universality. The same selectors will work across browsers and web scraping tools, which smoothes the lines between inspecting live websites, writing code to pull data from HTML pages, and identifying specific web elements to interact with using [browser automation](/browser_automation.html) tools.\n",
    "\n",
    "This section will teach you how to write simple, precise, and generalizable XPath expressions to parse and manipulate web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574c31a",
   "metadata": {},
   "source": [
    "# Introduction to XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3773431",
   "metadata": {},
   "source": [
    "We're going to identify all the recent article titles and links from NPR.\n",
    "In your browser and go to our example website: https://text.npr.org/\n",
    "\n",
    "Next, open the dev tools by right-clicking anywhere and selecting \"Inspect\" (or however else).\n",
    "\n",
    "Select any element in the \"Elements\" tab and copy the XPath:\n",
    "\n",
    "<figure>\n",
    "    <img src='assets/xpath_devtools.png' width=75%>\n",
    "    <figcaption align = \"left\" style=\"font-size:80%;\"> \n",
    "    How to copy the xpath of an element in Dev Tools.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "The element we're selecting is an `<a>` tag with a link and a title that looks like this:\n",
    "\n",
    "```\n",
    "<a class=\"topic-title\" href=\"/nx-s1-5035272\">What is in Project 2025? </a>\n",
    "```\n",
    "\n",
    "The resulting XPath that we copied looks like this:\n",
    "\n",
    "```\n",
    "/html/body/main/div/ul/li[1]/a\n",
    "```\n",
    "\n",
    "### What is XPath?\n",
    "\n",
    "XPath records hierarchically across a cascade of HTML tags, with the last tag denoting the destination.\n",
    "\n",
    "It designates where an element lives in an HTML document (as if you were honing in on a street address from the center of the earth).\n",
    "\n",
    "The example above is long and specific to one element on the page (BAD!). It is called an _absolute XPath_. \n",
    "\n",
    "Absolute XPath provides directions to a _specifc_ destination (for example the Shake Shack in Madison Square). However, there's another form of XPath called a _relative XPath_ that can provide directions to _every_ Shake Shack.\n",
    "\n",
    "With a little practice XPath can be both precise and generalizable, providing an elegant way to locate and select elements from web pages.\n",
    "\n",
    "Here is the other extreme: short and generic (ALSO BAD!!). \n",
    "\n",
    "```\n",
    "//a\n",
    "```\n",
    "\n",
    "This _relative XPath_ syntax yields the target element mixed with every other element on the page with an `<a>` tag. Following the Shake Shack analogy, this XPath represents directions to every restaurant on Earth.\n",
    "\n",
    "You'll notice the \"//\" before the `<a>` tag, which denotes a search _anywhere_ on the page. \n",
    "\n",
    "### Identifying the optimal XPath in the browser\n",
    "\n",
    "A key strength of XPath is that you can identify and refine them _in browser_, and use the same XPath in different frameworks to make web parsing a breeze. \n",
    "\n",
    "The Goldie Locks approach is not about specifying the exact route (absolute XPath), but rather identifying the destinguishing attributes of the destination (a precise relative XPath).\n",
    "\n",
    "Let's jump back to the live [NPR website](https://text.npr.org/):\n",
    "\n",
    "1. In Dev Tools, switch over to the \"console\" tab. This allows us to execute JavaScript on the page.\n",
    "\n",
    "2. We'll use the `$x()` [function](https://developer.chrome.com/docs/devtools/console/utilities#xpath-function) to select elements on the page by xpath (\"x\" for **x**path). This is unique to devtools and not a standard JavaScript function.\n",
    "\n",
    "As a start, type a HTML tag such as a \"header\", \"a\", \"div\":\n",
    "```\n",
    "$x('//a')\n",
    "```\n",
    "\n",
    "XPath offers an easy way specify attributes and other distinguishable features. \n",
    "\n",
    "You just add an \"@\" sign before the attribute name. This allows you to denote specific attribute values `//a[@href=\"/nx-s1-5035272\"]` or simply the presence of an attribute `//a[@href]`. \n",
    "\n",
    "Here's a workflow for honing in to the optimal XPath: \n",
    "\n",
    "- Start a XPath with a HTML tag with closed brackets: `//a[ ... ]`\n",
    "- copy + paste attributes from a live element (for example `<a class=\"topic-title\" href=\"/nx-s1-5035272\">`)\n",
    "- Add \"@\" before each attribute, ending up with: `//a[@class=\"topic-title\" and @href=\"/nx-s1-5035272\"]`.\n",
    "\n",
    "From there, you can remove overly-specific attributes. In the case above, the `class` is unique enough to isolate news articles.\n",
    "\n",
    "```\n",
    "$x('//a[@class=\"topic-title\"]')\n",
    "```\n",
    "\n",
    "When available, identify elements using accessibility features (`aria`) or `data` attributes. These are less subject to change and are easy to read.\n",
    "\n",
    "## Text Matching\n",
    "XPath also allows for text-matching. \n",
    "\n",
    "Here's how you can match for a link on the page with text mentioning \"2025\"\n",
    "```\n",
    "$x('//a[@href and contains(text(),\"2025\")]')\n",
    "```\n",
    "To sanity check your results, you can expand the resulting list and click any of the elements. This will highlight the element on the page and shoot you back to the Dev Tools \"Elements\" tab to view the element.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d4234",
   "metadata": {},
   "source": [
    "<video width=100% controls loop>\n",
    "    <source src=\"assets/xpath_simple.mp4\" type=video/mp4>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd6540",
   "metadata": {},
   "source": [
    "# Web Parsing with XPath \n",
    "\n",
    "## Static HTML\n",
    "\n",
    "With the correct xpath in hand, we can automate this parsing in Python using the `lxml` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572a9bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102bedb",
   "metadata": {},
   "source": [
    "Let's visit the website and retrieve the static HTML from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6f3406",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://text.npr.org/\"\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f5bed",
   "metadata": {},
   "source": [
    "## Compared to BeautifulSoup\n",
    "\n",
    "If you're familar with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), we're going to be doing the same steps as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6477c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'https://npr.org/nx-s1-5458512',\n",
       "  'title': 'How good was the forecast? Texas officials and the National Weather Service disagree'},\n",
       " {'link': 'https://npr.org/nx-s1-5458514',\n",
       "  'title': 'Texas officials race to find survivors after devastating floods '},\n",
       " {'link': 'https://npr.org/nx-s1-5457278',\n",
       "  'title': 'At least 78 dead and dozens missing after catastrophic Texas flooding '},\n",
       " {'link': 'https://npr.org/nx-s1-5454890',\n",
       "  'title': '4 things to know about the vaccine ingredient thimerosal'},\n",
       " {'link': 'https://npr.org/g-s1-75874',\n",
       "  'title': 'Knives, bullets and thieves: the quest for food in Gaza'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# read the webpage as bs4\n",
    "soup = BeautifulSoup(resp.text)\n",
    "\n",
    "# select all the \"a\" tags with the specified class.\n",
    "articles = soup.find_all(\"a\", {\"class\": \"topic-title\"})\n",
    "\n",
    "# iterate through each headline and grab the title and link of each story.\n",
    "data = []\n",
    "for elm in articles:\n",
    "    link = elm.get('href')\n",
    "    link = f\"https://npr.org{link}\"\n",
    "    title = elm.text\n",
    "    row = {'link' : link, 'title': title}\n",
    "    data.append(row)\n",
    "    \n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90509f04",
   "metadata": {},
   "source": [
    "## The same example using xpath\n",
    "\n",
    "Here we'll read the HTML into [lxml](https://lxml.de/) to parse elements based on xpath. Note: some frameworks prefer the `.//` relative syntax (lxml) compared to `//` (Playwright)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe384e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'https://npr.org/nx-s1-5458512',\n",
       "  'title': 'How good was the forecast? Texas officials and the National Weather Service disagree'},\n",
       " {'link': 'https://npr.org/nx-s1-5458514',\n",
       "  'title': 'Texas officials race to find survivors after devastating floods '},\n",
       " {'link': 'https://npr.org/nx-s1-5457278',\n",
       "  'title': 'At least 78 dead and dozens missing after catastrophic Texas flooding '},\n",
       " {'link': 'https://npr.org/nx-s1-5454890',\n",
       "  'title': '4 things to know about the vaccine ingredient thimerosal'},\n",
       " {'link': 'https://npr.org/g-s1-75874',\n",
       "  'title': 'Knives, bullets and thieves: the quest for food in Gaza'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# read the webpage as lxml\n",
    "tree = etree.HTML(resp.text)\n",
    "\n",
    "# select all the \"a\" tags with the specified class.\n",
    "xpath_article = './/a[@class=\"topic-title\"]'\n",
    "elements = tree.findall(xpath_article)\n",
    "\n",
    "# iterate through each headline and grab the title and link of each story.\n",
    "data = []\n",
    "for elm in elements:\n",
    "    link = elm.get('href')\n",
    "    link = f\"https://npr.org{link}\"\n",
    "    title = elm.text\n",
    "    row = {'link' : link, 'title': title}\n",
    "    data.append(row)\n",
    "    \n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820554a0",
   "metadata": {},
   "source": [
    "The outcome is the same between BeautifulSoup and lxml. However BeautifulSoup only works with static HTML you've saved or stored in memory.\n",
    "\n",
    "XPath can be used to parse data in this format, but also can be used to work in browser automation frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6543d",
   "metadata": {},
   "source": [
    "## Browser Automation\n",
    "\n",
    "The same xpath can be used for browser automation frameworks, such as Playwright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71aba542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Users/leon/miniconda3/lib/python3.7/site-packages (1.35.0)\n",
      "Requirement already satisfied: pyee==9.0.4 in /Users/leon/miniconda3/lib/python3.7/site-packages (from playwright) (9.0.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/leon/miniconda3/lib/python3.7/site-packages (from playwright) (3.7.4.3)\n",
      "Requirement already satisfied: greenlet==2.0.2 in /Users/leon/miniconda3/lib/python3.7/site-packages (from playwright) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "# download software\n",
    "!pip install playwright\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d168aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless = False)\n",
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ec494",
   "metadata": {},
   "source": [
    "The exact function to use XPath is a little different across scraping tools ('locator' in the case of Playwright), but the XPath expressions stay the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b1092f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'https://npr.org/nx-s1-5458512',\n",
       "  'title': 'How good was the forecast? Texas officials and the National Weather Service disagree'},\n",
       " {'link': 'https://npr.org/nx-s1-5458514',\n",
       "  'title': 'Texas officials race to find survivors after devastating floods '},\n",
       " {'link': 'https://npr.org/nx-s1-5457278',\n",
       "  'title': 'At least 78 dead and dozens missing after catastrophic Texas flooding '},\n",
       " {'link': 'https://npr.org/nx-s1-5454890',\n",
       "  'title': '4 things to know about the vaccine ingredient thimerosal'},\n",
       " {'link': 'https://npr.org/g-s1-75874',\n",
       "  'title': 'Knives, bullets and thieves: the quest for food in Gaza'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go to the webpage,\n",
    "await page.goto(url)\n",
    "\n",
    "# we use xpath in the `locator` function to select all the articles.\n",
    "xpath_article = '//a[@class=\"topic-title\"]'\n",
    "elms = await page.locator(xpath_article).all()\n",
    "\n",
    "# iterate through each headline and grab the title and link of each story.\n",
    "data = []\n",
    "for elm in elms:\n",
    "    title = await elm.text_content()\n",
    "    link = await elm.get_attribute('href')\n",
    "    link = f'https://npr.org{link}'\n",
    "    row = {'link': link, 'title': title,}\n",
    "    data.append(row)\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a165f68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Platform        | Function   | Example                                           |\n",
      "|:----------------|:-----------|:--------------------------------------------------|\n",
      "| BeautifulSoup   | `find_all` | `soup.find_all(\"a\", {\"class\": \"topic-title\"})`    |\n",
      "| Browser console | `$x`       | `$x('//a[@class=\"topic-title\"]')`                 |\n",
      "| lxml            | `findall`  | `tree.findall('//a[@class=\"topic-title\"]')`       |\n",
      "| Playwright      | `locator`  | `page.locator('//a[@class=\"topic-title\"]').all()` |\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "#| echo: false\n",
    "import pandas as pd\n",
    "x = pd.DataFrame([\n",
    "    {'Platform': 'BeautifulSoup', 'Function' : '`find_all`', 'Example' : '`soup.find_all(\"a\", {\"class\": \"topic-title\"})`'},\n",
    "    {'Platform': 'Browser console', 'Function' : '`$x`', 'Example' : '`$x(\\'//a[@class=\"topic-title\"]\\')`'},\n",
    "    {'Platform': 'lxml', 'Function' : '`findall`', 'Example' : '`tree.findall(\\'//a[@class=\"topic-title\"]\\')`'},\n",
    "    {'Platform': 'Playwright', 'Function' : '`locator`', 'Example' : '`page.locator(\\'//a[@class=\"topic-title\"]\\').all()`'},\n",
    "]).to_markdown(index=False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16d04b",
   "metadata": {},
   "source": [
    "To review, here is how we would find all the article elements across the frameworks we mentioned...\n",
    "\n",
    "# Equivalent functions across frameworks\n",
    "\n",
    "| Framework       | Function   | Example                                           |\n",
    "|:----------------|:-----------|:--------------------------------------------------|\n",
    "| bs4             | `find_all` | `soup.find_all(\"a\", {\"class\": \"topic-title\"})`    |\n",
    "| Browser console | `$x`       | `$x('//a[@class=\"topic-title\"]')`                 |\n",
    "| lxml            | `findall`  | `tree.findall('//a[@class=\"topic-title\"]')`       |\n",
    "| Playwright      | `locator`  | `page.locator('//a[@class=\"topic-title\"]').all()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1f8a3",
   "metadata": {},
   "source": [
    "## When things get more complicated\n",
    "\n",
    "We exhibited XPath's basic functionality on a simple static HTML page. It really shines when it comes to breaking down a complicated website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0aa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
